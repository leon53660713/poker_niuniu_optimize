{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用機器學習的方法改善策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "## niuniu function\n",
    "from niuniu_func import *\n",
    "\n",
    "## caculating\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "\n",
    "## os\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build niuniu env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env of niuniu\n",
    "# set myself as player 0\n",
    "class NiuNiuEnv:\n",
    "    # init \n",
    "    def __init__(self):\n",
    "        # generate deck\n",
    "        self.deck = self.generate_deck()\n",
    "        # generate player == 4\n",
    "        self.players = [[] for _ in range(4)]\n",
    "        self.banker_index = -1\n",
    "        # banker multiplier\n",
    "        self.banker_multiplier = 1\n",
    "        # bet number\n",
    "        self.bets = [0, 0, 0, 0]\n",
    "        # state, 0: bank step, 1: bet step, 2: result step\n",
    "        self.current_phase = 0\n",
    "        # reset\n",
    "        self.reset()\n",
    "\n",
    "    # generate deck\n",
    "    def generate_deck(self):\n",
    "        suits = ['heart', 'spade', 'diamond', 'club']\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "        return [(suit, rank) for suit in suits for rank in ranks]\n",
    "\n",
    "    # reset\n",
    "    def reset(self):\n",
    "        # regenerate deck & shuffle\n",
    "        self.deck = self.generate_deck()\n",
    "        random.shuffle(self.deck)\n",
    "        # every player have 4 cards\n",
    "        self.players = [[self.deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "        # init bets\n",
    "        self.bets = [0] * 4\n",
    "        self.banker_index = -1\n",
    "        self.banker_multiplier = 1\n",
    "\n",
    "        # init step\n",
    "        self.current_phase = 0\n",
    "\n",
    "        # reload state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "    \n",
    "    # get myself's hand number\n",
    "    def get_state(self):\n",
    "        # myself's hand\n",
    "        state = []\n",
    "        for card in self.players[0]:\n",
    "            state.append(get_suit_rank(card))\n",
    "            state.append(get_card_rank(card))\n",
    "        # which step\n",
    "        state.append(self.current_phase)\n",
    "        # who is banker\n",
    "        state.append(self.banker_index)\n",
    "        # banker multiplayer\n",
    "        state.append(self.banker_multiplier)\n",
    "        # every player's bet\n",
    "        state.extend(self.bets)\n",
    "        return np.array(state, dtype=np.float32)\n",
    "        \n",
    "    # step    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: [banker_action, bet_action]\n",
    "        banker_action: 0-4 is baker multi\n",
    "        bet_action: 1-5 is bet multi\n",
    "        \"\"\"\n",
    "        # unpack action\n",
    "        banker_action, bet_action = action \n",
    "        # banker\n",
    "        self.banker_bid = banker_action\n",
    "        # bet\n",
    "        self.bet_amount = bet_action\n",
    "\n",
    "        \"\"\"\n",
    "        step 1 : decide whether to get banker\n",
    "            * myself : by ppo agent\n",
    "            * others : by simulate_ev to decide\n",
    "        \"\"\"\n",
    "        bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        bank_multipliers[0] = self.banker_bid\n",
    "        # run time : 22s\n",
    "\n",
    "        \"\"\"\n",
    "        step 2 : decide final banker(the max multiplier)\n",
    "            * if all not want to be banker, random choose one & set multiplier = 1\n",
    "            * if more than one have same multiplier, random choose one\n",
    "        \"\"\"\n",
    "        max_bet = max(bank_multipliers)\n",
    "        if max_bet == 1:\n",
    "            random_banker = random.choice(range(4))\n",
    "            bank_multipliers[random_banker] = 1\n",
    "        banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "        self.banker_index = random.choice(banker_candidates)\n",
    "        self.banker_multiplier = max_bet\n",
    "        banker_hand = self.players[self.banker_index]\n",
    "\n",
    "        # whether myself is banker\n",
    "        is_banker = (self.banker_index == 0)\n",
    "\n",
    "        # go to next action -- bet\n",
    "        self.current_phase = 1\n",
    "\n",
    "        # bet action\n",
    "        if is_banker:\n",
    "            \"\"\"\n",
    "            step 3 : if myself is banker\n",
    "                * I don't need to bet\n",
    "                * others use `calculate_ev_against_banker` to bet, besides\n",
    "                if banker multiplier over 3, we assume banker have niu\n",
    "            \"\"\"\n",
    "            self.bets[0] = self.bet_amount\n",
    "            for i in range(1, 4):\n",
    "                have_niu = self.banker_multiplier >= 3\n",
    "                self.bets[i] = calculate_ev_against_banker(self.players[i], 100000, have_niu)[1]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            step 4 : if myself is not banker\n",
    "                * let ppo decide bet\n",
    "                * others we don't care\n",
    "            \"\"\"\n",
    "            self.bets[0] = max(1, min(5, action[1]))\n",
    "\n",
    "        \"\"\"\n",
    "        step 5 : add the 5th card to every player\n",
    "        \"\"\"\n",
    "        for i in range(4):\n",
    "            self.players[i].append(self.deck.pop())\n",
    "\n",
    "        # go to next action -- result\n",
    "        self.current_phase = 2\n",
    "\n",
    "        \"\"\"\n",
    "        step 6 : caculate ev of myself\n",
    "            * I am banker : caculate payout of the sum of me against others(use negative)\n",
    "            * I am not banker : calculate the payout against the banker\n",
    "        \"\"\"\n",
    "        if is_banker:\n",
    "            # I am banker\n",
    "            total_payout = -sum(\n",
    "                calculate_payout(self.players[i], banker_hand, False) * self.bets[i] * self.banker_multiplier\n",
    "                for i in range(4) if i != self.banker_index\n",
    "            )\n",
    "        else:\n",
    "            # I am not banker\n",
    "            total_payout = calculate_payout(self.players[0], banker_hand, False) * self.bets[0] * self.banker_multiplier\n",
    "\n",
    "        \"\"\"\n",
    "        step 7 : caculate reward(scaling & punishing)\n",
    "        \"\"\"\n",
    "        # 確保 max_possible_loss 至少是 1，避免除以 0\n",
    "        max_possible_loss = max(5 * self.banker_multiplier, 1)\n",
    "\n",
    "        # 標準化 reward，讓它在 -1 ~ 1 之間\n",
    "        reward = total_payout / max_possible_loss\n",
    "\n",
    "        # 增加對高倍率搶莊的風險懲罰\n",
    "        if is_banker:\n",
    "            # 如果選擇高倍率，額外懲罰 (例如 3 倍或 4 倍莊家)\n",
    "            risk_penalty = 0.05 * self.banker_multiplier  # 風險懲罰，倍率越高懲罰越大\n",
    "            if total_payout < 0:\n",
    "                reward -= risk_penalty  # 若莊家虧損，則增加懲罰\n",
    "\n",
    "        # 限制 reward 範圍，避免 PPO 過度偏向某個動作\n",
    "        reward = max(-1, min(1, reward))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        step 8 : finish one round\n",
    "        \"\"\"\n",
    "        done = True\n",
    "\n",
    "        # \"\"\"\n",
    "        # step 9 : reset\n",
    "        # \"\"\"\n",
    "        # self.reset()\n",
    "\n",
    "        return self.get_state(), reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple test\n",
    "test whether niuniu env is runnable <br>\n",
    "to avoid getting error of having NaN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [ 3. 13.  3.  6.  3.  5.  4. 11.  0. -1.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Step 0 - State: [ 4.  5.  3.  1.  3. 13.  1.  2.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -2.0\n",
      "Step 1 - State: [ 4.  1.  4.  4.  3.  8.  1.  2.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.2\n",
      "Step 2 - State: [ 3.  1.  3.  7.  2.  5.  4. 13.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 1.2\n",
      "Step 3 - State: [ 1. 13.  1.  3.  2.  6.  1.  2.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.8\n",
      "Step 4 - State: [ 1. 12.  2.  8.  3.  7.  2.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 0.8\n",
      "Step 5 - State: [ 2.  2.  1.  1.  4.  3.  2.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.4\n",
      "Step 6 - State: [ 2. 11.  2.  2.  1.  1.  2. 13.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.6\n",
      "Step 7 - State: [ 4.  1.  1. 12.  3. 12.  1.  3.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.4\n",
      "Step 8 - State: [ 4.  9.  1.  3.  3. 10.  3. 11.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.2\n",
      "Step 9 - State: [ 2.  2.  2. 10.  2.  4.  4. 10.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nresult explain :\\n    * the first 8 numbers represent 4 card in hands, (suit, card)\\n    * the others represent the other states\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test env of niuniu\n",
    "def test_env():\n",
    "    env = NiuNiuEnv()\n",
    "    state = env.reset()\n",
    "    print(\"Initial State:\", state)\n",
    "    # test 10 times\n",
    "    for i in range(10):\n",
    "        # random action\n",
    "        action = [np.random.randint(0, 5), np.random.randint(1, 6)]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if np.isnan(state).any():\n",
    "            print(f\"NaN detected in state at step {i}!\")\n",
    "        if np.isnan(reward):\n",
    "            print(f\"NaN detected in reward at step {i}!\")\n",
    "        print(f\"Step {i} - State: {state}, Reward: {reward}\")\n",
    "\n",
    "test_env()\n",
    "# run time : 3m 55s\n",
    "\"\"\"\n",
    "result explain :\n",
    "    * the first 8 numbers represent 4 card in hands, (suit, card)\n",
    "    * the others represent the other states\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 價值網絡 (V(s))\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),  # 避免標準化時出現 NaN\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1)  # 輸出 V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).squeeze(-1)  # 讓輸出維度變成 (batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 策略網絡\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.banker_fc = nn.Linear(128, output_dim1)  # 搶莊動作\n",
    "        self.bet_fc = nn.Linear(128, output_dim2)  # 下注動作\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        banker_logits = self.banker_fc(x)\n",
    "        bet_logits = self.bet_fc(x)\n",
    "\n",
    "        banker_probs = F.softmax(banker_logits, dim=-1)\n",
    "        bet_probs = F.softmax(bet_logits, dim=-1)\n",
    "\n",
    "        return banker_probs, bet_probs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # 轉成 batch\n",
    "        banker_probs, bet_probs = self.forward(state)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().item()\n",
    "        bet_action = bet_dist.sample().item()  # 這裡不加 +1，讓外部處理\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action))\n",
    "\n",
    "        return (banker_action, bet_action), banker_log_prob, bet_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=10, model_path=\"niu_ppo_model\", num_envs=8):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_envs = num_envs\n",
    "        \n",
    "        self.policy = PolicyNetwork(input_dim, output_dim1, output_dim2).to(self.device)\n",
    "        self.value = ValueNetwork(input_dim).to(self.device)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.model_path = model_path\n",
    "        # load saved model\n",
    "        self.load_model()\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        R = torch.zeros(1, dtype=torch.float32).to(self.device)  # 改為標量初始化\n",
    "\n",
    "        # 確保 rewards 和 dones 是 (T, 1) 形狀\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            R = rewards[t] + self.gamma * R * (1 - dones[t])\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        return torch.cat(returns).detach()\n",
    "\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, dones):\n",
    "        returns = self.compute_returns(rewards, dones)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).view(-1, 2).to(self.device)  # 確保 actions 維度正確\n",
    "        old_log_probs = torch.tensor(log_probs, dtype=torch.float32).view(-1).to(self.device)  # 轉為 1D\n",
    "\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            banker_probs, bet_probs = self.policy(states)\n",
    "\n",
    "            banker_probs = torch.nan_to_num(banker_probs, nan=0.0)\n",
    "            bet_probs = torch.nan_to_num(bet_probs, nan=0.0)\n",
    "\n",
    "            banker_dist = Categorical(banker_probs)\n",
    "            bet_dist = Categorical(bet_probs)\n",
    "\n",
    "            new_banker_log_prob = banker_dist.log_prob(actions[:, 0])\n",
    "            new_bet_log_prob = bet_dist.log_prob(actions[:, 1] - 1)\n",
    "            new_log_probs = new_banker_log_prob + new_bet_log_prob\n",
    "            new_log_probs = torch.nan_to_num(new_banker_log_prob, nan=0.0) + torch.nan_to_num(new_bet_log_prob, nan=0.0)\n",
    "    \n",
    "            value_estimates = self.value(states).view(-1)\n",
    "            value_estimates = torch.nan_to_num(value_estimates, nan=0.0)\n",
    "\n",
    "            advantages = returns - value_estimates.detach()\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "            self.optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        # state = torch.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        state = torch.nan_to_num(state, nan=0.0)\n",
    "\n",
    "        banker_probs, bet_probs = self.policy(state)\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().cpu().numpy()\n",
    "        bet_action = (bet_dist.sample() + 1).cpu().numpy()\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action, device=self.device))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action - 1, device=self.device))\n",
    "\n",
    "        return np.array([banker_action, bet_action]), banker_log_prob.detach().cpu().numpy(), bet_log_prob.detach().cpu().numpy()\n",
    "    \n",
    "    def save_model(self):\n",
    "        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "        torch.save(self.policy.state_dict(), f\"{self.model_path}_policy.pth\")\n",
    "        torch.save(self.value.state_dict(), f\"{self.model_path}_value.pth\")\n",
    "        print(\"model saved\")\n",
    "\n",
    "    def load_model(self):\n",
    "        policy_path = f\"{self.model_path}_policy.pth\"\n",
    "        value_path = f\"{self.model_path}_value.pth\"\n",
    "\n",
    "        if os.path.exists(policy_path) and os.path.exists(value_path):\n",
    "            self.policy.load_state_dict(torch.load(policy_path))\n",
    "            self.value.load_state_dict(torch.load(value_path))\n",
    "            print(\"load saved model\")\n",
    "        else:\n",
    "            print(\"model not found, start training from begining\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 120  # 訓練回合數\n",
    "batch_size = 2048\n",
    "gamma = 0.99\n",
    "clip_epsilon = 0.2\n",
    "lr = 3e-4\n",
    "update_epochs = 10\n",
    "save_interval = 5  # 每 5 回合存一次模型\n",
    "\n",
    "save_model_path = \"D:/python/poker_gto/ppo_models/ppo_model\"\n",
    "num_envs = 12  # 使用 8 個環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found, start training from begining\n"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "banker_action_dim = 5  # 搶莊倍率 (0~4)\n",
    "bet_action_dim = 5  # 下注倍率 (1~5)\n",
    "\n",
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim, model_path=save_model_path, num_envs=num_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 0.4\n",
      "Episode 2: Total Reward = 1\n",
      "Episode 3: Total Reward = -1\n",
      "Episode 4: Total Reward = -1\n",
      "Episode 5: Total Reward = -0.4\n",
      "model saved\n",
      "Episode 6: Total Reward = -0.6\n",
      "Episode 7: Total Reward = -0.8\n",
      "Episode 8: Total Reward = -0.6\n",
      "Episode 9: Total Reward = 1\n",
      "Episode 10: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 11: Total Reward = -0.6000000000000001\n",
      "Episode 12: Total Reward = 0.6\n",
      "Episode 13: Total Reward = 0.6\n",
      "Episode 14: Total Reward = 0.6\n",
      "Episode 15: Total Reward = 1\n",
      "model saved\n",
      "Episode 16: Total Reward = 1\n",
      "Episode 17: Total Reward = -0.6\n",
      "Episode 18: Total Reward = 0.2\n",
      "Episode 19: Total Reward = -0.6\n",
      "Episode 20: Total Reward = -1\n",
      "model saved\n",
      "Episode 21: Total Reward = 0.6\n",
      "Episode 22: Total Reward = -0.6\n",
      "Episode 23: Total Reward = 0.6\n",
      "Episode 24: Total Reward = -0.6\n",
      "Episode 25: Total Reward = -1\n",
      "model saved\n",
      "Episode 26: Total Reward = -1\n",
      "Episode 27: Total Reward = 0.4\n",
      "Episode 28: Total Reward = 1\n",
      "Episode 29: Total Reward = -0.8\n",
      "Episode 30: Total Reward = -1\n",
      "model saved\n",
      "Episode 31: Total Reward = 0.6\n",
      "Episode 32: Total Reward = 1\n",
      "Episode 33: Total Reward = -1\n",
      "Episode 34: Total Reward = -1\n",
      "Episode 35: Total Reward = 0.8\n",
      "model saved\n",
      "Episode 36: Total Reward = 0.2\n",
      "Episode 37: Total Reward = -1\n",
      "Episode 38: Total Reward = 1\n",
      "Episode 39: Total Reward = 1\n",
      "Episode 40: Total Reward = 1\n",
      "model saved\n",
      "Episode 41: Total Reward = -0.6000000000000001\n",
      "Episode 42: Total Reward = -0.8\n",
      "Episode 43: Total Reward = 1\n",
      "Episode 44: Total Reward = 0.6\n",
      "Episode 45: Total Reward = -1\n",
      "model saved\n",
      "Episode 46: Total Reward = 1\n",
      "Episode 47: Total Reward = -1\n",
      "Episode 48: Total Reward = -1\n",
      "Episode 49: Total Reward = -1\n",
      "Episode 50: Total Reward = -0.8\n",
      "model saved\n",
      "Episode 51: Total Reward = -1\n",
      "Episode 52: Total Reward = -0.6\n",
      "Episode 53: Total Reward = -1\n",
      "Episode 54: Total Reward = 1\n",
      "Episode 55: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 56: Total Reward = -1\n",
      "Episode 57: Total Reward = 0.6\n",
      "Episode 58: Total Reward = 1\n",
      "Episode 59: Total Reward = -1\n",
      "Episode 60: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 61: Total Reward = -0.6\n",
      "Episode 62: Total Reward = -0.6\n",
      "Episode 63: Total Reward = -1\n",
      "Episode 64: Total Reward = 1\n",
      "Episode 65: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 66: Total Reward = -1\n",
      "Episode 67: Total Reward = -0.6\n",
      "Episode 68: Total Reward = 0.6\n",
      "Episode 69: Total Reward = -1\n",
      "Episode 70: Total Reward = 1\n",
      "model saved\n",
      "Episode 71: Total Reward = 0.6\n",
      "Episode 72: Total Reward = 0.6\n",
      "Episode 73: Total Reward = -0.6\n",
      "Episode 74: Total Reward = -1\n",
      "Episode 75: Total Reward = -1\n",
      "model saved\n",
      "Episode 76: Total Reward = -0.6\n",
      "Episode 77: Total Reward = -1\n",
      "Episode 78: Total Reward = -1\n",
      "Episode 79: Total Reward = 1\n",
      "Episode 80: Total Reward = 0.6\n",
      "model saved\n",
      "Episode 81: Total Reward = -1\n",
      "Episode 82: Total Reward = 0.6\n",
      "Episode 83: Total Reward = -1\n",
      "Episode 84: Total Reward = -1\n",
      "Episode 85: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 86: Total Reward = 1\n",
      "Episode 87: Total Reward = -0.6\n",
      "Episode 88: Total Reward = -1\n",
      "Episode 89: Total Reward = -1\n",
      "Episode 90: Total Reward = 1\n",
      "model saved\n",
      "Episode 91: Total Reward = -1\n",
      "Episode 92: Total Reward = -1\n",
      "Episode 93: Total Reward = -1\n",
      "Episode 94: Total Reward = -0.6\n",
      "Episode 95: Total Reward = 1\n",
      "model saved\n",
      "Episode 96: Total Reward = -1\n",
      "Episode 97: Total Reward = 0.6\n",
      "Episode 98: Total Reward = 1\n",
      "Episode 99: Total Reward = -0.6\n",
      "Episode 100: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 101: Total Reward = -1\n",
      "Episode 102: Total Reward = -1\n",
      "Episode 103: Total Reward = -1\n",
      "Episode 104: Total Reward = -1\n",
      "Episode 105: Total Reward = -0.8\n",
      "model saved\n",
      "Episode 106: Total Reward = 1\n",
      "Episode 107: Total Reward = 0.6\n",
      "Episode 108: Total Reward = 1\n",
      "Episode 109: Total Reward = -0.8\n",
      "Episode 110: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 111: Total Reward = 1\n",
      "Episode 112: Total Reward = -1\n",
      "Episode 113: Total Reward = 1\n",
      "Episode 114: Total Reward = 0.6\n",
      "Episode 115: Total Reward = -1\n",
      "model saved\n",
      "Episode 116: Total Reward = 1\n",
      "Episode 117: Total Reward = -1\n",
      "Episode 118: Total Reward = 1\n",
      "Episode 119: Total Reward = -1\n",
      "Episode 120: Total Reward = -0.6\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# 訓練\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.select_action(state)\n",
    "\n",
    "        # 確保動作格式正確\n",
    "        banker_action, bet_action = action\n",
    "        next_state, reward, done, _ = env.step((banker_action, bet_action))  # 確保和環境兼容\n",
    "\n",
    "        # 記錄數據\n",
    "        states.append(state)\n",
    "        actions.append([banker_action, bet_action])  # 確保 actions 格式正確\n",
    "        log_probs.append(banker_log_prob + bet_log_prob)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # 更新 PPO\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # 顯示訓練結果\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "    \n",
    "    # 每 `save_interval` 次存一次模型\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        ppo_agent.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(agent, policy_model_path, value_model_path):\n",
    "    \"\"\"\n",
    "    載入 policy 和 value 模型，並顯示當前使用的模型名稱。\n",
    "    \"\"\"\n",
    "    print(f\"🔍 正在使用的模型: policy -> {policy_model_path}, value -> {value_model_path}\")\n",
    "    agent.policy.load_state_dict(torch.load(policy_model_path))\n",
    "    agent.value.load_state_dict(torch.load(value_model_path))\n",
    "    print(\"✅ 模型載入完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(env, agent, policy_model=\"D:\\python\\poker_gto\\ppo_models\\ppo_model_policy.pth\", value_model=\"D:\\python\\poker_gto\\ppo_models\\ppo_model_value.pth\"):\n",
    "    \"\"\"\n",
    "    使用訓練好的模型，讓使用者輸入 4 張手牌，並讓模型決策搶莊與下注倍率\n",
    "    \"\"\"\n",
    "    print(\"\\U0001F50D 測試模式啟動！輸入 `exit` 可離開測試模式。\")\n",
    "    \n",
    "    # 確保模型在 GPU 上運行\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    load_model(agent, policy_model, value_model)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # 取得環境的初始狀態\n",
    "            state = env.reset()\n",
    "\n",
    "            # 讓使用者輸入 4 張手牌\n",
    "            print(\"\\n請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\")\n",
    "            input_cards = input().strip()\n",
    "\n",
    "            if input_cards.lower() == 'exit':\n",
    "                print(\"\\U0001F44B 測試結束！\")\n",
    "                break\n",
    "\n",
    "            input_cards = input_cards.split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"❌ 錯誤！請輸入 4 張手牌的花色與數值（共 8 個字串）。\")\n",
    "                continue\n",
    "\n",
    "            # 解析輸入的手牌\n",
    "            player_hand = [(input_cards[i], input_cards[i + 1]) for i in range(0, 8, 2)]\n",
    "            print(f\"\\U0001F3B4 你的手牌: {player_hand}\")\n",
    "\n",
    "            # 更新 state，確保手牌資訊正確\n",
    "            for i in range(4):\n",
    "                state[i * 2] = get_suit_rank(player_hand[i])  # 花色\n",
    "                state[i * 2 + 1] = get_card_rank(player_hand[i])  # 點數\n",
    "\n",
    "            # 轉換為 PyTorch Tensor 並移至 GPU\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # 模型預測搶莊倍率\n",
    "            with torch.no_grad():\n",
    "                banker_dist, bet_dist = agent.policy(state_tensor)\n",
    "                print(f\"🔍 banker_dist: {banker_dist}\")  # 打印 banker_dist 的結構\n",
    "                print(f\"🔍 bet_dist: {bet_dist}\")  # 打印 bet_dist 的結構\n",
    "                \n",
    "                # 根據模型輸出結構決定如何取得概率\n",
    "                if hasattr(banker_dist, 'probs'):\n",
    "                    banker_action = torch.argmax(banker_dist.probs).item()\n",
    "                else:\n",
    "                    # 如果 banker_dist 沒有 'probs' 屬性，可以打印 banker_dist 或作其他處理\n",
    "                    banker_action = torch.argmax(banker_dist).item()\n",
    "                    \n",
    "                if hasattr(bet_dist, 'probs'):\n",
    "                    bet_action = torch.argmax(bet_dist.probs).item()\n",
    "                else:\n",
    "                    bet_action = torch.argmax(bet_dist).item()\n",
    "\n",
    "            print(f\"\\U0001F916 模型預測的搶莊倍率: {banker_action}\")\n",
    "\n",
    "            # 讓使用者輸入是否成功搶莊\n",
    "            is_banker = input(\"✅ 是否搶到莊？ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"\\U0001F389 你是莊家！不需要下注\")\n",
    "            else:\n",
    "                print(\"沒有搶到莊家\")\n",
    "                banker_multiplier = float(input(\"\\U0001F4E2 請輸入莊家的倍率: \").strip())\n",
    "                print(f\"\\U0001F916 莊家的倍率: {banker_multiplier}\")\n",
    "\n",
    "                # 更新 state 中的莊家倍率\n",
    "                state[-4] = banker_multiplier\n",
    "\n",
    "                # 轉換為 PyTorch Tensor 並移至 GPU\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                # 模型預測下注倍率\n",
    "                with torch.no_grad():\n",
    "                    _, bet_dist = agent.policy(state_tensor)\n",
    "                    print(f\"🔍 bet_dist: {bet_dist}\")  # 打印 bet_dist 的結構\n",
    "\n",
    "                    if hasattr(bet_dist, 'probs'):\n",
    "                        bet_action = torch.argmax(bet_dist.probs).item()\n",
    "                    else:\n",
    "                        bet_action = torch.argmax(bet_dist).item()\n",
    "\n",
    "                print(f\"\\U0001F916 模型建議的下注倍率: {bet_action}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 發生錯誤: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 測試模式啟動！輸入 `exit` 可離開測試模式。\n",
      "🔍 正在使用的模型: policy -> D:\\python\\poker_gto\\ppo_models\\ppo_model_policy.pth, value -> D:\\python\\poker_gto\\ppo_models\\ppo_model_value.pth\n",
      "✅ 模型載入完成！\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "🔍 banker_dist: tensor([[3.2925e-04, 9.9866e-01, 2.2667e-04, 3.5804e-05, 7.5206e-04]])\n",
      "🔍 bet_dist: tensor([[3.6316e-05, 3.8988e-04, 9.9925e-01, 3.1027e-04, 1.7785e-05]])\n",
      "🤖 模型預測的搶莊倍率: 1\n",
      "沒有搶到莊家\n",
      "❌ 發生錯誤: could not convert string to float: 'heart J diamond 10 club J spade K'\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'K')]\n",
      "🔍 banker_dist: tensor([[4.1909e-05, 9.9976e-01, 3.5180e-05, 8.5496e-06, 1.5434e-04]])\n",
      "🔍 bet_dist: tensor([[2.0682e-05, 1.7232e-04, 9.9945e-01, 3.4962e-04, 5.6794e-06]])\n",
      "🤖 模型預測的搶莊倍率: 1\n",
      "沒有搶到莊家\n",
      "❌ 發生錯誤: could not convert string to float: 'heart J diamond Q club K spade K'\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "🎴 你的手牌: [('heart', 'J'), ('diamond', 'K'), ('club', 'Q'), ('spade', 'K')]\n",
      "🔍 banker_dist: tensor([[1.7999e-05, 9.9990e-01, 1.2202e-05, 2.4744e-06, 6.9479e-05]])\n",
      "🔍 bet_dist: tensor([[5.1391e-06, 5.6284e-05, 9.9983e-01, 1.0886e-04, 1.2246e-06]])\n",
      "🤖 模型預測的搶莊倍率: 1\n",
      "🎉 你是莊家！不需要下注\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "👋 測試結束！\n"
     ]
    }
   ],
   "source": [
    "test_trained_model(env, ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "倍率 3: 100 次 (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({3: 100})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def generate_random_hand():\n",
    "    suits = [\"heart\", \"diamond\", \"club\", \"spade\"]\n",
    "    ranks = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\", \"A\"]\n",
    "    hand = np.random.choice([f\"{suit} {rank}\" for suit in suits for rank in ranks], 4, replace=False)\n",
    "    hand = [tuple(card.split()) for card in hand]\n",
    "    return hand\n",
    "\n",
    "def test_banker_distribution(env, agent, num_tests=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    \n",
    "    banker_choices = []\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        state = env.reset()\n",
    "        player_hand = generate_random_hand()\n",
    "        \n",
    "        for i in range(4):\n",
    "            state[i * 2] = get_suit_rank(player_hand[i])\n",
    "            state[i * 2 + 1] = get_card_rank(player_hand[i])\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            banker_dist, _ = agent.policy(state_tensor)\n",
    "            banker_action = torch.argmax(banker_dist).item()\n",
    "            banker_choices.append(banker_action)\n",
    "    \n",
    "    # 計算分布\n",
    "    counter = Counter(banker_choices)\n",
    "    total = sum(counter.values())\n",
    "    for action, count in sorted(counter.items()):\n",
    "        print(f\"倍率 {action}: {count} 次 ({count / total:.2%})\")\n",
    "    \n",
    "    return counter\n",
    "\n",
    "# 測試 100 次隨機手牌的 banker_dist 分布\n",
    "test_banker_distribution(env, ppo_agent, num_tests=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 測試模式啟動！輸入 `exit` 可離開測試模式。\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "📊 banker_dist 統計 (測試 100 次): {3: 100}\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "👋 測試結束！\n"
     ]
    }
   ],
   "source": [
    "test_trained_model(env, ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do list\n",
    "* **(finish)** 訓練時應該包含花色 : 可能將花色也轉為數值，變成一個 2 維度的 state  <br>\n",
    "* **(finish)** 訓練後應該可以保存模型，並且疊家每次訓練的成果上去 <br>\n",
    "* 新增一個輸入 : 假設我不是莊家時，現在的倍率是幾倍，這會影響到我後續的下注策略 <br>\n",
    "* <b>(maybe finish)</b>輸入我的手牌之類的資訊後應該要可以當作回測，紀錄到模型訓練當中，並加以改進 <br>\n",
    "\n",
    "* 完成後可能可以架設簡單的 app 或 api  <br>\n",
    "* 完成後可以接著改做 德州撲克的訓練 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_with_real_hands(env, agent):\n",
    "    \"\"\"\n",
    "    讓使用者輸入實際手牌，讓 AI 提供決策建議，並將結果回報給模型，增強學習\n",
    "    \"\"\"\n",
    "    print(\"\\U0001F4CA 進入回測模式！輸入 `exit` 可離開回測模式。\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            state = env.reset()\n",
    "\n",
    "            print(\"\\n請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\")\n",
    "            input_cards = input().strip()\n",
    "\n",
    "            if input_cards.lower() == 'exit':\n",
    "                print(\"\\U0001F44B 回測結束！\")\n",
    "                break\n",
    "\n",
    "            input_cards = input_cards.split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"❌ 錯誤！請輸入 4 張手牌的花色與數值（共 8 個字串）。\")\n",
    "                continue\n",
    "\n",
    "            player_hand = [(input_cards[i], input_cards[i + 1]) for i in range(0, 8, 2)]\n",
    "            print(f\"\\U0001F3B4 你的手牌: {player_hand}\")\n",
    "\n",
    "            for i in range(4):\n",
    "                state[i] = card_value(player_hand[i])\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                banker_dist, bet_dist = agent.policy(state_tensor)\n",
    "                banker_action = torch.argmax(banker_dist.probs).item()\n",
    "                bet_action = torch.argmax(bet_dist.probs).item()\n",
    "\n",
    "            print(f\"\\U0001F916 AI 建議的搶莊倍率: {banker_action}\")\n",
    "\n",
    "            is_banker = input(\"✅ 是否搶到莊？ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"\\U0001F389 你是莊家！不需要下注\")\n",
    "                bet_action = 0\n",
    "            else:\n",
    "                print(f\"\\U0001F916 AI 建議的下注倍率: {bet_action}\")\n",
    "\n",
    "            reward = float(input(\"\\U0001F4B0 請輸入這局的最終收益（負值代表虧損）: \").strip())\n",
    "\n",
    "            states = [state]\n",
    "            actions = [[banker_action, bet_action]]\n",
    "            rewards = [reward]\n",
    "            dones = [True]\n",
    "\n",
    "            agent.update(states, actions, rewards, dones)\n",
    "            print(\"\\U0001F4C8 AI 已學習這局的結果！\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 發生錯誤: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parellel computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
