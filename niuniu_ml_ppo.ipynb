{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用機器學習的方法改善策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niuniu_func import *\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build niuniu env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env of niuniu\n",
    "# set myself as player 0\n",
    "class NiuNiuEnv:\n",
    "    # init \n",
    "    def __init__(self):\n",
    "        # generate deck\n",
    "        self.deck = self.generate_deck()\n",
    "        # generate player == 4\n",
    "        self.players = [[] for _ in range(4)]\n",
    "        self.banker_index = -1\n",
    "        # banker multiplier\n",
    "        self.banker_multiplier = 1\n",
    "        # bet number\n",
    "        self.bets = [0, 0, 0, 0]\n",
    "        # generate state\n",
    "        self.state = None\n",
    "        # state, 0: bank step, 1: bet step, 2: result step\n",
    "        self.current_phase = 0\n",
    "        # reset\n",
    "        self.reset()\n",
    "\n",
    "    # generate deck\n",
    "    def generate_deck(self):\n",
    "        suits = ['heart', 'spade', 'diamond', 'club']\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "        return [(suit, rank) for suit in suits for rank in ranks]\n",
    "\n",
    "    # reset\n",
    "    def reset(self):\n",
    "        # regenerate deck & shuffle\n",
    "        self.deck = self.generate_deck()\n",
    "        random.shuffle(self.deck)\n",
    "        # every player have 4 cards\n",
    "        self.players = [[self.deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "        # init bets\n",
    "        self.bets = [0] * 4\n",
    "        self.banker_index = -1\n",
    "        self.banker_multiplier = 1\n",
    "\n",
    "        # init step\n",
    "        self.current_phase = 0\n",
    "\n",
    "        # reload state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "    \n",
    "    # get myself's hand number\n",
    "    def get_state(self):\n",
    "        # myself's hand\n",
    "        state = [card_value(card) for card in self.players[0]]\n",
    "        # which step\n",
    "        state.append(self.current_phase)\n",
    "        # who is banker\n",
    "        state.append(self.banker_index)\n",
    "        # banker multiplayer\n",
    "        state.append(self.banker_multiplier)\n",
    "        # every player's bet\n",
    "        state.extend(self.bets)\n",
    "\n",
    "        # ensure banker_bid & bet are init\n",
    "        if self.current_phase > 0:\n",
    "            state.append(self.banker_bid)\n",
    "        else:\n",
    "            # init baker_bid value\n",
    "            state.append(0)\n",
    "        # player bet\n",
    "        state.append(self.bets[0])\n",
    "\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    # step    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: [banker_action, bet_action]\n",
    "        banker_action: 0-4 is baker multi\n",
    "        bet_action: 1-5 is bet multi\n",
    "        \"\"\"\n",
    "        # unpack action\n",
    "        banker_action, bet_action = action \n",
    "        # banker\n",
    "        self.banker_bid = banker_action\n",
    "        # bet\n",
    "        self.bet_amount = bet_action\n",
    "\n",
    "        \"\"\"\n",
    "        step 1 : decide whether to get banker\n",
    "            * myself : by ppo agent\n",
    "            * others : by simulate_ev to decide\n",
    "        \"\"\"\n",
    "        bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        bank_multipliers[0] = self.banker_bid\n",
    "        # run time : 22s\n",
    "\n",
    "        \"\"\"\n",
    "        step 2 : decide final banker(the max multiplier)\n",
    "            * if all not want to be banker, random choose one & set multiplier = 1\n",
    "            * if more than one have same multiplier, random choose one\n",
    "        \"\"\"\n",
    "        max_bet = max(bank_multipliers)\n",
    "        if max_bet == 1:\n",
    "            random_banker = random.choice(range(4))\n",
    "            bank_multipliers[random_banker] = 1\n",
    "        banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "        self.banker_index = random.choice(banker_candidates)\n",
    "        self.banker_multiplier = max_bet\n",
    "        banker_hand = self.players[self.banker_index]\n",
    "\n",
    "        # whether myself is banker\n",
    "        is_banker = (self.banker_index == 0)\n",
    "\n",
    "        # go to next action -- bet\n",
    "        self.current_phase = 1\n",
    "\n",
    "        # bet action\n",
    "        if is_banker:\n",
    "            \"\"\"\n",
    "            step 3 : if myself is banker\n",
    "                * I don't need to bet\n",
    "                * others use `calculate_ev_against_banker` to bet, besides\n",
    "                if banker multiplier over 3, we assume banker have niu\n",
    "            \"\"\"\n",
    "            self.bets[0] = self.bet_amount\n",
    "            for i in range(1, 4):\n",
    "                have_niu = self.banker_multiplier >= 3\n",
    "                self.bets[i] = calculate_ev_against_banker(self.players[i], 100000, have_niu)[1]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            step 4 : if myself is not banker\n",
    "                * let ppo decide bet\n",
    "                * others we don't care\n",
    "            \"\"\"\n",
    "            self.bets[0] = max(1, min(5, action[1]))\n",
    "\n",
    "        \"\"\"\n",
    "        step 5 : add the 5th card to every player\n",
    "        \"\"\"\n",
    "        for i in range(4):\n",
    "            self.players[i].append(self.deck.pop())\n",
    "\n",
    "        # go to next action -- result\n",
    "        self.current_phase = 2\n",
    "\n",
    "        \"\"\"\n",
    "        step 6 : caculate ev of myself\n",
    "            * I am banker : caculate payout of the sum of me against others(use negative)\n",
    "            * I am not banker : calculate the payout against the banker\n",
    "        \"\"\"\n",
    "        if is_banker:\n",
    "            # I am banker\n",
    "            total_payout = -sum(\n",
    "                calculate_payout(self.players[i], banker_hand, False) * self.bets[i] * self.banker_multiplier\n",
    "                for i in range(4) if i != self.banker_index\n",
    "            )\n",
    "        else:\n",
    "            # I am not banker\n",
    "            total_payout = calculate_payout(self.players[0], banker_hand, False) * self.bets[0] * self.banker_multiplier\n",
    "\n",
    "        \"\"\"\n",
    "        step 7 : caculate reward(scaling & punishing)\n",
    "        \"\"\"\n",
    "        max_possible_loss = 5 * self.banker_multiplier\n",
    "        reward = total_payout / max_possible_loss\n",
    "        # have punish value for being the banker\n",
    "        if is_banker and total_payout > 0:\n",
    "            reward += 0.2\n",
    "        if is_banker and total_payout < 0:\n",
    "            reward -= 0.2\n",
    "\n",
    "        \"\"\"\n",
    "        step 8 : finish one round\n",
    "        \"\"\"\n",
    "        done = True\n",
    "\n",
    "        \"\"\"\n",
    "        step 9 : reset\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple test\n",
    "test whether niuniu env is runnable <br>\n",
    "to avoid getting error of having NaN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [ 3. 10.  2. 10.  0. -1.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Step 0 - State: [ 4. 10.  9.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.8\n",
      "Step 1 - State: [10.  1.  8.  8.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 3.0\n",
      "Step 2 - State: [ 6. 10.  7.  6.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.8\n",
      "Step 3 - State: [ 7.  7. 10.  8.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.2\n",
      "Step 4 - State: [10. 10.  8.  7.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.8\n",
      "Step 5 - State: [10.  2.  5.  9.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 0.0\n",
      "Step 6 - State: [ 6.  2. 10.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -2.0\n",
      "Step 7 - State: [10. 10.  5.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.2\n",
      "Step 8 - State: [ 8.  1.  7.  8.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.8\n",
      "Step 9 - State: [10.  1.  2.  7.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -3.2\n"
     ]
    }
   ],
   "source": [
    "# test env of niuniu\n",
    "def test_env():\n",
    "    env = NiuNiuEnv()\n",
    "    state = env.reset()\n",
    "    print(\"Initial State:\", state)\n",
    "    # test 10 times\n",
    "    for i in range(10):\n",
    "        # random action\n",
    "        action = [np.random.randint(0, 5), np.random.randint(1, 6)]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if np.isnan(state).any():\n",
    "            print(f\"NaN detected in state at step {i}!\")\n",
    "        if np.isnan(reward):\n",
    "            print(f\"NaN detected in reward at step {i}!\")\n",
    "        print(f\"Step {i} - State: {state}, Reward: {reward}\")\n",
    "\n",
    "test_env()\n",
    "# run time : 3m 52s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 價值網絡 (V(s))\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),  # 避免標準化時出現 NaN\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1)  # 輸出 V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 策略網絡\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.banker_fc = nn.Linear(128, output_dim1)  # 搶莊動作\n",
    "        self.bet_fc = nn.Linear(128, output_dim2)  # 下注動作\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        banker_logits = self.banker_fc(x)\n",
    "        bet_logits = self.bet_fc(x)\n",
    "\n",
    "        # 檢查 NaN 並修正\n",
    "        banker_logits = torch.nan_to_num(banker_logits, nan=0.0)\n",
    "        bet_logits = torch.nan_to_num(bet_logits, nan=0.0)\n",
    "\n",
    "        banker_probs = F.softmax(banker_logits, dim=-1)\n",
    "        bet_probs = F.softmax(bet_logits, dim=-1)\n",
    "\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)  # 確保概率不為 NaN\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        return banker_probs, bet_probs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # 轉成 batch\n",
    "        banker_probs, bet_probs = self.forward(state)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample()\n",
    "        bet_action = bet_dist.sample() + 1  # 下注倍率是 1~5，所以要 +1\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(banker_action)\n",
    "        bet_log_prob = bet_dist.log_prob(bet_action - 1)\n",
    "\n",
    "        return (banker_action.item(), bet_action.item()), banker_log_prob, bet_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=10):\n",
    "        self.policy = PolicyNetwork(input_dim, output_dim1, output_dim2)\n",
    "        self.value = ValueNetwork(input_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            if d:\n",
    "                R = 0\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # 避免標準化時出現 NaN（returns.std() 可能為 0）\n",
    "        return (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, dones):\n",
    "        returns = self.compute_returns(rewards, dones)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        old_log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            banker_probs, bet_probs = self.policy(states)\n",
    "\n",
    "            banker_probs = torch.nan_to_num(banker_probs, nan=0.0)\n",
    "            bet_probs = torch.nan_to_num(bet_probs, nan=0.0)\n",
    "\n",
    "            banker_dist = Categorical(banker_probs)\n",
    "            bet_dist = Categorical(bet_probs)\n",
    "\n",
    "            new_banker_log_prob = banker_dist.log_prob(actions[:, 0])\n",
    "            new_bet_log_prob = bet_dist.log_prob(actions[:, 1] - 1)\n",
    "            new_log_probs = new_banker_log_prob + new_bet_log_prob\n",
    "\n",
    "            # 確保 log_probs 不為 NaN\n",
    "            new_log_probs = torch.nan_to_num(new_log_probs, nan=0.0)\n",
    "\n",
    "            value_estimates = self.value(states).view(-1)\n",
    "            value_estimates = torch.nan_to_num(value_estimates, nan=0.0)\n",
    "\n",
    "            advantages = returns - value_estimates.detach()\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "            self.optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        state = torch.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        banker_probs, bet_probs = self.policy(state)\n",
    "\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().item()\n",
    "        bet_action = bet_dist.sample().item() + 1\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action - 1))\n",
    "\n",
    "        return [banker_action, bet_action], banker_log_prob.item(), bet_log_prob.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter \n",
    "# 訓練超參數\n",
    "num_episodes = 10000  # 訓練回合數\n",
    "batch_size = 64       # 每次更新時使用的數據批次\n",
    "gamma = 0.99          # 折扣因子\n",
    "clip_epsilon = 0.2    # PPO clip 範圍\n",
    "lr = 3e-4             # 學習率\n",
    "update_epochs = 10    # 每次更新的迭代次數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "banker_action_dim = 5  # 搶莊倍率 (0~4)\n",
    "bet_action_dim = 5  # 下注倍率 (1~5)\n",
    "\n",
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_22692\\2750752085.py:59: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(value_estimates, returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward: -0.6\n",
      "Episode 1: Total Reward: 0.6\n",
      "Episode 2: Total Reward: -2.0\n",
      "Episode 3: Total Reward: -1.2\n",
      "Episode 4: Total Reward: 1.8\n",
      "Episode 5: Total Reward: 4.0\n",
      "Episode 6: Total Reward: 0.8\n",
      "Episode 7: Total Reward: -0.8\n",
      "Episode 8: Total Reward: -1.8\n",
      "Episode 9: Total Reward: -3.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10  # 設定訓練回合數\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.policy.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)  # 這樣才符合修改後的 step\n",
    "\n",
    "        # 記錄數據\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append([banker_log_prob.item(), bet_log_prob.item()])\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state  # 更新 state\n",
    "\n",
    "    # 更新 PPO\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # 每 100 回合顯示一次訓練結果\n",
    "    # if episode % 100 == 0:\n",
    "    print(f\"Episode {episode}: Total Reward: {sum(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10  # 測試訓練回合數\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "clip_epsilon = 0.2\n",
    "lr = 3e-4\n",
    "update_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "banker_action_dim = 5  # 搶莊倍率 (0~4)\n",
    "bet_action_dim = 5  # 下注倍率 (1~5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -0.4\n",
      "Episode 2: Total Reward = -0.8\n",
      "Episode 3: Total Reward = 1.0\n",
      "Episode 4: Total Reward = -0.6\n",
      "Episode 5: Total Reward = 2.0\n",
      "Episode 6: Total Reward = 3.0\n",
      "Episode 7: Total Reward = 0.4\n",
      "Episode 8: Total Reward = -1.2\n",
      "Episode 9: Total Reward = -2.0\n",
      "Episode 10: Total Reward = -1.2\n"
     ]
    }
   ],
   "source": [
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim)\n",
    "\n",
    "# 訓練\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.policy.select_action(state)\n",
    "\n",
    "        # 確保動作格式正確\n",
    "        banker_action, bet_action = action\n",
    "        next_state, reward, done, _ = env.step((banker_action, bet_action))  # 這樣才符合修改後的 step\n",
    "\n",
    "        # 記錄數據\n",
    "        states.append(state)\n",
    "        actions.append([banker_action, bet_action])  # 確保 actions 格式正確\n",
    "        log_probs.append([banker_log_prob.detach().item(), bet_log_prob.detach().item()])\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # 更新 PPO\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # 顯示訓練結果\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A）：\n",
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "🤖 模型預測的搶莊倍率: 3\n",
      "🤖 模型建議的下注倍率: 5\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A）：\n",
      "❌ 請確保輸入 4 張手牌的花色與數值！\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A）：\n",
      "❌ 請確保輸入 4 張手牌的花色與數值！\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A）：\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 發生錯誤: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 測試模型\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtest_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_agent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36mtest_trained_model\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 讓使用者輸入 4 張手牌\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m請輸入 4 張手牌（格式：heart J diamond 10 club J spade A）：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m input_cards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_cards) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 請確保輸入 4 張手牌的花色與數值！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_trained_model(env, agent):\n",
    "    \"\"\"\n",
    "    使用訓練好的模型，讓使用者輸入 4 張手牌，並讓模型決策搶莊與下注倍率\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # 取得環境的初始狀態，確保 state 維度正確\n",
    "            state = env.reset()\n",
    "\n",
    "            # 讓使用者輸入 4 張手牌\n",
    "            print(\"\\n請輸入 4 張手牌（格式：heart J diamond 10 club J spade A）：\")\n",
    "            input_cards = input().split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"❌ 請確保輸入 4 張手牌的花色與數值！\")\n",
    "                continue\n",
    "\n",
    "            # 解析輸入的手牌\n",
    "            player_hand = [(input_cards[i], input_cards[i+1]) for i in range(0, 8, 2)]\n",
    "            print(f\"🎴 你的手牌: {player_hand}\")\n",
    "\n",
    "            # 替換 state 前 4 個數值（確保其他狀態資訊保持不變）\n",
    "            for i in range(4):\n",
    "                state[i] = card_value(player_hand[i])\n",
    "\n",
    "            # 轉換為 PyTorch Tensor\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # 加 batch 維度\n",
    "\n",
    "            # 模型預測搶莊倍率\n",
    "            with torch.no_grad():\n",
    "                (banker_action, bet_action), _, _ = agent.select_action(state_tensor)\n",
    "\n",
    "            print(f\"🤖 模型預測的搶莊倍率: {banker_action}\")\n",
    "\n",
    "            # 讓使用者輸入是否成功搶莊\n",
    "            is_banker = input(\"✅ 是否搶到莊？ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"🎉 你是莊家！不需要下注\")\n",
    "            else:\n",
    "                print(f\"🤖 模型建議的下注倍率: {bet_action}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 發生錯誤: {e}\")\n",
    "\n",
    "# 測試模型\n",
    "test_trained_model(env, ppo_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO 價值網絡\n",
    "# class ValueNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(ValueNetwork, self).__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.LayerNorm(64),  # 增加 LayerNorm\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.LayerNorm(64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64, 1)  # 直接輸出數值\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "\n",
    "# class PolicyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(PolicyNetwork, self).__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, output_dim),\n",
    "#             nn.Softmax(dim=-1)  # 選擇行動\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.value = ValueNetwork(state_dim)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    def train(self, states, actions, log_probs, rewards):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        values = self.value(states).squeeze()\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        new_probs = self.policy(states)\n",
    "        new_dist = Categorical(new_probs)\n",
    "        new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip)\n",
    "        loss_policy = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "        loss_value = (returns - values).pow(2).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        loss_policy.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        self.value_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終推論\n",
    "class NiuNiuDecisionHelper:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def decide_action(self, hand):\n",
    "        state = np.array([card_value(card) for card in hand], dtype=np.float32)\n",
    "        action, _ = self.agent.select_action(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 6x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 14\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     16\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 6x64)"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(state_dim=6, action_dim=10)\n",
    "\n",
    "# 訓練\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        state = new_state\n",
    "\n",
    "    agent.train(states, actions, log_probs, rewards)\n",
    "\n",
    "helper = NiuNiuDecisionHelper(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 確保異常檢測開啟\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=1e-3, gamma=0.99, clip_epsilon=0.2):\n",
    "        self.policy_network = PolicyNetwork(input_dim, action_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.policy_network(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, log_prob, next_state):\n",
    "        self.memory.append((state, action, reward, log_prob, next_state))\n",
    "\n",
    "    def train(self):\n",
    "        if not self.memory:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, log_probs, next_states = zip(*self.memory)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "\n",
    "        # 計算新的 log_prob\n",
    "        new_probs = self.policy_network(states)\n",
    "        new_dist = torch.distributions.Categorical(new_probs)\n",
    "        new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "        # 計算 ratio (並確保不會 in-place 操作)\n",
    "        ratio = torch.exp(new_log_probs - log_probs.detach())\n",
    "\n",
    "        # 計算 advantage\n",
    "        advantages = rewards - rewards.mean()\n",
    "\n",
    "        # PPO 損失函數\n",
    "        unclipped = ratio * advantages\n",
    "        clipped = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "        # 更新策略網路\n",
    "        self.optimizer_policy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer_policy.step()\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "\n",
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=len(env.get_state()), action_dim=10)  # 確保動作空間是 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -12\n",
      "Episode 1, Total Reward: 8\n",
      "Episode 2, Total Reward: -12\n",
      "Episode 3, Total Reward: -24\n",
      "Episode 4, Total Reward: -16\n",
      "Episode 5, Total Reward: -12\n",
      "Episode 6, Total Reward: -8\n",
      "Episode 7, Total Reward: 8\n",
      "Episode 8, Total Reward: 4\n",
      "Episode 9, Total Reward: 16\n",
      "Episode 10, Total Reward: -4\n",
      "Episode 11, Total Reward: -8\n",
      "Episode 12, Total Reward: -16\n",
      "Episode 13, Total Reward: 4\n",
      "Episode 14, Total Reward: -60\n",
      "Episode 15, Total Reward: 8\n",
      "Episode 16, Total Reward: 3\n",
      "Episode 17, Total Reward: 5\n",
      "Episode 18, Total Reward: -48\n",
      "Episode 19, Total Reward: -16\n",
      "Episode 20, Total Reward: 6\n",
      "Episode 21, Total Reward: -8\n",
      "Episode 22, Total Reward: 24\n",
      "Episode 23, Total Reward: -4\n",
      "Episode 24, Total Reward: 8\n",
      "Episode 25, Total Reward: 4\n",
      "Episode 26, Total Reward: -4\n",
      "Episode 27, Total Reward: 20\n",
      "Episode 28, Total Reward: -24\n",
      "Episode 29, Total Reward: -4\n",
      "Episode 30, Total Reward: -8\n",
      "Episode 31, Total Reward: 4\n",
      "Episode 32, Total Reward: -2\n",
      "Episode 33, Total Reward: -4\n",
      "Episode 34, Total Reward: 12\n",
      "Episode 35, Total Reward: -8\n",
      "Episode 36, Total Reward: 12\n",
      "Episode 37, Total Reward: -16\n",
      "Episode 38, Total Reward: -8\n",
      "Episode 39, Total Reward: -4\n",
      "Episode 40, Total Reward: -8\n",
      "Episode 41, Total Reward: 4\n",
      "Episode 42, Total Reward: 12\n",
      "Episode 43, Total Reward: -24\n",
      "Episode 44, Total Reward: 12\n",
      "Episode 45, Total Reward: -6\n",
      "Episode 46, Total Reward: -24\n",
      "Episode 47, Total Reward: 1\n",
      "Episode 48, Total Reward: 12\n",
      "Episode 49, Total Reward: -8\n",
      "Episode 50, Total Reward: 1\n",
      "Episode 51, Total Reward: -4\n",
      "Episode 52, Total Reward: -32\n",
      "Episode 53, Total Reward: 12\n",
      "Episode 54, Total Reward: 6\n",
      "Episode 55, Total Reward: -3\n",
      "Episode 56, Total Reward: -8\n",
      "Episode 57, Total Reward: -8\n",
      "Episode 58, Total Reward: 24\n",
      "Episode 59, Total Reward: 8\n",
      "Episode 60, Total Reward: -24\n",
      "Episode 61, Total Reward: 16\n",
      "Episode 62, Total Reward: -4\n",
      "Episode 63, Total Reward: 12\n",
      "Episode 64, Total Reward: -4\n",
      "Episode 65, Total Reward: -8\n",
      "Episode 66, Total Reward: -80\n",
      "Episode 67, Total Reward: 8\n",
      "Episode 68, Total Reward: 3\n",
      "Episode 69, Total Reward: 40\n",
      "Episode 70, Total Reward: 36\n",
      "Episode 71, Total Reward: -36\n",
      "Episode 72, Total Reward: 8\n",
      "Episode 73, Total Reward: -4\n",
      "Episode 74, Total Reward: 12\n",
      "Episode 75, Total Reward: -12\n",
      "Episode 76, Total Reward: 12\n",
      "Episode 77, Total Reward: -12\n",
      "Episode 78, Total Reward: 2\n",
      "Episode 79, Total Reward: -2\n",
      "Episode 80, Total Reward: -9\n",
      "Episode 81, Total Reward: 40\n",
      "Episode 82, Total Reward: 4\n",
      "Episode 83, Total Reward: 20\n",
      "Episode 84, Total Reward: -8\n",
      "Episode 85, Total Reward: -24\n",
      "Episode 86, Total Reward: -8\n",
      "Episode 87, Total Reward: 12\n",
      "Episode 88, Total Reward: 8\n",
      "Episode 89, Total Reward: 12\n",
      "Episode 90, Total Reward: -8\n",
      "Episode 91, Total Reward: -12\n",
      "Episode 92, Total Reward: -8\n",
      "Episode 93, Total Reward: 8\n",
      "Episode 94, Total Reward: -48\n",
      "Episode 95, Total Reward: 12\n",
      "Episode 96, Total Reward: -24\n",
      "Episode 97, Total Reward: -24\n",
      "Episode 98, Total Reward: 12\n",
      "Episode 99, Total Reward: -4\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100  # 訓練 10,000 場遊戲\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, log_prob, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.train()  # 更新 PPO\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # if episode % 100 == 0:\n",
    "    #     print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# everage : 22.5s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOAgent' object has no attribute 'policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# **使用確定性策略來選擇動作**\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdeterministic_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 解讀動作\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mdeterministic_action\u001b[1;34m(agent, state)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# 停止梯度計算，加快運算速度\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 轉換為 tensor\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m(state_tensor)  \u001b[38;5;66;03m# 取得動作機率分布\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_probs)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# 選擇機率最高的動作\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PPOAgent' object has no attribute 'policy'"
     ]
    }
   ],
   "source": [
    "# 確保 torch 和其他相關庫已匯入\n",
    "import torch\n",
    "\n",
    "# 定義一個確定性動作選擇函數\n",
    "def deterministic_action(agent, state):\n",
    "    with torch.no_grad():  # 停止梯度計算，加快運算速度\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)  # 轉換為 tensor\n",
    "        action_probs = agent.policy(state_tensor)  # 取得動作機率分布\n",
    "        action = torch.argmax(action_probs).item()  # 選擇機率最高的動作\n",
    "    return action\n",
    "\n",
    "# 假設環境已初始化\n",
    "env = NiuNiuEnv()\n",
    "\n",
    "# 假設你已經訓練好 PPO Agent\n",
    "agent = PPOAgent(input_dim=env.get_state().shape[0], action_dim=10)\n",
    "\n",
    "# 給定特定的手牌\n",
    "my_hand = [('diamond', 'K'), ('diamond', '9'), ('diamond', '6'), ('club', '4')]\n",
    "\n",
    "# 取得手牌對應的環境狀態\n",
    "state = env.get_state()\n",
    "\n",
    "# **使用確定性策略來選擇動作**\n",
    "action = deterministic_action(agent, state)\n",
    "\n",
    "# 解讀動作\n",
    "if action < 5:\n",
    "    print(f\"AI 決定搶莊，倍率為 {action + 1} 倍\")\n",
    "else:\n",
    "    print(f\"AI 決定下注，倍率為 {action - 4} 倍\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiuNiuDecisionHelper:\n",
    "    def __init__(self, agent, env):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.suit_map = {'diamond': 0, 'club': 1, 'heart': 2, 'spade': 3}\n",
    "        self.rank_map = {'J': 11, 'Q': 12, 'K': 13, 'A': 14}\n",
    "        self.rank_map.update({str(i): i for i in range(2, 11)})\n",
    "\n",
    "    def preprocess_state(self, hand_cards):\n",
    "        \"\"\"\n",
    "        將手牌轉換為數值型 NumPy 陣列\n",
    "        :param hand_cards: 玩家的手牌 (ex: [('diamond', '7'), ('diamond', '9')])\n",
    "        :return: 數值型 NumPy 陣列\n",
    "        \"\"\"\n",
    "        numerical_hand = []\n",
    "        for suit, rank in hand_cards:\n",
    "            suit_num = self.suit_map[suit]\n",
    "            rank_num = self.rank_map[rank]\n",
    "            numerical_hand.extend([suit_num, rank_num])  \n",
    "        return np.array(numerical_hand, dtype=np.float32)\n",
    "\n",
    "    def decide_qiangzhuang(self, hand_cards):\n",
    "        \"\"\"\n",
    "        根據手牌決定是否搶莊，並提供搶莊倍率 (1~4 倍) 或 不搶時下注倍率 (1~5 倍)\n",
    "        \"\"\"\n",
    "        processed_state = self.preprocess_state(hand_cards)\n",
    "        action, _ = self.agent.select_action(processed_state)\n",
    "\n",
    "        if action < 4:  # 0~3 搶莊 (倍率 1~4)\n",
    "            return True, action + 1\n",
    "        else:  # 4~9 不搶莊 (倍率 1~5)\n",
    "            return False, action - 3\n",
    "\n",
    "    def compute_banker_loss(self, hand_cards, final_banker, qiangzhuang_multiplier):\n",
    "        \"\"\"\n",
    "        計算搶莊失敗時應該下注的倍率\n",
    "        :param hand_cards: 玩家的手牌\n",
    "        :param final_banker: 是否成為莊家 (True/False)\n",
    "        :param qiangzhuang_multiplier: 當初搶莊時的倍率\n",
    "        :return: 搶莊失敗時，應該下注的倍率 (1~5 倍)，若成功成為莊家則回傳 None\n",
    "        \"\"\"\n",
    "        if final_banker:\n",
    "            return None  # 成功當莊，不用算賠率\n",
    "\n",
    "        # **使用 AI 來決定下注倍率**\n",
    "        processed_state = self.preprocess_state(hand_cards)\n",
    "        action, _ = self.agent.select_action(processed_state)\n",
    "\n",
    "        if action >= 4:  # 4~9 代表下注 (對應倍率 1~5)\n",
    "            return action - 3\n",
    "        else:\n",
    "            return 1  # 預設為最低倍率 1 倍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "應該搶莊，建議倍率: 3 倍\n",
      "搶莊失敗，AI 建議下注倍率: 1 倍\n"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=8, action_dim=10)  # 輸入 8 維，輸出 10 個動作\n",
    "decision_helper = NiuNiuDecisionHelper(agent, env)\n",
    "\n",
    "hand_cards = [('diamond', 'J'), ('diamond', 'K'), ('diamond', '10'), ('club', '4')]\n",
    "should_qiang, multiplier = decision_helper.decide_qiangzhuang(hand_cards)\n",
    "\n",
    "if should_qiang:\n",
    "    print(f\"應該搶莊，建議倍率: {multiplier} 倍\")\n",
    "    final_banker = bool(int(input(\"最後是否成功成為莊家？(1: 是, 0: 否): \")))\n",
    "    banker_loss = decision_helper.compute_banker_loss(hand_cards, final_banker, multiplier)\n",
    "    if banker_loss:\n",
    "        print(f\"搶莊失敗，AI 建議下注倍率: {banker_loss} 倍\")\n",
    "else:\n",
    "    print(f\"不搶莊，建議下注倍率: {multiplier} 倍\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 代理人\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=0.002, gamma=0.99, epsilon=0.2, update_steps=5):\n",
    "        self.policy = PolicyNetwork(input_dim, action_dim)  # ✅ 確保這裡有 PolicyNetwork\n",
    "        self.value = ValueNetwork(input_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.update_steps = update_steps\n",
    "        self.memory = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\" 使用策略網絡選擇行動 \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # 加維度 (batch=1)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, next_state):\n",
    "        \"\"\" 存儲交互數據 \"\"\"\n",
    "        self.memory.append((state, action, reward, log_prob, next_state))\n",
    "\n",
    "    def compute_discounted_rewards(self, rewards):\n",
    "        \"\"\" 計算折扣回報 G_t \"\"\"\n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "        return torch.FloatTensor(discounted_rewards)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" 使用 PPO 來訓練策略網絡和價值網絡 \"\"\"\n",
    "        if len(self.memory) == 0:\n",
    "            return\n",
    "        \n",
    "        # 1. 解析記憶\n",
    "        states, actions, rewards, log_probs, next_states = zip(*self.memory)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        rewards = self.compute_discounted_rewards(rewards)\n",
    "        \n",
    "        # 2. 計算優勢值 Advantage = G_t - V(s)\n",
    "        values = self.value(states).squeeze()\n",
    "        advantages = rewards - values.detach().clone()  # 使用 clone() 來避免原地修改\n",
    "\n",
    "        # 3. 更新策略網絡 (PPO Loss)\n",
    "        for _ in range(self.update_steps):  # 重複多次更新\n",
    "            probs = self.policy(states)\n",
    "            dist = Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - log_probs)  # 重要性權重\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "            policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "            \n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)  # 確保計算圖不會被釋放\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "        # 4. 更新價值網絡 (MSE 損失)\n",
    "        value_loss = (self.value(states).squeeze() - rewards).pow(2).mean()\n",
    "        self.optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optimizer_value.step()\n",
    "\n",
    "        # 5. 清空記憶\n",
    "        self.memory = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     18\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# 確保 reward 是數字\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 訓練一次\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [116]\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m     policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmin(ratio \u001b[38;5;241m*\u001b[39m advantages, clipped_ratio \u001b[38;5;241m*\u001b[39m advantages)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 確保計算圖不會被釋放\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 4. 更新價值網絡 (MSE 損失)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=len(env.get_state()), action_dim=10)\n",
    "\n",
    "num_episodes = 10  # 進行 10 次訓練\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)  # 注意這裡解構返回值\n",
    "        \n",
    "        agent.store_transition(state, action, reward, log_prob, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward  # 確保 reward 是數字\n",
    "\n",
    "    agent.train()  # 訓練一次\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# 最後的訓練結果\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 12\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 呼叫 step() 並獲得下一步狀態和獎勳\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)"
     ]
    }
   ],
   "source": [
    "# 訓練 PPO\n",
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=4, action_dim=10)\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    memory = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        \n",
    "        # 呼叫 step() 並獲得下一步狀態和獎勳\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        memory.append((state, action, reward, log_prob, next_state))\n",
    "        state = next_state\n",
    "    \n",
    "    agent.train(memory)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Last Reward: {reward}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 測試 1: 使用代理選擇行為\u001b[39;00m\n\u001b[0;32m      8\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 9\u001b[0m action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChosen Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Log Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_prob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 測試 2: 執行一步遊戲並查看結果\u001b[39;00m\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)"
     ]
    }
   ],
   "source": [
    "# 測試主程式\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化遊戲環境和代理\n",
    "    env = NiuNiuEnv()\n",
    "    agent = PPOAgent(input_dim=4, action_dim=10)\n",
    "    \n",
    "    # 測試 1: 使用代理選擇行為\n",
    "    state = env.reset()\n",
    "    action, log_prob = agent.select_action(state)\n",
    "    print(f\"Chosen Action: {action}, Log Probability: {log_prob}\")\n",
    "    \n",
    "    # 測試 2: 執行一步遊戲並查看結果\n",
    "    is_banker = (action < 5)  # 假設選擇 0-4 為搶莊，5-9 為下注\n",
    "    next_state, reward, done = env.step(action, is_banker)\n",
    "    print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設模型已經訓練完成並保存在 agent 中\n",
    "def get_action_from_model(hand, agent, is_banker=False):\n",
    "    \"\"\"\n",
    "    根據手牌與模型決策是否搶莊，並在搶莊與否時決定下注策略。\n",
    "    hand: 玩家手牌\n",
    "    agent: 訓練好的PPOAgent\n",
    "    is_banker: 是否為莊家，True表示是莊家，False表示閒家\n",
    "    \"\"\"\n",
    "    # 將手牌轉換為模型的狀態向量\n",
    "    state = np.array([card_value(card) for card in hand], dtype=np.float32)  # 轉換為數字狀態\n",
    "    \n",
    "    # 如果是閒家，先決定是否搶莊（選擇動作）\n",
    "    if not is_banker:\n",
    "        action, _ = agent.select_action(state)  # 根據狀態選擇動作\n",
    "        if action < 5:  # 如果選擇的動作是搶莊（0-4表示搶莊）\n",
    "            is_banker = True\n",
    "            print(\"決定搶莊！\")\n",
    "        else:\n",
    "            print(\"決定不搶莊，選擇下注。\")\n",
    "    \n",
    "    # 根據是否搶莊來決定下注\n",
    "    if is_banker:\n",
    "        # 如果是莊家，決定下注策略\n",
    "        action, _ = agent.select_action(state)  # 莊家可以下注的動作範圍是 0-4\n",
    "        print(f\"作為莊家，下注倍率為 {action % 5 + 1}\")\n",
    "    else:\n",
    "        # 如果是閒家，根據手牌決定下注\n",
    "        action, _ = agent.select_action(state)  # 閒家的下注動作範圍是 5-9\n",
    "        print(f\"作為閒家，下注倍率為 {action % 5 + 1}\")\n",
    "\n",
    "    return action\n",
    "\n",
    "# 假設我們有一副手牌\n",
    "player_hand = ['3♠', '7♣', 'K♦', '9♥']  # 玩家手牌\n",
    "\n",
    "# 使用訓練完成的模型來決定策略\n",
    "get_action_from_model(player_hand, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "player_hand = [('heart', '9'), ('diamond', 'J'), ('club', '3'), ('spade', '6'), ('heart', '2')]\n",
    "banker_hand = [('spade', '10'), ('club', 'J'), ('heart', '4'), ('diamond', '6'), ('diamond', '2')]\n",
    "print(type(calculate_payout(player_hand, banker_hand, verbose=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
