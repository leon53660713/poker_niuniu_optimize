{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’çš„æ–¹æ³•æ”¹å–„ç­–ç•¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niuniu_func import *\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build niuniu env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env of niuniu\n",
    "# set myself as player 0\n",
    "class NiuNiuEnv:\n",
    "    # init \n",
    "    def __init__(self):\n",
    "        # generate deck\n",
    "        self.deck = self.generate_deck()\n",
    "        # generate player == 4\n",
    "        self.players = [[] for _ in range(4)]\n",
    "        self.banker_index = -1\n",
    "        # banker multiplier\n",
    "        self.banker_multiplier = 1\n",
    "        # bet number\n",
    "        self.bets = [0, 0, 0, 0]\n",
    "        # generate state\n",
    "        self.state = None\n",
    "        # state, 0: bank step, 1: bet step, 2: result step\n",
    "        self.current_phase = 0\n",
    "        # reset\n",
    "        self.reset()\n",
    "\n",
    "    # generate deck\n",
    "    def generate_deck(self):\n",
    "        suits = ['heart', 'spade', 'diamond', 'club']\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "        return [(suit, rank) for suit in suits for rank in ranks]\n",
    "\n",
    "    # reset\n",
    "    def reset(self):\n",
    "        # regenerate deck & shuffle\n",
    "        self.deck = self.generate_deck()\n",
    "        random.shuffle(self.deck)\n",
    "        # every player have 4 cards\n",
    "        self.players = [[self.deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "        # init bets\n",
    "        self.bets = [0] * 4\n",
    "        self.banker_index = -1\n",
    "        self.banker_multiplier = 1\n",
    "\n",
    "        # init step\n",
    "        self.current_phase = 0\n",
    "\n",
    "        # reload state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "    \n",
    "    # get myself's hand number\n",
    "    def get_state(self):\n",
    "        # myself's hand\n",
    "        state = [card_value(card) for card in self.players[0]]\n",
    "        # which step\n",
    "        state.append(self.current_phase)\n",
    "        # who is banker\n",
    "        state.append(self.banker_index)\n",
    "        # banker multiplayer\n",
    "        state.append(self.banker_multiplier)\n",
    "        # every player's bet\n",
    "        state.extend(self.bets)\n",
    "\n",
    "        # ensure banker_bid & bet are init\n",
    "        if self.current_phase > 0:\n",
    "            state.append(self.banker_bid)\n",
    "        else:\n",
    "            # init baker_bid value\n",
    "            state.append(0)\n",
    "        # player bet\n",
    "        state.append(self.bets[0])\n",
    "\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    # step    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: [banker_action, bet_action]\n",
    "        banker_action: 0-4 is baker multi\n",
    "        bet_action: 1-5 is bet multi\n",
    "        \"\"\"\n",
    "        # unpack action\n",
    "        banker_action, bet_action = action \n",
    "        # banker\n",
    "        self.banker_bid = banker_action\n",
    "        # bet\n",
    "        self.bet_amount = bet_action\n",
    "\n",
    "        \"\"\"\n",
    "        step 1 : decide whether to get banker\n",
    "            * myself : by ppo agent\n",
    "            * others : by simulate_ev to decide\n",
    "        \"\"\"\n",
    "        bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        bank_multipliers[0] = self.banker_bid\n",
    "        # run time : 22s\n",
    "\n",
    "        \"\"\"\n",
    "        step 2 : decide final banker(the max multiplier)\n",
    "            * if all not want to be banker, random choose one & set multiplier = 1\n",
    "            * if more than one have same multiplier, random choose one\n",
    "        \"\"\"\n",
    "        max_bet = max(bank_multipliers)\n",
    "        if max_bet == 1:\n",
    "            random_banker = random.choice(range(4))\n",
    "            bank_multipliers[random_banker] = 1\n",
    "        banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "        self.banker_index = random.choice(banker_candidates)\n",
    "        self.banker_multiplier = max_bet\n",
    "        banker_hand = self.players[self.banker_index]\n",
    "\n",
    "        # whether myself is banker\n",
    "        is_banker = (self.banker_index == 0)\n",
    "\n",
    "        # go to next action -- bet\n",
    "        self.current_phase = 1\n",
    "\n",
    "        # bet action\n",
    "        if is_banker:\n",
    "            \"\"\"\n",
    "            step 3 : if myself is banker\n",
    "                * I don't need to bet\n",
    "                * others use `calculate_ev_against_banker` to bet, besides\n",
    "                if banker multiplier over 3, we assume banker have niu\n",
    "            \"\"\"\n",
    "            self.bets[0] = self.bet_amount\n",
    "            for i in range(1, 4):\n",
    "                have_niu = self.banker_multiplier >= 3\n",
    "                self.bets[i] = calculate_ev_against_banker(self.players[i], 100000, have_niu)[1]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            step 4 : if myself is not banker\n",
    "                * let ppo decide bet\n",
    "                * others we don't care\n",
    "            \"\"\"\n",
    "            self.bets[0] = max(1, min(5, action[1]))\n",
    "\n",
    "        \"\"\"\n",
    "        step 5 : add the 5th card to every player\n",
    "        \"\"\"\n",
    "        for i in range(4):\n",
    "            self.players[i].append(self.deck.pop())\n",
    "\n",
    "        # go to next action -- result\n",
    "        self.current_phase = 2\n",
    "\n",
    "        \"\"\"\n",
    "        step 6 : caculate ev of myself\n",
    "            * I am banker : caculate payout of the sum of me against others(use negative)\n",
    "            * I am not banker : calculate the payout against the banker\n",
    "        \"\"\"\n",
    "        if is_banker:\n",
    "            # I am banker\n",
    "            total_payout = -sum(\n",
    "                calculate_payout(self.players[i], banker_hand, False) * self.bets[i] * self.banker_multiplier\n",
    "                for i in range(4) if i != self.banker_index\n",
    "            )\n",
    "        else:\n",
    "            # I am not banker\n",
    "            total_payout = calculate_payout(self.players[0], banker_hand, False) * self.bets[0] * self.banker_multiplier\n",
    "\n",
    "        \"\"\"\n",
    "        step 7 : caculate reward(scaling & punishing)\n",
    "        \"\"\"\n",
    "        max_possible_loss = 5 * self.banker_multiplier\n",
    "        reward = total_payout / max_possible_loss\n",
    "        # have punish value for being the banker\n",
    "        if is_banker and total_payout > 0:\n",
    "            reward += 0.2\n",
    "        if is_banker and total_payout < 0:\n",
    "            reward -= 0.2\n",
    "\n",
    "        \"\"\"\n",
    "        step 8 : finish one round\n",
    "        \"\"\"\n",
    "        done = True\n",
    "\n",
    "        \"\"\"\n",
    "        step 9 : reset\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple test\n",
    "test whether niuniu env is runnable <br>\n",
    "to avoid getting error of having NaN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [ 3. 10.  2. 10.  0. -1.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Step 0 - State: [ 4. 10.  9.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.8\n",
      "Step 1 - State: [10.  1.  8.  8.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 3.0\n",
      "Step 2 - State: [ 6. 10.  7.  6.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.8\n",
      "Step 3 - State: [ 7.  7. 10.  8.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.2\n",
      "Step 4 - State: [10. 10.  8.  7.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.8\n",
      "Step 5 - State: [10.  2.  5.  9.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 0.0\n",
      "Step 6 - State: [ 6.  2. 10.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -2.0\n",
      "Step 7 - State: [10. 10.  5.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.2\n",
      "Step 8 - State: [ 8.  1.  7.  8.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.8\n",
      "Step 9 - State: [10.  1.  2.  7.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -3.2\n"
     ]
    }
   ],
   "source": [
    "# test env of niuniu\n",
    "def test_env():\n",
    "    env = NiuNiuEnv()\n",
    "    state = env.reset()\n",
    "    print(\"Initial State:\", state)\n",
    "    # test 10 times\n",
    "    for i in range(10):\n",
    "        # random action\n",
    "        action = [np.random.randint(0, 5), np.random.randint(1, 6)]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if np.isnan(state).any():\n",
    "            print(f\"NaN detected in state at step {i}!\")\n",
    "        if np.isnan(reward):\n",
    "            print(f\"NaN detected in reward at step {i}!\")\n",
    "        print(f\"Step {i} - State: {state}, Reward: {reward}\")\n",
    "\n",
    "test_env()\n",
    "# run time : 3m 52s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO åƒ¹å€¼ç¶²çµ¡ (V(s))\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),  # é¿å…æ¨™æº–åŒ–æ™‚å‡ºç¾ NaN\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1)  # è¼¸å‡º V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO ç­–ç•¥ç¶²çµ¡\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.banker_fc = nn.Linear(128, output_dim1)  # æ¶èŠå‹•ä½œ\n",
    "        self.bet_fc = nn.Linear(128, output_dim2)  # ä¸‹æ³¨å‹•ä½œ\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        banker_logits = self.banker_fc(x)\n",
    "        bet_logits = self.bet_fc(x)\n",
    "\n",
    "        # æª¢æŸ¥ NaN ä¸¦ä¿®æ­£\n",
    "        banker_logits = torch.nan_to_num(banker_logits, nan=0.0)\n",
    "        bet_logits = torch.nan_to_num(bet_logits, nan=0.0)\n",
    "\n",
    "        banker_probs = F.softmax(banker_logits, dim=-1)\n",
    "        bet_probs = F.softmax(bet_logits, dim=-1)\n",
    "\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)  # ç¢ºä¿æ¦‚ç‡ä¸ç‚º NaN\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        return banker_probs, bet_probs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # è½‰æˆ batch\n",
    "        banker_probs, bet_probs = self.forward(state)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample()\n",
    "        bet_action = bet_dist.sample() + 1  # ä¸‹æ³¨å€ç‡æ˜¯ 1~5ï¼Œæ‰€ä»¥è¦ +1\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(banker_action)\n",
    "        bet_log_prob = bet_dist.log_prob(bet_action - 1)\n",
    "\n",
    "        return (banker_action.item(), bet_action.item()), banker_log_prob, bet_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=10):\n",
    "        self.policy = PolicyNetwork(input_dim, output_dim1, output_dim2)\n",
    "        self.value = ValueNetwork(input_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            if d:\n",
    "                R = 0\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # é¿å…æ¨™æº–åŒ–æ™‚å‡ºç¾ NaNï¼ˆreturns.std() å¯èƒ½ç‚º 0ï¼‰\n",
    "        return (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, dones):\n",
    "        returns = self.compute_returns(rewards, dones)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        old_log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            banker_probs, bet_probs = self.policy(states)\n",
    "\n",
    "            banker_probs = torch.nan_to_num(banker_probs, nan=0.0)\n",
    "            bet_probs = torch.nan_to_num(bet_probs, nan=0.0)\n",
    "\n",
    "            banker_dist = Categorical(banker_probs)\n",
    "            bet_dist = Categorical(bet_probs)\n",
    "\n",
    "            new_banker_log_prob = banker_dist.log_prob(actions[:, 0])\n",
    "            new_bet_log_prob = bet_dist.log_prob(actions[:, 1] - 1)\n",
    "            new_log_probs = new_banker_log_prob + new_bet_log_prob\n",
    "\n",
    "            # ç¢ºä¿ log_probs ä¸ç‚º NaN\n",
    "            new_log_probs = torch.nan_to_num(new_log_probs, nan=0.0)\n",
    "\n",
    "            value_estimates = self.value(states).view(-1)\n",
    "            value_estimates = torch.nan_to_num(value_estimates, nan=0.0)\n",
    "\n",
    "            advantages = returns - value_estimates.detach()\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "            self.optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        state = torch.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        banker_probs, bet_probs = self.policy(state)\n",
    "\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().item()\n",
    "        bet_action = bet_dist.sample().item() + 1\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action - 1))\n",
    "\n",
    "        return [banker_action, bet_action], banker_log_prob.item(), bet_log_prob.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter \n",
    "# è¨“ç·´è¶…åƒæ•¸\n",
    "num_episodes = 10000  # è¨“ç·´å›åˆæ•¸\n",
    "batch_size = 64       # æ¯æ¬¡æ›´æ–°æ™‚ä½¿ç”¨çš„æ•¸æ“šæ‰¹æ¬¡\n",
    "gamma = 0.99          # æŠ˜æ‰£å› å­\n",
    "clip_epsilon = 0.2    # PPO clip ç¯„åœ\n",
    "lr = 3e-4             # å­¸ç¿’ç‡\n",
    "update_epochs = 10    # æ¯æ¬¡æ›´æ–°çš„è¿­ä»£æ¬¡æ•¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "banker_action_dim = 5  # æ¶èŠå€ç‡ (0~4)\n",
    "bet_action_dim = 5  # ä¸‹æ³¨å€ç‡ (1~5)\n",
    "\n",
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_22692\\2750752085.py:59: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(value_estimates, returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward: -0.6\n",
      "Episode 1: Total Reward: 0.6\n",
      "Episode 2: Total Reward: -2.0\n",
      "Episode 3: Total Reward: -1.2\n",
      "Episode 4: Total Reward: 1.8\n",
      "Episode 5: Total Reward: 4.0\n",
      "Episode 6: Total Reward: 0.8\n",
      "Episode 7: Total Reward: -0.8\n",
      "Episode 8: Total Reward: -1.8\n",
      "Episode 9: Total Reward: -3.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10  # è¨­å®šè¨“ç·´å›åˆæ•¸\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.policy.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)  # é€™æ¨£æ‰ç¬¦åˆä¿®æ”¹å¾Œçš„ step\n",
    "\n",
    "        # è¨˜éŒ„æ•¸æ“š\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append([banker_log_prob.item(), bet_log_prob.item()])\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state  # æ›´æ–° state\n",
    "\n",
    "    # æ›´æ–° PPO\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # æ¯ 100 å›åˆé¡¯ç¤ºä¸€æ¬¡è¨“ç·´çµæœ\n",
    "    # if episode % 100 == 0:\n",
    "    print(f\"Episode {episode}: Total Reward: {sum(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10  # æ¸¬è©¦è¨“ç·´å›åˆæ•¸\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "clip_epsilon = 0.2\n",
    "lr = 3e-4\n",
    "update_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "banker_action_dim = 5  # æ¶èŠå€ç‡ (0~4)\n",
    "bet_action_dim = 5  # ä¸‹æ³¨å€ç‡ (1~5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -0.4\n",
      "Episode 2: Total Reward = -0.8\n",
      "Episode 3: Total Reward = 1.0\n",
      "Episode 4: Total Reward = -0.6\n",
      "Episode 5: Total Reward = 2.0\n",
      "Episode 6: Total Reward = 3.0\n",
      "Episode 7: Total Reward = 0.4\n",
      "Episode 8: Total Reward = -1.2\n",
      "Episode 9: Total Reward = -2.0\n",
      "Episode 10: Total Reward = -1.2\n"
     ]
    }
   ],
   "source": [
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim)\n",
    "\n",
    "# è¨“ç·´\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.policy.select_action(state)\n",
    "\n",
    "        # ç¢ºä¿å‹•ä½œæ ¼å¼æ­£ç¢º\n",
    "        banker_action, bet_action = action\n",
    "        next_state, reward, done, _ = env.step((banker_action, bet_action))  # é€™æ¨£æ‰ç¬¦åˆä¿®æ”¹å¾Œçš„ step\n",
    "\n",
    "        # è¨˜éŒ„æ•¸æ“š\n",
    "        states.append(state)\n",
    "        actions.append([banker_action, bet_action])  # ç¢ºä¿ actions æ ¼å¼æ­£ç¢º\n",
    "        log_probs.append([banker_log_prob.detach().item(), bet_log_prob.detach().item()])\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # æ›´æ–° PPO\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # é¡¯ç¤ºè¨“ç·´çµæœ\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼š\n",
      "ğŸ´ ä½ çš„æ‰‹ç‰Œ: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "ğŸ¤– æ¨¡å‹é æ¸¬çš„æ¶èŠå€ç‡: 3\n",
      "ğŸ¤– æ¨¡å‹å»ºè­°çš„ä¸‹æ³¨å€ç‡: 5\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼š\n",
      "âŒ è«‹ç¢ºä¿è¼¸å…¥ 4 å¼µæ‰‹ç‰Œçš„èŠ±è‰²èˆ‡æ•¸å€¼ï¼\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼š\n",
      "âŒ è«‹ç¢ºä¿è¼¸å…¥ 4 å¼µæ‰‹ç‰Œçš„èŠ±è‰²èˆ‡æ•¸å€¼ï¼\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼š\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ ç™¼ç”ŸéŒ¯èª¤: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# æ¸¬è©¦æ¨¡å‹\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtest_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_agent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36mtest_trained_model\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# è®“ä½¿ç”¨è€…è¼¸å…¥ 4 å¼µæ‰‹ç‰Œ\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mè«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼š\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m input_cards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_cards) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ è«‹ç¢ºä¿è¼¸å…¥ 4 å¼µæ‰‹ç‰Œçš„èŠ±è‰²èˆ‡æ•¸å€¼ï¼\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_trained_model(env, agent):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œè®“ä½¿ç”¨è€…è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼Œä¸¦è®“æ¨¡å‹æ±ºç­–æ¶èŠèˆ‡ä¸‹æ³¨å€ç‡\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # å–å¾—ç’°å¢ƒçš„åˆå§‹ç‹€æ…‹ï¼Œç¢ºä¿ state ç¶­åº¦æ­£ç¢º\n",
    "            state = env.reset()\n",
    "\n",
    "            # è®“ä½¿ç”¨è€…è¼¸å…¥ 4 å¼µæ‰‹ç‰Œ\n",
    "            print(\"\\nè«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼š\")\n",
    "            input_cards = input().split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"âŒ è«‹ç¢ºä¿è¼¸å…¥ 4 å¼µæ‰‹ç‰Œçš„èŠ±è‰²èˆ‡æ•¸å€¼ï¼\")\n",
    "                continue\n",
    "\n",
    "            # è§£æè¼¸å…¥çš„æ‰‹ç‰Œ\n",
    "            player_hand = [(input_cards[i], input_cards[i+1]) for i in range(0, 8, 2)]\n",
    "            print(f\"ğŸ´ ä½ çš„æ‰‹ç‰Œ: {player_hand}\")\n",
    "\n",
    "            # æ›¿æ› state å‰ 4 å€‹æ•¸å€¼ï¼ˆç¢ºä¿å…¶ä»–ç‹€æ…‹è³‡è¨Šä¿æŒä¸è®Šï¼‰\n",
    "            for i in range(4):\n",
    "                state[i] = card_value(player_hand[i])\n",
    "\n",
    "            # è½‰æ›ç‚º PyTorch Tensor\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # åŠ  batch ç¶­åº¦\n",
    "\n",
    "            # æ¨¡å‹é æ¸¬æ¶èŠå€ç‡\n",
    "            with torch.no_grad():\n",
    "                (banker_action, bet_action), _, _ = agent.select_action(state_tensor)\n",
    "\n",
    "            print(f\"ğŸ¤– æ¨¡å‹é æ¸¬çš„æ¶èŠå€ç‡: {banker_action}\")\n",
    "\n",
    "            # è®“ä½¿ç”¨è€…è¼¸å…¥æ˜¯å¦æˆåŠŸæ¶èŠ\n",
    "            is_banker = input(\"âœ… æ˜¯å¦æ¶åˆ°èŠï¼Ÿ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"ğŸ‰ ä½ æ˜¯èŠå®¶ï¼ä¸éœ€è¦ä¸‹æ³¨\")\n",
    "            else:\n",
    "                print(f\"ğŸ¤– æ¨¡å‹å»ºè­°çš„ä¸‹æ³¨å€ç‡: {bet_action}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "\n",
    "# æ¸¬è©¦æ¨¡å‹\n",
    "test_trained_model(env, ppo_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO åƒ¹å€¼ç¶²çµ¡\n",
    "# class ValueNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(ValueNetwork, self).__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.LayerNorm(64),  # å¢åŠ  LayerNorm\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.LayerNorm(64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64, 1)  # ç›´æ¥è¼¸å‡ºæ•¸å€¼\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "\n",
    "# class PolicyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(PolicyNetwork, self).__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, output_dim),\n",
    "#             nn.Softmax(dim=-1)  # é¸æ“‡è¡Œå‹•\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.value = ValueNetwork(state_dim)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    def train(self, states, actions, log_probs, rewards):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        values = self.value(states).squeeze()\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        new_probs = self.policy(states)\n",
    "        new_dist = Categorical(new_probs)\n",
    "        new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip)\n",
    "        loss_policy = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "        loss_value = (returns - values).pow(2).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        loss_policy.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        self.value_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚æ¨è«–\n",
    "class NiuNiuDecisionHelper:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def decide_action(self, hand):\n",
    "        state = np.array([card_value(card) for card in hand], dtype=np.float32)\n",
    "        action, _ = self.agent.select_action(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 6x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 14\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     16\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 6x64)"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(state_dim=6, action_dim=10)\n",
    "\n",
    "# è¨“ç·´\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        state = new_state\n",
    "\n",
    "    agent.train(states, actions, log_probs, rewards)\n",
    "\n",
    "helper = NiuNiuDecisionHelper(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ç¢ºä¿ç•°å¸¸æª¢æ¸¬é–‹å•Ÿ\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=1e-3, gamma=0.99, clip_epsilon=0.2):\n",
    "        self.policy_network = PolicyNetwork(input_dim, action_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.policy_network(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, log_prob, next_state):\n",
    "        self.memory.append((state, action, reward, log_prob, next_state))\n",
    "\n",
    "    def train(self):\n",
    "        if not self.memory:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, log_probs, next_states = zip(*self.memory)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "\n",
    "        # è¨ˆç®—æ–°çš„ log_prob\n",
    "        new_probs = self.policy_network(states)\n",
    "        new_dist = torch.distributions.Categorical(new_probs)\n",
    "        new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "        # è¨ˆç®— ratio (ä¸¦ç¢ºä¿ä¸æœƒ in-place æ“ä½œ)\n",
    "        ratio = torch.exp(new_log_probs - log_probs.detach())\n",
    "\n",
    "        # è¨ˆç®— advantage\n",
    "        advantages = rewards - rewards.mean()\n",
    "\n",
    "        # PPO æå¤±å‡½æ•¸\n",
    "        unclipped = ratio * advantages\n",
    "        clipped = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "        # æ›´æ–°ç­–ç•¥ç¶²è·¯\n",
    "        self.optimizer_policy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer_policy.step()\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "\n",
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=len(env.get_state()), action_dim=10)  # ç¢ºä¿å‹•ä½œç©ºé–“æ˜¯ 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -12\n",
      "Episode 1, Total Reward: 8\n",
      "Episode 2, Total Reward: -12\n",
      "Episode 3, Total Reward: -24\n",
      "Episode 4, Total Reward: -16\n",
      "Episode 5, Total Reward: -12\n",
      "Episode 6, Total Reward: -8\n",
      "Episode 7, Total Reward: 8\n",
      "Episode 8, Total Reward: 4\n",
      "Episode 9, Total Reward: 16\n",
      "Episode 10, Total Reward: -4\n",
      "Episode 11, Total Reward: -8\n",
      "Episode 12, Total Reward: -16\n",
      "Episode 13, Total Reward: 4\n",
      "Episode 14, Total Reward: -60\n",
      "Episode 15, Total Reward: 8\n",
      "Episode 16, Total Reward: 3\n",
      "Episode 17, Total Reward: 5\n",
      "Episode 18, Total Reward: -48\n",
      "Episode 19, Total Reward: -16\n",
      "Episode 20, Total Reward: 6\n",
      "Episode 21, Total Reward: -8\n",
      "Episode 22, Total Reward: 24\n",
      "Episode 23, Total Reward: -4\n",
      "Episode 24, Total Reward: 8\n",
      "Episode 25, Total Reward: 4\n",
      "Episode 26, Total Reward: -4\n",
      "Episode 27, Total Reward: 20\n",
      "Episode 28, Total Reward: -24\n",
      "Episode 29, Total Reward: -4\n",
      "Episode 30, Total Reward: -8\n",
      "Episode 31, Total Reward: 4\n",
      "Episode 32, Total Reward: -2\n",
      "Episode 33, Total Reward: -4\n",
      "Episode 34, Total Reward: 12\n",
      "Episode 35, Total Reward: -8\n",
      "Episode 36, Total Reward: 12\n",
      "Episode 37, Total Reward: -16\n",
      "Episode 38, Total Reward: -8\n",
      "Episode 39, Total Reward: -4\n",
      "Episode 40, Total Reward: -8\n",
      "Episode 41, Total Reward: 4\n",
      "Episode 42, Total Reward: 12\n",
      "Episode 43, Total Reward: -24\n",
      "Episode 44, Total Reward: 12\n",
      "Episode 45, Total Reward: -6\n",
      "Episode 46, Total Reward: -24\n",
      "Episode 47, Total Reward: 1\n",
      "Episode 48, Total Reward: 12\n",
      "Episode 49, Total Reward: -8\n",
      "Episode 50, Total Reward: 1\n",
      "Episode 51, Total Reward: -4\n",
      "Episode 52, Total Reward: -32\n",
      "Episode 53, Total Reward: 12\n",
      "Episode 54, Total Reward: 6\n",
      "Episode 55, Total Reward: -3\n",
      "Episode 56, Total Reward: -8\n",
      "Episode 57, Total Reward: -8\n",
      "Episode 58, Total Reward: 24\n",
      "Episode 59, Total Reward: 8\n",
      "Episode 60, Total Reward: -24\n",
      "Episode 61, Total Reward: 16\n",
      "Episode 62, Total Reward: -4\n",
      "Episode 63, Total Reward: 12\n",
      "Episode 64, Total Reward: -4\n",
      "Episode 65, Total Reward: -8\n",
      "Episode 66, Total Reward: -80\n",
      "Episode 67, Total Reward: 8\n",
      "Episode 68, Total Reward: 3\n",
      "Episode 69, Total Reward: 40\n",
      "Episode 70, Total Reward: 36\n",
      "Episode 71, Total Reward: -36\n",
      "Episode 72, Total Reward: 8\n",
      "Episode 73, Total Reward: -4\n",
      "Episode 74, Total Reward: 12\n",
      "Episode 75, Total Reward: -12\n",
      "Episode 76, Total Reward: 12\n",
      "Episode 77, Total Reward: -12\n",
      "Episode 78, Total Reward: 2\n",
      "Episode 79, Total Reward: -2\n",
      "Episode 80, Total Reward: -9\n",
      "Episode 81, Total Reward: 40\n",
      "Episode 82, Total Reward: 4\n",
      "Episode 83, Total Reward: 20\n",
      "Episode 84, Total Reward: -8\n",
      "Episode 85, Total Reward: -24\n",
      "Episode 86, Total Reward: -8\n",
      "Episode 87, Total Reward: 12\n",
      "Episode 88, Total Reward: 8\n",
      "Episode 89, Total Reward: 12\n",
      "Episode 90, Total Reward: -8\n",
      "Episode 91, Total Reward: -12\n",
      "Episode 92, Total Reward: -8\n",
      "Episode 93, Total Reward: 8\n",
      "Episode 94, Total Reward: -48\n",
      "Episode 95, Total Reward: 12\n",
      "Episode 96, Total Reward: -24\n",
      "Episode 97, Total Reward: -24\n",
      "Episode 98, Total Reward: 12\n",
      "Episode 99, Total Reward: -4\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100  # è¨“ç·´ 10,000 å ´éŠæˆ²\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, log_prob, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.train()  # æ›´æ–° PPO\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # if episode % 100 == 0:\n",
    "    #     print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# everage : 22.5s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOAgent' object has no attribute 'policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# **ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥ä¾†é¸æ“‡å‹•ä½œ**\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdeterministic_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# è§£è®€å‹•ä½œ\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mdeterministic_action\u001b[1;34m(agent, state)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# åœæ­¢æ¢¯åº¦è¨ˆç®—ï¼ŒåŠ å¿«é‹ç®—é€Ÿåº¦\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# è½‰æ›ç‚º tensor\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m(state_tensor)  \u001b[38;5;66;03m# å–å¾—å‹•ä½œæ©Ÿç‡åˆ†å¸ƒ\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_probs)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# é¸æ“‡æ©Ÿç‡æœ€é«˜çš„å‹•ä½œ\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PPOAgent' object has no attribute 'policy'"
     ]
    }
   ],
   "source": [
    "# ç¢ºä¿ torch å’Œå…¶ä»–ç›¸é—œåº«å·²åŒ¯å…¥\n",
    "import torch\n",
    "\n",
    "# å®šç¾©ä¸€å€‹ç¢ºå®šæ€§å‹•ä½œé¸æ“‡å‡½æ•¸\n",
    "def deterministic_action(agent, state):\n",
    "    with torch.no_grad():  # åœæ­¢æ¢¯åº¦è¨ˆç®—ï¼ŒåŠ å¿«é‹ç®—é€Ÿåº¦\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)  # è½‰æ›ç‚º tensor\n",
    "        action_probs = agent.policy(state_tensor)  # å–å¾—å‹•ä½œæ©Ÿç‡åˆ†å¸ƒ\n",
    "        action = torch.argmax(action_probs).item()  # é¸æ“‡æ©Ÿç‡æœ€é«˜çš„å‹•ä½œ\n",
    "    return action\n",
    "\n",
    "# å‡è¨­ç’°å¢ƒå·²åˆå§‹åŒ–\n",
    "env = NiuNiuEnv()\n",
    "\n",
    "# å‡è¨­ä½ å·²ç¶“è¨“ç·´å¥½ PPO Agent\n",
    "agent = PPOAgent(input_dim=env.get_state().shape[0], action_dim=10)\n",
    "\n",
    "# çµ¦å®šç‰¹å®šçš„æ‰‹ç‰Œ\n",
    "my_hand = [('diamond', 'K'), ('diamond', '9'), ('diamond', '6'), ('club', '4')]\n",
    "\n",
    "# å–å¾—æ‰‹ç‰Œå°æ‡‰çš„ç’°å¢ƒç‹€æ…‹\n",
    "state = env.get_state()\n",
    "\n",
    "# **ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥ä¾†é¸æ“‡å‹•ä½œ**\n",
    "action = deterministic_action(agent, state)\n",
    "\n",
    "# è§£è®€å‹•ä½œ\n",
    "if action < 5:\n",
    "    print(f\"AI æ±ºå®šæ¶èŠï¼Œå€ç‡ç‚º {action + 1} å€\")\n",
    "else:\n",
    "    print(f\"AI æ±ºå®šä¸‹æ³¨ï¼Œå€ç‡ç‚º {action - 4} å€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiuNiuDecisionHelper:\n",
    "    def __init__(self, agent, env):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.suit_map = {'diamond': 0, 'club': 1, 'heart': 2, 'spade': 3}\n",
    "        self.rank_map = {'J': 11, 'Q': 12, 'K': 13, 'A': 14}\n",
    "        self.rank_map.update({str(i): i for i in range(2, 11)})\n",
    "\n",
    "    def preprocess_state(self, hand_cards):\n",
    "        \"\"\"\n",
    "        å°‡æ‰‹ç‰Œè½‰æ›ç‚ºæ•¸å€¼å‹ NumPy é™£åˆ—\n",
    "        :param hand_cards: ç©å®¶çš„æ‰‹ç‰Œ (ex: [('diamond', '7'), ('diamond', '9')])\n",
    "        :return: æ•¸å€¼å‹ NumPy é™£åˆ—\n",
    "        \"\"\"\n",
    "        numerical_hand = []\n",
    "        for suit, rank in hand_cards:\n",
    "            suit_num = self.suit_map[suit]\n",
    "            rank_num = self.rank_map[rank]\n",
    "            numerical_hand.extend([suit_num, rank_num])  \n",
    "        return np.array(numerical_hand, dtype=np.float32)\n",
    "\n",
    "    def decide_qiangzhuang(self, hand_cards):\n",
    "        \"\"\"\n",
    "        æ ¹æ“šæ‰‹ç‰Œæ±ºå®šæ˜¯å¦æ¶èŠï¼Œä¸¦æä¾›æ¶èŠå€ç‡ (1~4 å€) æˆ– ä¸æ¶æ™‚ä¸‹æ³¨å€ç‡ (1~5 å€)\n",
    "        \"\"\"\n",
    "        processed_state = self.preprocess_state(hand_cards)\n",
    "        action, _ = self.agent.select_action(processed_state)\n",
    "\n",
    "        if action < 4:  # 0~3 æ¶èŠ (å€ç‡ 1~4)\n",
    "            return True, action + 1\n",
    "        else:  # 4~9 ä¸æ¶èŠ (å€ç‡ 1~5)\n",
    "            return False, action - 3\n",
    "\n",
    "    def compute_banker_loss(self, hand_cards, final_banker, qiangzhuang_multiplier):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¶èŠå¤±æ•—æ™‚æ‡‰è©²ä¸‹æ³¨çš„å€ç‡\n",
    "        :param hand_cards: ç©å®¶çš„æ‰‹ç‰Œ\n",
    "        :param final_banker: æ˜¯å¦æˆç‚ºèŠå®¶ (True/False)\n",
    "        :param qiangzhuang_multiplier: ç•¶åˆæ¶èŠæ™‚çš„å€ç‡\n",
    "        :return: æ¶èŠå¤±æ•—æ™‚ï¼Œæ‡‰è©²ä¸‹æ³¨çš„å€ç‡ (1~5 å€)ï¼Œè‹¥æˆåŠŸæˆç‚ºèŠå®¶å‰‡å›å‚³ None\n",
    "        \"\"\"\n",
    "        if final_banker:\n",
    "            return None  # æˆåŠŸç•¶èŠï¼Œä¸ç”¨ç®—è³ ç‡\n",
    "\n",
    "        # **ä½¿ç”¨ AI ä¾†æ±ºå®šä¸‹æ³¨å€ç‡**\n",
    "        processed_state = self.preprocess_state(hand_cards)\n",
    "        action, _ = self.agent.select_action(processed_state)\n",
    "\n",
    "        if action >= 4:  # 4~9 ä»£è¡¨ä¸‹æ³¨ (å°æ‡‰å€ç‡ 1~5)\n",
    "            return action - 3\n",
    "        else:\n",
    "            return 1  # é è¨­ç‚ºæœ€ä½å€ç‡ 1 å€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‡‰è©²æ¶èŠï¼Œå»ºè­°å€ç‡: 3 å€\n",
      "æ¶èŠå¤±æ•—ï¼ŒAI å»ºè­°ä¸‹æ³¨å€ç‡: 1 å€\n"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=8, action_dim=10)  # è¼¸å…¥ 8 ç¶­ï¼Œè¼¸å‡º 10 å€‹å‹•ä½œ\n",
    "decision_helper = NiuNiuDecisionHelper(agent, env)\n",
    "\n",
    "hand_cards = [('diamond', 'J'), ('diamond', 'K'), ('diamond', '10'), ('club', '4')]\n",
    "should_qiang, multiplier = decision_helper.decide_qiangzhuang(hand_cards)\n",
    "\n",
    "if should_qiang:\n",
    "    print(f\"æ‡‰è©²æ¶èŠï¼Œå»ºè­°å€ç‡: {multiplier} å€\")\n",
    "    final_banker = bool(int(input(\"æœ€å¾Œæ˜¯å¦æˆåŠŸæˆç‚ºèŠå®¶ï¼Ÿ(1: æ˜¯, 0: å¦): \")))\n",
    "    banker_loss = decision_helper.compute_banker_loss(hand_cards, final_banker, multiplier)\n",
    "    if banker_loss:\n",
    "        print(f\"æ¶èŠå¤±æ•—ï¼ŒAI å»ºè­°ä¸‹æ³¨å€ç‡: {banker_loss} å€\")\n",
    "else:\n",
    "    print(f\"ä¸æ¶èŠï¼Œå»ºè­°ä¸‹æ³¨å€ç‡: {multiplier} å€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO ä»£ç†äºº\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=0.002, gamma=0.99, epsilon=0.2, update_steps=5):\n",
    "        self.policy = PolicyNetwork(input_dim, action_dim)  # âœ… ç¢ºä¿é€™è£¡æœ‰ PolicyNetwork\n",
    "        self.value = ValueNetwork(input_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.update_steps = update_steps\n",
    "        self.memory = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\" ä½¿ç”¨ç­–ç•¥ç¶²çµ¡é¸æ“‡è¡Œå‹• \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # åŠ ç¶­åº¦ (batch=1)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, next_state):\n",
    "        \"\"\" å­˜å„²äº¤äº’æ•¸æ“š \"\"\"\n",
    "        self.memory.append((state, action, reward, log_prob, next_state))\n",
    "\n",
    "    def compute_discounted_rewards(self, rewards):\n",
    "        \"\"\" è¨ˆç®—æŠ˜æ‰£å›å ± G_t \"\"\"\n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "        return torch.FloatTensor(discounted_rewards)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" ä½¿ç”¨ PPO ä¾†è¨“ç·´ç­–ç•¥ç¶²çµ¡å’Œåƒ¹å€¼ç¶²çµ¡ \"\"\"\n",
    "        if len(self.memory) == 0:\n",
    "            return\n",
    "        \n",
    "        # 1. è§£æè¨˜æ†¶\n",
    "        states, actions, rewards, log_probs, next_states = zip(*self.memory)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        rewards = self.compute_discounted_rewards(rewards)\n",
    "        \n",
    "        # 2. è¨ˆç®—å„ªå‹¢å€¼ Advantage = G_t - V(s)\n",
    "        values = self.value(states).squeeze()\n",
    "        advantages = rewards - values.detach().clone()  # ä½¿ç”¨ clone() ä¾†é¿å…åŸåœ°ä¿®æ”¹\n",
    "\n",
    "        # 3. æ›´æ–°ç­–ç•¥ç¶²çµ¡ (PPO Loss)\n",
    "        for _ in range(self.update_steps):  # é‡è¤‡å¤šæ¬¡æ›´æ–°\n",
    "            probs = self.policy(states)\n",
    "            dist = Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - log_probs)  # é‡è¦æ€§æ¬Šé‡\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "            policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "            \n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)  # ç¢ºä¿è¨ˆç®—åœ–ä¸æœƒè¢«é‡‹æ”¾\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "        # 4. æ›´æ–°åƒ¹å€¼ç¶²çµ¡ (MSE æå¤±)\n",
    "        value_loss = (self.value(states).squeeze() - rewards).pow(2).mean()\n",
    "        self.optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optimizer_value.step()\n",
    "\n",
    "        # 5. æ¸…ç©ºè¨˜æ†¶\n",
    "        self.memory = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     18\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# ç¢ºä¿ reward æ˜¯æ•¸å­—\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# è¨“ç·´ä¸€æ¬¡\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [116]\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m     policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmin(ratio \u001b[38;5;241m*\u001b[39m advantages, clipped_ratio \u001b[38;5;241m*\u001b[39m advantages)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ç¢ºä¿è¨ˆç®—åœ–ä¸æœƒè¢«é‡‹æ”¾\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 4. æ›´æ–°åƒ¹å€¼ç¶²çµ¡ (MSE æå¤±)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=len(env.get_state()), action_dim=10)\n",
    "\n",
    "num_episodes = 10  # é€²è¡Œ 10 æ¬¡è¨“ç·´\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)  # æ³¨æ„é€™è£¡è§£æ§‹è¿”å›å€¼\n",
    "        \n",
    "        agent.store_transition(state, action, reward, log_prob, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward  # ç¢ºä¿ reward æ˜¯æ•¸å­—\n",
    "\n",
    "    agent.train()  # è¨“ç·´ä¸€æ¬¡\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# æœ€å¾Œçš„è¨“ç·´çµæœ\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 12\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# å‘¼å« step() ä¸¦ç²å¾—ä¸‹ä¸€æ­¥ç‹€æ…‹å’Œçå‹³\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ PPO\n",
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=4, action_dim=10)\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    memory = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        \n",
    "        # å‘¼å« step() ä¸¦ç²å¾—ä¸‹ä¸€æ­¥ç‹€æ…‹å’Œçå‹³\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        memory.append((state, action, reward, log_prob, next_state))\n",
    "        state = next_state\n",
    "    \n",
    "    agent.train(memory)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Last Reward: {reward}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# æ¸¬è©¦ 1: ä½¿ç”¨ä»£ç†é¸æ“‡è¡Œç‚º\u001b[39;00m\n\u001b[0;32m      8\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 9\u001b[0m action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChosen Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Log Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_prob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# æ¸¬è©¦ 2: åŸ·è¡Œä¸€æ­¥éŠæˆ²ä¸¦æŸ¥çœ‹çµæœ\u001b[39;00m\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦ä¸»ç¨‹å¼\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆå§‹åŒ–éŠæˆ²ç’°å¢ƒå’Œä»£ç†\n",
    "    env = NiuNiuEnv()\n",
    "    agent = PPOAgent(input_dim=4, action_dim=10)\n",
    "    \n",
    "    # æ¸¬è©¦ 1: ä½¿ç”¨ä»£ç†é¸æ“‡è¡Œç‚º\n",
    "    state = env.reset()\n",
    "    action, log_prob = agent.select_action(state)\n",
    "    print(f\"Chosen Action: {action}, Log Probability: {log_prob}\")\n",
    "    \n",
    "    # æ¸¬è©¦ 2: åŸ·è¡Œä¸€æ­¥éŠæˆ²ä¸¦æŸ¥çœ‹çµæœ\n",
    "    is_banker = (action < 5)  # å‡è¨­é¸æ“‡ 0-4 ç‚ºæ¶èŠï¼Œ5-9 ç‚ºä¸‹æ³¨\n",
    "    next_state, reward, done = env.step(action, is_banker)\n",
    "    print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡è¨­æ¨¡å‹å·²ç¶“è¨“ç·´å®Œæˆä¸¦ä¿å­˜åœ¨ agent ä¸­\n",
    "def get_action_from_model(hand, agent, is_banker=False):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šæ‰‹ç‰Œèˆ‡æ¨¡å‹æ±ºç­–æ˜¯å¦æ¶èŠï¼Œä¸¦åœ¨æ¶èŠèˆ‡å¦æ™‚æ±ºå®šä¸‹æ³¨ç­–ç•¥ã€‚\n",
    "    hand: ç©å®¶æ‰‹ç‰Œ\n",
    "    agent: è¨“ç·´å¥½çš„PPOAgent\n",
    "    is_banker: æ˜¯å¦ç‚ºèŠå®¶ï¼ŒTrueè¡¨ç¤ºæ˜¯èŠå®¶ï¼ŒFalseè¡¨ç¤ºé–’å®¶\n",
    "    \"\"\"\n",
    "    # å°‡æ‰‹ç‰Œè½‰æ›ç‚ºæ¨¡å‹çš„ç‹€æ…‹å‘é‡\n",
    "    state = np.array([card_value(card) for card in hand], dtype=np.float32)  # è½‰æ›ç‚ºæ•¸å­—ç‹€æ…‹\n",
    "    \n",
    "    # å¦‚æœæ˜¯é–’å®¶ï¼Œå…ˆæ±ºå®šæ˜¯å¦æ¶èŠï¼ˆé¸æ“‡å‹•ä½œï¼‰\n",
    "    if not is_banker:\n",
    "        action, _ = agent.select_action(state)  # æ ¹æ“šç‹€æ…‹é¸æ“‡å‹•ä½œ\n",
    "        if action < 5:  # å¦‚æœé¸æ“‡çš„å‹•ä½œæ˜¯æ¶èŠï¼ˆ0-4è¡¨ç¤ºæ¶èŠï¼‰\n",
    "            is_banker = True\n",
    "            print(\"æ±ºå®šæ¶èŠï¼\")\n",
    "        else:\n",
    "            print(\"æ±ºå®šä¸æ¶èŠï¼Œé¸æ“‡ä¸‹æ³¨ã€‚\")\n",
    "    \n",
    "    # æ ¹æ“šæ˜¯å¦æ¶èŠä¾†æ±ºå®šä¸‹æ³¨\n",
    "    if is_banker:\n",
    "        # å¦‚æœæ˜¯èŠå®¶ï¼Œæ±ºå®šä¸‹æ³¨ç­–ç•¥\n",
    "        action, _ = agent.select_action(state)  # èŠå®¶å¯ä»¥ä¸‹æ³¨çš„å‹•ä½œç¯„åœæ˜¯ 0-4\n",
    "        print(f\"ä½œç‚ºèŠå®¶ï¼Œä¸‹æ³¨å€ç‡ç‚º {action % 5 + 1}\")\n",
    "    else:\n",
    "        # å¦‚æœæ˜¯é–’å®¶ï¼Œæ ¹æ“šæ‰‹ç‰Œæ±ºå®šä¸‹æ³¨\n",
    "        action, _ = agent.select_action(state)  # é–’å®¶çš„ä¸‹æ³¨å‹•ä½œç¯„åœæ˜¯ 5-9\n",
    "        print(f\"ä½œç‚ºé–’å®¶ï¼Œä¸‹æ³¨å€ç‡ç‚º {action % 5 + 1}\")\n",
    "\n",
    "    return action\n",
    "\n",
    "# å‡è¨­æˆ‘å€‘æœ‰ä¸€å‰¯æ‰‹ç‰Œ\n",
    "player_hand = ['3â™ ', '7â™£', 'Kâ™¦', '9â™¥']  # ç©å®¶æ‰‹ç‰Œ\n",
    "\n",
    "# ä½¿ç”¨è¨“ç·´å®Œæˆçš„æ¨¡å‹ä¾†æ±ºå®šç­–ç•¥\n",
    "get_action_from_model(player_hand, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "player_hand = [('heart', '9'), ('diamond', 'J'), ('club', '3'), ('spade', '6'), ('heart', '2')]\n",
    "banker_hand = [('spade', '10'), ('club', 'J'), ('heart', '4'), ('diamond', '6'), ('diamond', '2')]\n",
    "print(type(calculate_payout(player_hand, banker_hand, verbose=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
