{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’çš„æ–¹æ³•æ”¹å–„ç­–ç•¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "## niuniu function\n",
    "from niuniu_func import *\n",
    "\n",
    "## caculating\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "\n",
    "## os\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build niuniu env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env of niuniu\n",
    "# set myself as player 0\n",
    "class NiuNiuEnv:\n",
    "    # init \n",
    "    def __init__(self):\n",
    "        # generate deck\n",
    "        self.deck = self.generate_deck()\n",
    "        # generate player == 4\n",
    "        self.players = [[] for _ in range(4)]\n",
    "        self.banker_index = -1\n",
    "        # banker multiplier\n",
    "        self.banker_multiplier = 1\n",
    "        # bet number\n",
    "        self.bets = [0, 0, 0, 0]\n",
    "        # state, 0: bank step, 1: bet step, 2: result step\n",
    "        self.current_phase = 0\n",
    "        # reset\n",
    "        self.reset()\n",
    "\n",
    "    # generate deck\n",
    "    def generate_deck(self):\n",
    "        suits = ['heart', 'spade', 'diamond', 'club']\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "        return [(suit, rank) for suit in suits for rank in ranks]\n",
    "\n",
    "    # reset\n",
    "    def reset(self):\n",
    "        # regenerate deck & shuffle\n",
    "        self.deck = self.generate_deck()\n",
    "        random.shuffle(self.deck)\n",
    "        # every player have 4 cards\n",
    "        self.players = [[self.deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "        # init bets\n",
    "        self.bets = [0] * 4\n",
    "        self.banker_index = -1\n",
    "        self.banker_multiplier = 1\n",
    "\n",
    "        # init step\n",
    "        self.current_phase = 0\n",
    "\n",
    "        # reload state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "    \n",
    "    # get myself's hand number\n",
    "    def get_state(self):\n",
    "        # myself's hand\n",
    "        state = []\n",
    "        for card in self.players[0]:\n",
    "            state.append(get_suit_rank(card))\n",
    "            state.append(get_card_rank(card))\n",
    "        # which step\n",
    "        state.append(self.current_phase)\n",
    "        # who is banker\n",
    "        state.append(self.banker_index)\n",
    "        # banker multiplayer\n",
    "        state.append(self.banker_multiplier)\n",
    "        # every player's bet\n",
    "        state.extend(self.bets)\n",
    "        return np.array(state, dtype=np.float32)\n",
    "        \n",
    "    # step    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: [banker_action, bet_action]\n",
    "        banker_action: 0-4 is baker multi\n",
    "        bet_action: 1-5 is bet multi\n",
    "        \"\"\"\n",
    "        # unpack action\n",
    "        banker_action, bet_action = action \n",
    "        # banker\n",
    "        self.banker_bid = banker_action\n",
    "        # bet\n",
    "        self.bet_amount = bet_action\n",
    "\n",
    "        \"\"\"\n",
    "        step 1 : decide whether to get banker\n",
    "            * myself : by ppo agent\n",
    "            * others : by simulate_ev to decide\n",
    "        \"\"\"\n",
    "        bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        bank_multipliers[0] = self.banker_bid\n",
    "        # run time : 22s\n",
    "\n",
    "        \"\"\"\n",
    "        step 2 : decide final banker(the max multiplier)\n",
    "            * if all not want to be banker, random choose one & set multiplier = 1\n",
    "            * if more than one have same multiplier, random choose one\n",
    "        \"\"\"\n",
    "        max_bet = max(bank_multipliers)\n",
    "        if max_bet == 1:\n",
    "            random_banker = random.choice(range(4))\n",
    "            bank_multipliers[random_banker] = 1\n",
    "        banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "        self.banker_index = random.choice(banker_candidates)\n",
    "        self.banker_multiplier = max_bet\n",
    "        banker_hand = self.players[self.banker_index]\n",
    "\n",
    "        # whether myself is banker\n",
    "        is_banker = (self.banker_index == 0)\n",
    "\n",
    "        # go to next action -- bet\n",
    "        self.current_phase = 1\n",
    "\n",
    "        # bet action\n",
    "        if is_banker:\n",
    "            \"\"\"\n",
    "            step 3 : if myself is banker\n",
    "                * I don't need to bet\n",
    "                * others use `calculate_ev_against_banker` to bet, besides\n",
    "                if banker multiplier over 3, we assume banker have niu\n",
    "            \"\"\"\n",
    "            self.bets[0] = self.bet_amount\n",
    "            for i in range(1, 4):\n",
    "                have_niu = self.banker_multiplier >= 3\n",
    "                self.bets[i] = calculate_ev_against_banker(self.players[i], 100000, have_niu)[1]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            step 4 : if myself is not banker\n",
    "                * let ppo decide bet\n",
    "                * others we don't care\n",
    "            \"\"\"\n",
    "            self.bets[0] = max(1, min(5, action[1]))\n",
    "\n",
    "        \"\"\"\n",
    "        step 5 : add the 5th card to every player\n",
    "        \"\"\"\n",
    "        for i in range(4):\n",
    "            self.players[i].append(self.deck.pop())\n",
    "\n",
    "        # go to next action -- result\n",
    "        self.current_phase = 2\n",
    "\n",
    "        \"\"\"\n",
    "        step 6 : caculate ev of myself\n",
    "            * I am banker : caculate payout of the sum of me against others(use negative)\n",
    "            * I am not banker : calculate the payout against the banker\n",
    "        \"\"\"\n",
    "        if is_banker:\n",
    "            # I am banker\n",
    "            total_payout = -sum(\n",
    "                calculate_payout(self.players[i], banker_hand, False) * self.bets[i] * self.banker_multiplier\n",
    "                for i in range(4) if i != self.banker_index\n",
    "            )\n",
    "        else:\n",
    "            # I am not banker\n",
    "            total_payout = calculate_payout(self.players[0], banker_hand, False) * self.bets[0] * self.banker_multiplier\n",
    "\n",
    "        \"\"\"\n",
    "        step 7 : caculate reward(scaling & punishing)\n",
    "        \"\"\"\n",
    "        # ç¢ºä¿ max_possible_loss è‡³å°‘æ˜¯ 1ï¼Œé¿å…é™¤ä»¥ 0\n",
    "        max_possible_loss = max(5 * self.banker_multiplier, 1)\n",
    "\n",
    "        # æ¨™æº–åŒ– rewardï¼Œè®“å®ƒåœ¨ -1 ~ 1 ä¹‹é–“\n",
    "        reward = total_payout / max_possible_loss\n",
    "\n",
    "        # å¢åŠ å°é«˜å€ç‡æ¶èŠçš„é¢¨éšªæ‡²ç½°\n",
    "        if is_banker:\n",
    "            # å¦‚æœé¸æ“‡é«˜å€ç‡ï¼Œé¡å¤–æ‡²ç½° (ä¾‹å¦‚ 3 å€æˆ– 4 å€èŠå®¶)\n",
    "            risk_penalty = 0.05 * self.banker_multiplier  # é¢¨éšªæ‡²ç½°ï¼Œå€ç‡è¶Šé«˜æ‡²ç½°è¶Šå¤§\n",
    "            if total_payout < 0:\n",
    "                reward -= risk_penalty  # è‹¥èŠå®¶è™§æï¼Œå‰‡å¢åŠ æ‡²ç½°\n",
    "\n",
    "        # é™åˆ¶ reward ç¯„åœï¼Œé¿å… PPO éåº¦åå‘æŸå€‹å‹•ä½œ\n",
    "        reward = max(-1, min(1, reward))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        step 8 : finish one round\n",
    "        \"\"\"\n",
    "        done = True\n",
    "\n",
    "        # \"\"\"\n",
    "        # step 9 : reset\n",
    "        # \"\"\"\n",
    "        # self.reset()\n",
    "\n",
    "        return self.get_state(), reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple test\n",
    "test whether niuniu env is runnable <br>\n",
    "to avoid getting error of having NaN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [ 3. 13.  3.  6.  3.  5.  4. 11.  0. -1.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Step 0 - State: [ 4.  5.  3.  1.  3. 13.  1.  2.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -2.0\n",
      "Step 1 - State: [ 4.  1.  4.  4.  3.  8.  1.  2.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.2\n",
      "Step 2 - State: [ 3.  1.  3.  7.  2.  5.  4. 13.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 1.2\n",
      "Step 3 - State: [ 1. 13.  1.  3.  2.  6.  1.  2.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.8\n",
      "Step 4 - State: [ 1. 12.  2.  8.  3.  7.  2.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: 0.8\n",
      "Step 5 - State: [ 2.  2.  1.  1.  4.  3.  2.  4.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.4\n",
      "Step 6 - State: [ 2. 11.  2.  2.  1.  1.  2. 13.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.6\n",
      "Step 7 - State: [ 4.  1.  1. 12.  3. 12.  1.  3.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -0.4\n",
      "Step 8 - State: [ 4.  9.  1.  3.  3. 10.  3. 11.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.2\n",
      "Step 9 - State: [ 2.  2.  2. 10.  2.  4.  4. 10.  0. -1.  1.  0.  0.  0.  0.  0.  0.], Reward: -1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nresult explain :\\n    * the first 8 numbers represent 4 card in hands, (suit, card)\\n    * the others represent the other states\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test env of niuniu\n",
    "def test_env():\n",
    "    env = NiuNiuEnv()\n",
    "    state = env.reset()\n",
    "    print(\"Initial State:\", state)\n",
    "    # test 10 times\n",
    "    for i in range(10):\n",
    "        # random action\n",
    "        action = [np.random.randint(0, 5), np.random.randint(1, 6)]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if np.isnan(state).any():\n",
    "            print(f\"NaN detected in state at step {i}!\")\n",
    "        if np.isnan(reward):\n",
    "            print(f\"NaN detected in reward at step {i}!\")\n",
    "        print(f\"Step {i} - State: {state}, Reward: {reward}\")\n",
    "\n",
    "test_env()\n",
    "# run time : 3m 55s\n",
    "\"\"\"\n",
    "result explain :\n",
    "    * the first 8 numbers represent 4 card in hands, (suit, card)\n",
    "    * the others represent the other states\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO åƒ¹å€¼ç¶²çµ¡ (V(s))\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),  # é¿å…æ¨™æº–åŒ–æ™‚å‡ºç¾ NaN\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1)  # è¼¸å‡º V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).squeeze(-1)  # è®“è¼¸å‡ºç¶­åº¦è®Šæˆ (batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO ç­–ç•¥ç¶²çµ¡\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.banker_fc = nn.Linear(128, output_dim1)  # æ¶èŠå‹•ä½œ\n",
    "        self.bet_fc = nn.Linear(128, output_dim2)  # ä¸‹æ³¨å‹•ä½œ\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        banker_logits = self.banker_fc(x)\n",
    "        bet_logits = self.bet_fc(x)\n",
    "\n",
    "        banker_probs = F.softmax(banker_logits, dim=-1)\n",
    "        bet_probs = F.softmax(bet_logits, dim=-1)\n",
    "\n",
    "        return banker_probs, bet_probs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # è½‰æˆ batch\n",
    "        banker_probs, bet_probs = self.forward(state)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().item()\n",
    "        bet_action = bet_dist.sample().item()  # é€™è£¡ä¸åŠ  +1ï¼Œè®“å¤–éƒ¨è™•ç†\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action))\n",
    "\n",
    "        return (banker_action, bet_action), banker_log_prob, bet_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=10, model_path=\"niu_ppo_model\", num_envs=8):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_envs = num_envs\n",
    "        \n",
    "        self.policy = PolicyNetwork(input_dim, output_dim1, output_dim2).to(self.device)\n",
    "        self.value = ValueNetwork(input_dim).to(self.device)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.model_path = model_path\n",
    "        # load saved model\n",
    "        self.load_model()\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        R = torch.zeros(1, dtype=torch.float32).to(self.device)  # æ”¹ç‚ºæ¨™é‡åˆå§‹åŒ–\n",
    "\n",
    "        # ç¢ºä¿ rewards å’Œ dones æ˜¯ (T, 1) å½¢ç‹€\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            R = rewards[t] + self.gamma * R * (1 - dones[t])\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        return torch.cat(returns).detach()\n",
    "\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, dones):\n",
    "        returns = self.compute_returns(rewards, dones)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).view(-1, 2).to(self.device)  # ç¢ºä¿ actions ç¶­åº¦æ­£ç¢º\n",
    "        old_log_probs = torch.tensor(log_probs, dtype=torch.float32).view(-1).to(self.device)  # è½‰ç‚º 1D\n",
    "\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            banker_probs, bet_probs = self.policy(states)\n",
    "\n",
    "            banker_probs = torch.nan_to_num(banker_probs, nan=0.0)\n",
    "            bet_probs = torch.nan_to_num(bet_probs, nan=0.0)\n",
    "\n",
    "            banker_dist = Categorical(banker_probs)\n",
    "            bet_dist = Categorical(bet_probs)\n",
    "\n",
    "            new_banker_log_prob = banker_dist.log_prob(actions[:, 0])\n",
    "            new_bet_log_prob = bet_dist.log_prob(actions[:, 1] - 1)\n",
    "            new_log_probs = new_banker_log_prob + new_bet_log_prob\n",
    "            new_log_probs = torch.nan_to_num(new_banker_log_prob, nan=0.0) + torch.nan_to_num(new_bet_log_prob, nan=0.0)\n",
    "    \n",
    "            value_estimates = self.value(states).view(-1)\n",
    "            value_estimates = torch.nan_to_num(value_estimates, nan=0.0)\n",
    "\n",
    "            advantages = returns - value_estimates.detach()\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "            self.optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        # state = torch.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        state = torch.nan_to_num(state, nan=0.0)\n",
    "\n",
    "        banker_probs, bet_probs = self.policy(state)\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().cpu().numpy()\n",
    "        bet_action = (bet_dist.sample() + 1).cpu().numpy()\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action, device=self.device))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action - 1, device=self.device))\n",
    "\n",
    "        return np.array([banker_action, bet_action]), banker_log_prob.detach().cpu().numpy(), bet_log_prob.detach().cpu().numpy()\n",
    "    \n",
    "    def save_model(self):\n",
    "        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "        torch.save(self.policy.state_dict(), f\"{self.model_path}_policy.pth\")\n",
    "        torch.save(self.value.state_dict(), f\"{self.model_path}_value.pth\")\n",
    "        print(\"model saved\")\n",
    "\n",
    "    def load_model(self):\n",
    "        policy_path = f\"{self.model_path}_policy.pth\"\n",
    "        value_path = f\"{self.model_path}_value.pth\"\n",
    "\n",
    "        if os.path.exists(policy_path) and os.path.exists(value_path):\n",
    "            self.policy.load_state_dict(torch.load(policy_path))\n",
    "            self.value.load_state_dict(torch.load(value_path))\n",
    "            print(\"load saved model\")\n",
    "        else:\n",
    "            print(\"model not found, start training from begining\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 120  # è¨“ç·´å›åˆæ•¸\n",
    "batch_size = 2048\n",
    "gamma = 0.99\n",
    "clip_epsilon = 0.2\n",
    "lr = 3e-4\n",
    "update_epochs = 10\n",
    "save_interval = 5  # æ¯ 5 å›åˆå­˜ä¸€æ¬¡æ¨¡å‹\n",
    "\n",
    "save_model_path = \"D:/python/poker_gto/ppo_models/ppo_model\"\n",
    "num_envs = 12  # ä½¿ç”¨ 8 å€‹ç’°å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found, start training from begining\n"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "banker_action_dim = 5  # æ¶èŠå€ç‡ (0~4)\n",
    "bet_action_dim = 5  # ä¸‹æ³¨å€ç‡ (1~5)\n",
    "\n",
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim, model_path=save_model_path, num_envs=num_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 0.4\n",
      "Episode 2: Total Reward = 1\n",
      "Episode 3: Total Reward = -1\n",
      "Episode 4: Total Reward = -1\n",
      "Episode 5: Total Reward = -0.4\n",
      "model saved\n",
      "Episode 6: Total Reward = -0.6\n",
      "Episode 7: Total Reward = -0.8\n",
      "Episode 8: Total Reward = -0.6\n",
      "Episode 9: Total Reward = 1\n",
      "Episode 10: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 11: Total Reward = -0.6000000000000001\n",
      "Episode 12: Total Reward = 0.6\n",
      "Episode 13: Total Reward = 0.6\n",
      "Episode 14: Total Reward = 0.6\n",
      "Episode 15: Total Reward = 1\n",
      "model saved\n",
      "Episode 16: Total Reward = 1\n",
      "Episode 17: Total Reward = -0.6\n",
      "Episode 18: Total Reward = 0.2\n",
      "Episode 19: Total Reward = -0.6\n",
      "Episode 20: Total Reward = -1\n",
      "model saved\n",
      "Episode 21: Total Reward = 0.6\n",
      "Episode 22: Total Reward = -0.6\n",
      "Episode 23: Total Reward = 0.6\n",
      "Episode 24: Total Reward = -0.6\n",
      "Episode 25: Total Reward = -1\n",
      "model saved\n",
      "Episode 26: Total Reward = -1\n",
      "Episode 27: Total Reward = 0.4\n",
      "Episode 28: Total Reward = 1\n",
      "Episode 29: Total Reward = -0.8\n",
      "Episode 30: Total Reward = -1\n",
      "model saved\n",
      "Episode 31: Total Reward = 0.6\n",
      "Episode 32: Total Reward = 1\n",
      "Episode 33: Total Reward = -1\n",
      "Episode 34: Total Reward = -1\n",
      "Episode 35: Total Reward = 0.8\n",
      "model saved\n",
      "Episode 36: Total Reward = 0.2\n",
      "Episode 37: Total Reward = -1\n",
      "Episode 38: Total Reward = 1\n",
      "Episode 39: Total Reward = 1\n",
      "Episode 40: Total Reward = 1\n",
      "model saved\n",
      "Episode 41: Total Reward = -0.6000000000000001\n",
      "Episode 42: Total Reward = -0.8\n",
      "Episode 43: Total Reward = 1\n",
      "Episode 44: Total Reward = 0.6\n",
      "Episode 45: Total Reward = -1\n",
      "model saved\n",
      "Episode 46: Total Reward = 1\n",
      "Episode 47: Total Reward = -1\n",
      "Episode 48: Total Reward = -1\n",
      "Episode 49: Total Reward = -1\n",
      "Episode 50: Total Reward = -0.8\n",
      "model saved\n",
      "Episode 51: Total Reward = -1\n",
      "Episode 52: Total Reward = -0.6\n",
      "Episode 53: Total Reward = -1\n",
      "Episode 54: Total Reward = 1\n",
      "Episode 55: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 56: Total Reward = -1\n",
      "Episode 57: Total Reward = 0.6\n",
      "Episode 58: Total Reward = 1\n",
      "Episode 59: Total Reward = -1\n",
      "Episode 60: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 61: Total Reward = -0.6\n",
      "Episode 62: Total Reward = -0.6\n",
      "Episode 63: Total Reward = -1\n",
      "Episode 64: Total Reward = 1\n",
      "Episode 65: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 66: Total Reward = -1\n",
      "Episode 67: Total Reward = -0.6\n",
      "Episode 68: Total Reward = 0.6\n",
      "Episode 69: Total Reward = -1\n",
      "Episode 70: Total Reward = 1\n",
      "model saved\n",
      "Episode 71: Total Reward = 0.6\n",
      "Episode 72: Total Reward = 0.6\n",
      "Episode 73: Total Reward = -0.6\n",
      "Episode 74: Total Reward = -1\n",
      "Episode 75: Total Reward = -1\n",
      "model saved\n",
      "Episode 76: Total Reward = -0.6\n",
      "Episode 77: Total Reward = -1\n",
      "Episode 78: Total Reward = -1\n",
      "Episode 79: Total Reward = 1\n",
      "Episode 80: Total Reward = 0.6\n",
      "model saved\n",
      "Episode 81: Total Reward = -1\n",
      "Episode 82: Total Reward = 0.6\n",
      "Episode 83: Total Reward = -1\n",
      "Episode 84: Total Reward = -1\n",
      "Episode 85: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 86: Total Reward = 1\n",
      "Episode 87: Total Reward = -0.6\n",
      "Episode 88: Total Reward = -1\n",
      "Episode 89: Total Reward = -1\n",
      "Episode 90: Total Reward = 1\n",
      "model saved\n",
      "Episode 91: Total Reward = -1\n",
      "Episode 92: Total Reward = -1\n",
      "Episode 93: Total Reward = -1\n",
      "Episode 94: Total Reward = -0.6\n",
      "Episode 95: Total Reward = 1\n",
      "model saved\n",
      "Episode 96: Total Reward = -1\n",
      "Episode 97: Total Reward = 0.6\n",
      "Episode 98: Total Reward = 1\n",
      "Episode 99: Total Reward = -0.6\n",
      "Episode 100: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 101: Total Reward = -1\n",
      "Episode 102: Total Reward = -1\n",
      "Episode 103: Total Reward = -1\n",
      "Episode 104: Total Reward = -1\n",
      "Episode 105: Total Reward = -0.8\n",
      "model saved\n",
      "Episode 106: Total Reward = 1\n",
      "Episode 107: Total Reward = 0.6\n",
      "Episode 108: Total Reward = 1\n",
      "Episode 109: Total Reward = -0.8\n",
      "Episode 110: Total Reward = -0.6\n",
      "model saved\n",
      "Episode 111: Total Reward = 1\n",
      "Episode 112: Total Reward = -1\n",
      "Episode 113: Total Reward = 1\n",
      "Episode 114: Total Reward = 0.6\n",
      "Episode 115: Total Reward = -1\n",
      "model saved\n",
      "Episode 116: Total Reward = 1\n",
      "Episode 117: Total Reward = -1\n",
      "Episode 118: Total Reward = 1\n",
      "Episode 119: Total Reward = -1\n",
      "Episode 120: Total Reward = -0.6\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.select_action(state)\n",
    "\n",
    "        # ç¢ºä¿å‹•ä½œæ ¼å¼æ­£ç¢º\n",
    "        banker_action, bet_action = action\n",
    "        next_state, reward, done, _ = env.step((banker_action, bet_action))  # ç¢ºä¿å’Œç’°å¢ƒå…¼å®¹\n",
    "\n",
    "        # è¨˜éŒ„æ•¸æ“š\n",
    "        states.append(state)\n",
    "        actions.append([banker_action, bet_action])  # ç¢ºä¿ actions æ ¼å¼æ­£ç¢º\n",
    "        log_probs.append(banker_log_prob + bet_log_prob)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # æ›´æ–° PPO\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # é¡¯ç¤ºè¨“ç·´çµæœ\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "    \n",
    "    # æ¯ `save_interval` æ¬¡å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        ppo_agent.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(agent, policy_model_path, value_model_path):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥ policy å’Œ value æ¨¡å‹ï¼Œä¸¦é¡¯ç¤ºç•¶å‰ä½¿ç”¨çš„æ¨¡å‹åç¨±ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹: policy -> {policy_model_path}, value -> {value_model_path}\")\n",
    "    agent.policy.load_state_dict(torch.load(policy_model_path))\n",
    "    agent.value.load_state_dict(torch.load(value_model_path))\n",
    "    print(\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(env, agent, policy_model=\"D:\\python\\poker_gto\\ppo_models\\ppo_model_policy.pth\", value_model=\"D:\\python\\poker_gto\\ppo_models\\ppo_model_value.pth\"):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œè®“ä½¿ç”¨è€…è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼Œä¸¦è®“æ¨¡å‹æ±ºç­–æ¶èŠèˆ‡ä¸‹æ³¨å€ç‡\n",
    "    \"\"\"\n",
    "    print(\"\\U0001F50D æ¸¬è©¦æ¨¡å¼å•Ÿå‹•ï¼è¼¸å…¥ `exit` å¯é›¢é–‹æ¸¬è©¦æ¨¡å¼ã€‚\")\n",
    "    \n",
    "    # ç¢ºä¿æ¨¡å‹åœ¨ GPU ä¸Šé‹è¡Œ\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    load_model(agent, policy_model, value_model)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # å–å¾—ç’°å¢ƒçš„åˆå§‹ç‹€æ…‹\n",
    "            state = env.reset()\n",
    "\n",
    "            # è®“ä½¿ç”¨è€…è¼¸å…¥ 4 å¼µæ‰‹ç‰Œ\n",
    "            print(\"\\nè«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\")\n",
    "            input_cards = input().strip()\n",
    "\n",
    "            if input_cards.lower() == 'exit':\n",
    "                print(\"\\U0001F44B æ¸¬è©¦çµæŸï¼\")\n",
    "                break\n",
    "\n",
    "            input_cards = input_cards.split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"âŒ éŒ¯èª¤ï¼è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œçš„èŠ±è‰²èˆ‡æ•¸å€¼ï¼ˆå…± 8 å€‹å­—ä¸²ï¼‰ã€‚\")\n",
    "                continue\n",
    "\n",
    "            # è§£æè¼¸å…¥çš„æ‰‹ç‰Œ\n",
    "            player_hand = [(input_cards[i], input_cards[i + 1]) for i in range(0, 8, 2)]\n",
    "            print(f\"\\U0001F3B4 ä½ çš„æ‰‹ç‰Œ: {player_hand}\")\n",
    "\n",
    "            # æ›´æ–° stateï¼Œç¢ºä¿æ‰‹ç‰Œè³‡è¨Šæ­£ç¢º\n",
    "            for i in range(4):\n",
    "                state[i * 2] = get_suit_rank(player_hand[i])  # èŠ±è‰²\n",
    "                state[i * 2 + 1] = get_card_rank(player_hand[i])  # é»æ•¸\n",
    "\n",
    "            # è½‰æ›ç‚º PyTorch Tensor ä¸¦ç§»è‡³ GPU\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # æ¨¡å‹é æ¸¬æ¶èŠå€ç‡\n",
    "            with torch.no_grad():\n",
    "                banker_dist, bet_dist = agent.policy(state_tensor)\n",
    "                print(f\"ğŸ” banker_dist: {banker_dist}\")  # æ‰“å° banker_dist çš„çµæ§‹\n",
    "                print(f\"ğŸ” bet_dist: {bet_dist}\")  # æ‰“å° bet_dist çš„çµæ§‹\n",
    "                \n",
    "                # æ ¹æ“šæ¨¡å‹è¼¸å‡ºçµæ§‹æ±ºå®šå¦‚ä½•å–å¾—æ¦‚ç‡\n",
    "                if hasattr(banker_dist, 'probs'):\n",
    "                    banker_action = torch.argmax(banker_dist.probs).item()\n",
    "                else:\n",
    "                    # å¦‚æœ banker_dist æ²’æœ‰ 'probs' å±¬æ€§ï¼Œå¯ä»¥æ‰“å° banker_dist æˆ–ä½œå…¶ä»–è™•ç†\n",
    "                    banker_action = torch.argmax(banker_dist).item()\n",
    "                    \n",
    "                if hasattr(bet_dist, 'probs'):\n",
    "                    bet_action = torch.argmax(bet_dist.probs).item()\n",
    "                else:\n",
    "                    bet_action = torch.argmax(bet_dist).item()\n",
    "\n",
    "            print(f\"\\U0001F916 æ¨¡å‹é æ¸¬çš„æ¶èŠå€ç‡: {banker_action}\")\n",
    "\n",
    "            # è®“ä½¿ç”¨è€…è¼¸å…¥æ˜¯å¦æˆåŠŸæ¶èŠ\n",
    "            is_banker = input(\"âœ… æ˜¯å¦æ¶åˆ°èŠï¼Ÿ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"\\U0001F389 ä½ æ˜¯èŠå®¶ï¼ä¸éœ€è¦ä¸‹æ³¨\")\n",
    "            else:\n",
    "                print(\"æ²’æœ‰æ¶åˆ°èŠå®¶\")\n",
    "                banker_multiplier = float(input(\"\\U0001F4E2 è«‹è¼¸å…¥èŠå®¶çš„å€ç‡: \").strip())\n",
    "                print(f\"\\U0001F916 èŠå®¶çš„å€ç‡: {banker_multiplier}\")\n",
    "\n",
    "                # æ›´æ–° state ä¸­çš„èŠå®¶å€ç‡\n",
    "                state[-4] = banker_multiplier\n",
    "\n",
    "                # è½‰æ›ç‚º PyTorch Tensor ä¸¦ç§»è‡³ GPU\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                # æ¨¡å‹é æ¸¬ä¸‹æ³¨å€ç‡\n",
    "                with torch.no_grad():\n",
    "                    _, bet_dist = agent.policy(state_tensor)\n",
    "                    print(f\"ğŸ” bet_dist: {bet_dist}\")  # æ‰“å° bet_dist çš„çµæ§‹\n",
    "\n",
    "                    if hasattr(bet_dist, 'probs'):\n",
    "                        bet_action = torch.argmax(bet_dist.probs).item()\n",
    "                    else:\n",
    "                        bet_action = torch.argmax(bet_dist).item()\n",
    "\n",
    "                print(f\"\\U0001F916 æ¨¡å‹å»ºè­°çš„ä¸‹æ³¨å€ç‡: {bet_action}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ¸¬è©¦æ¨¡å¼å•Ÿå‹•ï¼è¼¸å…¥ `exit` å¯é›¢é–‹æ¸¬è©¦æ¨¡å¼ã€‚\n",
      "ğŸ” æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹: policy -> D:\\python\\poker_gto\\ppo_models\\ppo_model_policy.pth, value -> D:\\python\\poker_gto\\ppo_models\\ppo_model_value.pth\n",
      "âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ´ ä½ çš„æ‰‹ç‰Œ: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "ğŸ” banker_dist: tensor([[3.2925e-04, 9.9866e-01, 2.2667e-04, 3.5804e-05, 7.5206e-04]])\n",
      "ğŸ” bet_dist: tensor([[3.6316e-05, 3.8988e-04, 9.9925e-01, 3.1027e-04, 1.7785e-05]])\n",
      "ğŸ¤– æ¨¡å‹é æ¸¬çš„æ¶èŠå€ç‡: 1\n",
      "æ²’æœ‰æ¶åˆ°èŠå®¶\n",
      "âŒ ç™¼ç”ŸéŒ¯èª¤: could not convert string to float: 'heart J diamond 10 club J spade K'\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\n",
      "ğŸ´ ä½ çš„æ‰‹ç‰Œ: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'K')]\n",
      "ğŸ” banker_dist: tensor([[4.1909e-05, 9.9976e-01, 3.5180e-05, 8.5496e-06, 1.5434e-04]])\n",
      "ğŸ” bet_dist: tensor([[2.0682e-05, 1.7232e-04, 9.9945e-01, 3.4962e-04, 5.6794e-06]])\n",
      "ğŸ¤– æ¨¡å‹é æ¸¬çš„æ¶èŠå€ç‡: 1\n",
      "æ²’æœ‰æ¶åˆ°èŠå®¶\n",
      "âŒ ç™¼ç”ŸéŒ¯èª¤: could not convert string to float: 'heart J diamond Q club K spade K'\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\n",
      "ğŸ´ ä½ çš„æ‰‹ç‰Œ: [('heart', 'J'), ('diamond', 'K'), ('club', 'Q'), ('spade', 'K')]\n",
      "ğŸ” banker_dist: tensor([[1.7999e-05, 9.9990e-01, 1.2202e-05, 2.4744e-06, 6.9479e-05]])\n",
      "ğŸ” bet_dist: tensor([[5.1391e-06, 5.6284e-05, 9.9983e-01, 1.0886e-04, 1.2246e-06]])\n",
      "ğŸ¤– æ¨¡å‹é æ¸¬çš„æ¶èŠå€ç‡: 1\n",
      "ğŸ‰ ä½ æ˜¯èŠå®¶ï¼ä¸éœ€è¦ä¸‹æ³¨\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\n",
      "ğŸ‘‹ æ¸¬è©¦çµæŸï¼\n"
     ]
    }
   ],
   "source": [
    "test_trained_model(env, ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å€ç‡ 3: 100 æ¬¡ (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({3: 100})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def generate_random_hand():\n",
    "    suits = [\"heart\", \"diamond\", \"club\", \"spade\"]\n",
    "    ranks = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\", \"A\"]\n",
    "    hand = np.random.choice([f\"{suit} {rank}\" for suit in suits for rank in ranks], 4, replace=False)\n",
    "    hand = [tuple(card.split()) for card in hand]\n",
    "    return hand\n",
    "\n",
    "def test_banker_distribution(env, agent, num_tests=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    \n",
    "    banker_choices = []\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        state = env.reset()\n",
    "        player_hand = generate_random_hand()\n",
    "        \n",
    "        for i in range(4):\n",
    "            state[i * 2] = get_suit_rank(player_hand[i])\n",
    "            state[i * 2 + 1] = get_card_rank(player_hand[i])\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            banker_dist, _ = agent.policy(state_tensor)\n",
    "            banker_action = torch.argmax(banker_dist).item()\n",
    "            banker_choices.append(banker_action)\n",
    "    \n",
    "    # è¨ˆç®—åˆ†å¸ƒ\n",
    "    counter = Counter(banker_choices)\n",
    "    total = sum(counter.values())\n",
    "    for action, count in sorted(counter.items()):\n",
    "        print(f\"å€ç‡ {action}: {count} æ¬¡ ({count / total:.2%})\")\n",
    "    \n",
    "    return counter\n",
    "\n",
    "# æ¸¬è©¦ 100 æ¬¡éš¨æ©Ÿæ‰‹ç‰Œçš„ banker_dist åˆ†å¸ƒ\n",
    "test_banker_distribution(env, ppo_agent, num_tests=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ¸¬è©¦æ¨¡å¼å•Ÿå‹•ï¼è¼¸å…¥ `exit` å¯é›¢é–‹æ¸¬è©¦æ¨¡å¼ã€‚\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\n",
      "ğŸ´ ä½ çš„æ‰‹ç‰Œ: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "ğŸ“Š banker_dist çµ±è¨ˆ (æ¸¬è©¦ 100 æ¬¡): {3: 100}\n",
      "\n",
      "è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\n",
      "ğŸ‘‹ æ¸¬è©¦çµæŸï¼\n"
     ]
    }
   ],
   "source": [
    "test_trained_model(env, ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do list\n",
    "* **(finish)** è¨“ç·´æ™‚æ‡‰è©²åŒ…å«èŠ±è‰² : å¯èƒ½å°‡èŠ±è‰²ä¹Ÿè½‰ç‚ºæ•¸å€¼ï¼Œè®Šæˆä¸€å€‹ 2 ç¶­åº¦çš„ state  <br>\n",
    "* **(finish)** è¨“ç·´å¾Œæ‡‰è©²å¯ä»¥ä¿å­˜æ¨¡å‹ï¼Œä¸¦ä¸”ç–Šå®¶æ¯æ¬¡è¨“ç·´çš„æˆæœä¸Šå» <br>\n",
    "* æ–°å¢ä¸€å€‹è¼¸å…¥ : å‡è¨­æˆ‘ä¸æ˜¯èŠå®¶æ™‚ï¼Œç¾åœ¨çš„å€ç‡æ˜¯å¹¾å€ï¼Œé€™æœƒå½±éŸ¿åˆ°æˆ‘å¾ŒçºŒçš„ä¸‹æ³¨ç­–ç•¥ <br>\n",
    "* <b>(maybe finish)</b>è¼¸å…¥æˆ‘çš„æ‰‹ç‰Œä¹‹é¡çš„è³‡è¨Šå¾Œæ‡‰è©²è¦å¯ä»¥ç•¶ä½œå›æ¸¬ï¼Œç´€éŒ„åˆ°æ¨¡å‹è¨“ç·´ç•¶ä¸­ï¼Œä¸¦åŠ ä»¥æ”¹é€² <br>\n",
    "\n",
    "* å®Œæˆå¾Œå¯èƒ½å¯ä»¥æ¶è¨­ç°¡å–®çš„ app æˆ– api  <br>\n",
    "* å®Œæˆå¾Œå¯ä»¥æ¥è‘—æ”¹åš å¾·å·æ’²å…‹çš„è¨“ç·´ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_with_real_hands(env, agent):\n",
    "    \"\"\"\n",
    "    è®“ä½¿ç”¨è€…è¼¸å…¥å¯¦éš›æ‰‹ç‰Œï¼Œè®“ AI æä¾›æ±ºç­–å»ºè­°ï¼Œä¸¦å°‡çµæœå›å ±çµ¦æ¨¡å‹ï¼Œå¢å¼·å­¸ç¿’\n",
    "    \"\"\"\n",
    "    print(\"\\U0001F4CA é€²å…¥å›æ¸¬æ¨¡å¼ï¼è¼¸å…¥ `exit` å¯é›¢é–‹å›æ¸¬æ¨¡å¼ã€‚\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            state = env.reset()\n",
    "\n",
    "            print(\"\\nè«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œï¼ˆæ ¼å¼ï¼šheart J diamond 10 club J spade Aï¼‰ï¼Œæˆ–è¼¸å…¥ `exit` é›¢é–‹:\")\n",
    "            input_cards = input().strip()\n",
    "\n",
    "            if input_cards.lower() == 'exit':\n",
    "                print(\"\\U0001F44B å›æ¸¬çµæŸï¼\")\n",
    "                break\n",
    "\n",
    "            input_cards = input_cards.split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"âŒ éŒ¯èª¤ï¼è«‹è¼¸å…¥ 4 å¼µæ‰‹ç‰Œçš„èŠ±è‰²èˆ‡æ•¸å€¼ï¼ˆå…± 8 å€‹å­—ä¸²ï¼‰ã€‚\")\n",
    "                continue\n",
    "\n",
    "            player_hand = [(input_cards[i], input_cards[i + 1]) for i in range(0, 8, 2)]\n",
    "            print(f\"\\U0001F3B4 ä½ çš„æ‰‹ç‰Œ: {player_hand}\")\n",
    "\n",
    "            for i in range(4):\n",
    "                state[i] = card_value(player_hand[i])\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                banker_dist, bet_dist = agent.policy(state_tensor)\n",
    "                banker_action = torch.argmax(banker_dist.probs).item()\n",
    "                bet_action = torch.argmax(bet_dist.probs).item()\n",
    "\n",
    "            print(f\"\\U0001F916 AI å»ºè­°çš„æ¶èŠå€ç‡: {banker_action}\")\n",
    "\n",
    "            is_banker = input(\"âœ… æ˜¯å¦æ¶åˆ°èŠï¼Ÿ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"\\U0001F389 ä½ æ˜¯èŠå®¶ï¼ä¸éœ€è¦ä¸‹æ³¨\")\n",
    "                bet_action = 0\n",
    "            else:\n",
    "                print(f\"\\U0001F916 AI å»ºè­°çš„ä¸‹æ³¨å€ç‡: {bet_action}\")\n",
    "\n",
    "            reward = float(input(\"\\U0001F4B0 è«‹è¼¸å…¥é€™å±€çš„æœ€çµ‚æ”¶ç›Šï¼ˆè² å€¼ä»£è¡¨è™§æï¼‰: \").strip())\n",
    "\n",
    "            states = [state]\n",
    "            actions = [[banker_action, bet_action]]\n",
    "            rewards = [reward]\n",
    "            dones = [True]\n",
    "\n",
    "            agent.update(states, actions, rewards, dones)\n",
    "            print(\"\\U0001F4C8 AI å·²å­¸ç¿’é€™å±€çš„çµæœï¼\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parellel computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
