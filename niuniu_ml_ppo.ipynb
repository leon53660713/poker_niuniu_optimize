{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用機器學習的方法改善策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niuniu_func import *\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('diamond', '7'), ('diamond', '9'), ('diamond', '6'), ('club', '4')], [('heart', 'J'), ('club', '2'), ('club', '6'), ('diamond', 'K')], [('heart', '7'), ('spade', '3'), ('heart', '3'), ('heart', '5')], [('spade', 'Q'), ('club', '3'), ('diamond', '3'), ('heart', '10')]]\n",
      "[4, 4, 1, 4]\n",
      "4\n",
      "0\n",
      "[4, 4, 1, 4]\n",
      "[('diamond', '7'), ('diamond', '9'), ('diamond', '6'), ('club', '4')]\n"
     ]
    }
   ],
   "source": [
    "deck = generate_deck()\n",
    "random.shuffle(deck)\n",
    "players = [[deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "print(players)\n",
    "bank_multipliers = [simulate_ev(players[i], 100000)[0] for i in range(4)]\n",
    "print(bank_multipliers)\n",
    "max_bet = max(bank_multipliers)\n",
    "print(max_bet)\n",
    "banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "banker_index = random.choice(banker_candidates)\n",
    "print(banker_index)\n",
    "banker_multiplier = max_bet\n",
    "print(bank_multipliers)\n",
    "banker_hand = players[banker_index]\n",
    "print(banker_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diamond', '7'), ('diamond', '9'), ('diamond', '6'), ('club', '4')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "bets = [0] * 4\n",
    "is_banker = (banker_index == 0)\n",
    "if is_banker:\n",
    "    bets[0] = 0  # 莊家不下注\n",
    "    for i in range(1, 4):\n",
    "        have_niu = banker_multiplier >= 3\n",
    "        bets[i] = calculate_ev_against_banker(players[i], 100000, have_niu)[1]\n",
    "else:\n",
    "    # 4 step : if myslef is not banker, decide the bet\n",
    "    # also use calculate_ev_against_banker(player_hand, num_simulations=100000, have_niu=True)\n",
    "    # if banker multi is 3 or 4, have_niu == True, otherwise, have_niu == Flase\n",
    "    have_niu = banker_multiplier >= 3\n",
    "    bets[0] = calculate_ev_against_banker(players[0], 100000, have_niu)[1]\n",
    "    \n",
    "print(bets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    players[i].append(deck.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diamond', '7'),\n",
       " ('diamond', '9'),\n",
       " ('diamond', '6'),\n",
       " ('club', '4'),\n",
       " ('spade', '6')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banker_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diamond', '7'),\n",
       " ('diamond', '9'),\n",
       " ('diamond', '6'),\n",
       " ('club', '4'),\n",
       " ('spade', '6')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 6 step : caculate ev of myself\n",
    "# if myself is banker, caculate the payout of sum me against other 3 players\n",
    "# if myself is not banke, caculate the payout of me agaginst banker\n",
    "# the payout == calculate_payout(player_hand, banker_hand, verbose=False) multi * bet\n",
    "if is_banker:\n",
    "    # If I am the banker, calculate the payout against other 3 players\n",
    "    total_payout = -sum(\n",
    "        calculate_payout(players[i], banker_hand, False) * bets[i] * banker_multiplier\n",
    "        for i in range(4) if i != banker_index\n",
    "    )\n",
    "else:\n",
    "    # If I am not the banker, calculate the payout against the banker\n",
    "    total_payout = calculate_payout(players[0], banker_hand, False) * bets[0] * banker_multiplier\n",
    "\n",
    "# 7 step : get reward\n",
    "reward = total_payout\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env of niuniu\n",
    "# set myself as player 0\n",
    "class NiuNiuEnv:\n",
    "    # init \n",
    "    def __init__(self):\n",
    "        # generate deck\n",
    "        self.deck = self.generate_deck()\n",
    "        # generate player == 4\n",
    "        self.players = [[] for _ in range(4)]\n",
    "        # generate banker == none\n",
    "        self.banker_index = -1\n",
    "        # banker multiplier\n",
    "        self.banker_multiplier = 1\n",
    "        # bet number\n",
    "        self.bets = [0, 0, 0, 0]\n",
    "        # generate state\n",
    "        self.state = None\n",
    "        # reset\n",
    "        self.reset()\n",
    "\n",
    "    # generate deck\n",
    "    def generate_deck(self):\n",
    "        suits = ['heart', 'spade', 'diamond', 'club']\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "        deck = [(suit, rank) for suit in suits for rank in ranks]\n",
    "        return deck\n",
    "\n",
    "    # reset\n",
    "    def reset(self):\n",
    "        # regenerate deck & shuffle\n",
    "        self.deck = self.generate_deck()\n",
    "        random.shuffle(self.deck)\n",
    "        # every player have 4 cards\n",
    "        self.players = [[self.deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "        # init bets\n",
    "        self.bets = [0] * 4\n",
    "        self.banker_index = -1\n",
    "        self.banker_multiplier = 1\n",
    "\n",
    "        # reload state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "    \n",
    "    # get myself's hand number\n",
    "    def get_state(self):\n",
    "        # myslef's hand\n",
    "        state = [card_value(card) for card in self.players[0]]\n",
    "        # who is banker\n",
    "        state.append(self.banker_index)\n",
    "        # banker multiplayer\n",
    "        state.append(self.banker_multiplier)\n",
    "        # every player's bet\n",
    "        state.extend(self.bets)\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    # step    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: 0-4 為搶莊倍率，5-9 為下注倍率 (共 10 種行為)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 step : decide whether to get banker, using simulate_ev(player_hand, num_simulations=100000)\n",
    "        # to find use what multipler to get\n",
    "        # return 4 player's bank multi\n",
    "        ### bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        # use ppo to decide whether to get banker\n",
    "        self.banker_bid = action if action < 5 else 0\n",
    "        # other player use simulate_ev to decide\n",
    "        bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        bank_multipliers[0] = self.banker_bid\n",
    "        # run time : 22s\n",
    "\n",
    "        # 2 step : decide final banker, the one have max multiplier is banker\n",
    "        # if more than one have the same multi, random choose one\n",
    "        max_bet = max(bank_multipliers)\n",
    "        banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "        self.banker_index = random.choice(banker_candidates)\n",
    "        self.banker_multiplier = max_bet\n",
    "        banker_hand = self.players[self.banker_index]\n",
    "        # whether myself is banker\n",
    "        is_banker = (self.banker_index == 0)\n",
    "\n",
    "        # 3 step : if myslef is banker, no need to decide the bet\n",
    "        # however the other player have to decide the bet(1~5)\n",
    "        # using calculate_ev_against_banker(player_hand, num_simulations=100000, have_niu=True)\n",
    "        # if banker multi is 3 or 4, have_niu == True, otherwise, have_niu == Flase\n",
    "        if is_banker:\n",
    "            self.bets[0] = 0\n",
    "            for i in range(1, 4):\n",
    "                have_niu = self.banker_multiplier >= 3\n",
    "                self.bets[i] = calculate_ev_against_banker(self.players[i], 100000, have_niu)[1]\n",
    "        else:\n",
    "            # 4 step : if myslef is not banker, decide the bet\n",
    "            # also use calculate_ev_against_banker(player_hand, num_simulations=100000, have_niu=True)\n",
    "            # if banker multi is 3 or 4, have_niu == True, otherwise, have_niu == Flase\n",
    "            # let ppo decide bets\n",
    "            self.bets[0] = (action - 4) if action >= 5 else 1\n",
    "\n",
    "\n",
    "        # 5 step : append the 5th card to every player's hand\n",
    "        for i in range(4):\n",
    "            self.players[i].append(self.deck.pop())\n",
    "\n",
    "        # 6 step : caculate ev of myself\n",
    "        # if myself is banker, caculate the payout of sum me against other 3 players\n",
    "        # if myself is not banke, caculate the payout of me agaginst banker\n",
    "        # the payout == calculate_payout(player_hand, banker_hand, verbose=False) multi * bet\n",
    "        if is_banker:\n",
    "            # If I am the banker, calculate the payout against other 3 players\n",
    "            total_payout = -sum(\n",
    "                calculate_payout(self.players[i], banker_hand, False) * self.bets[i] * self.banker_multiplier\n",
    "                for i in range(4) if i != self.banker_index\n",
    "            )\n",
    "        else:\n",
    "            # If I am not the banker, calculate the payout against the banker\n",
    "            total_payout = calculate_payout(self.players[0], banker_hand, False) * self.bets[0] * self.banker_multiplier\n",
    "\n",
    "\n",
    "        # 7 step : get reward\n",
    "        reward = total_payout\n",
    "\n",
    "        # 8 step : finish one round\n",
    "        done = True\n",
    "\n",
    "        # 9 step : reset\n",
    "        self.reset()\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 價值網絡\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 策略網絡\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.value = ValueNetwork(state_dim)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    def train(self, states, actions, log_probs, rewards):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        values = self.value(states).squeeze()\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        new_probs = self.policy(states)\n",
    "        new_dist = Categorical(new_probs)\n",
    "        new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip)\n",
    "        loss_policy = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "        loss_value = (returns - values).pow(2).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        loss_policy.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        self.value_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終推論\n",
    "class NiuNiuDecisionHelper:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def decide_action(self, hand):\n",
    "        state = np.array([card_value(card) for card in hand], dtype=np.float32)\n",
    "        action, _ = self.agent.select_action(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 6x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 14\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     16\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 6x64)"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(state_dim=6, action_dim=10)\n",
    "\n",
    "# 訓練\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        state = new_state\n",
    "\n",
    "    agent.train(states, actions, log_probs, rewards)\n",
    "\n",
    "helper = NiuNiuDecisionHelper(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 確保異常檢測開啟\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=1e-3, gamma=0.99, clip_epsilon=0.2):\n",
    "        self.policy_network = PolicyNetwork(input_dim, action_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.policy_network(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, log_prob, next_state):\n",
    "        self.memory.append((state, action, reward, log_prob, next_state))\n",
    "\n",
    "    def train(self):\n",
    "        if not self.memory:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, log_probs, next_states = zip(*self.memory)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "\n",
    "        # 計算新的 log_prob\n",
    "        new_probs = self.policy_network(states)\n",
    "        new_dist = torch.distributions.Categorical(new_probs)\n",
    "        new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "        # 計算 ratio (並確保不會 in-place 操作)\n",
    "        ratio = torch.exp(new_log_probs - log_probs.detach())\n",
    "\n",
    "        # 計算 advantage\n",
    "        advantages = rewards - rewards.mean()\n",
    "\n",
    "        # PPO 損失函數\n",
    "        unclipped = ratio * advantages\n",
    "        clipped = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "        # 更新策略網路\n",
    "        self.optimizer_policy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer_policy.step()\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "\n",
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=len(env.get_state()), action_dim=10)  # 確保動作空間是 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -12\n",
      "Episode 1, Total Reward: 8\n",
      "Episode 2, Total Reward: -12\n",
      "Episode 3, Total Reward: -24\n",
      "Episode 4, Total Reward: -16\n",
      "Episode 5, Total Reward: -12\n",
      "Episode 6, Total Reward: -8\n",
      "Episode 7, Total Reward: 8\n",
      "Episode 8, Total Reward: 4\n",
      "Episode 9, Total Reward: 16\n",
      "Episode 10, Total Reward: -4\n",
      "Episode 11, Total Reward: -8\n",
      "Episode 12, Total Reward: -16\n",
      "Episode 13, Total Reward: 4\n",
      "Episode 14, Total Reward: -60\n",
      "Episode 15, Total Reward: 8\n",
      "Episode 16, Total Reward: 3\n",
      "Episode 17, Total Reward: 5\n",
      "Episode 18, Total Reward: -48\n",
      "Episode 19, Total Reward: -16\n",
      "Episode 20, Total Reward: 6\n",
      "Episode 21, Total Reward: -8\n",
      "Episode 22, Total Reward: 24\n",
      "Episode 23, Total Reward: -4\n",
      "Episode 24, Total Reward: 8\n",
      "Episode 25, Total Reward: 4\n",
      "Episode 26, Total Reward: -4\n",
      "Episode 27, Total Reward: 20\n",
      "Episode 28, Total Reward: -24\n",
      "Episode 29, Total Reward: -4\n",
      "Episode 30, Total Reward: -8\n",
      "Episode 31, Total Reward: 4\n",
      "Episode 32, Total Reward: -2\n",
      "Episode 33, Total Reward: -4\n",
      "Episode 34, Total Reward: 12\n",
      "Episode 35, Total Reward: -8\n",
      "Episode 36, Total Reward: 12\n",
      "Episode 37, Total Reward: -16\n",
      "Episode 38, Total Reward: -8\n",
      "Episode 39, Total Reward: -4\n",
      "Episode 40, Total Reward: -8\n",
      "Episode 41, Total Reward: 4\n",
      "Episode 42, Total Reward: 12\n",
      "Episode 43, Total Reward: -24\n",
      "Episode 44, Total Reward: 12\n",
      "Episode 45, Total Reward: -6\n",
      "Episode 46, Total Reward: -24\n",
      "Episode 47, Total Reward: 1\n",
      "Episode 48, Total Reward: 12\n",
      "Episode 49, Total Reward: -8\n",
      "Episode 50, Total Reward: 1\n",
      "Episode 51, Total Reward: -4\n",
      "Episode 52, Total Reward: -32\n",
      "Episode 53, Total Reward: 12\n",
      "Episode 54, Total Reward: 6\n",
      "Episode 55, Total Reward: -3\n",
      "Episode 56, Total Reward: -8\n",
      "Episode 57, Total Reward: -8\n",
      "Episode 58, Total Reward: 24\n",
      "Episode 59, Total Reward: 8\n",
      "Episode 60, Total Reward: -24\n",
      "Episode 61, Total Reward: 16\n",
      "Episode 62, Total Reward: -4\n",
      "Episode 63, Total Reward: 12\n",
      "Episode 64, Total Reward: -4\n",
      "Episode 65, Total Reward: -8\n",
      "Episode 66, Total Reward: -80\n",
      "Episode 67, Total Reward: 8\n",
      "Episode 68, Total Reward: 3\n",
      "Episode 69, Total Reward: 40\n",
      "Episode 70, Total Reward: 36\n",
      "Episode 71, Total Reward: -36\n",
      "Episode 72, Total Reward: 8\n",
      "Episode 73, Total Reward: -4\n",
      "Episode 74, Total Reward: 12\n",
      "Episode 75, Total Reward: -12\n",
      "Episode 76, Total Reward: 12\n",
      "Episode 77, Total Reward: -12\n",
      "Episode 78, Total Reward: 2\n",
      "Episode 79, Total Reward: -2\n",
      "Episode 80, Total Reward: -9\n",
      "Episode 81, Total Reward: 40\n",
      "Episode 82, Total Reward: 4\n",
      "Episode 83, Total Reward: 20\n",
      "Episode 84, Total Reward: -8\n",
      "Episode 85, Total Reward: -24\n",
      "Episode 86, Total Reward: -8\n",
      "Episode 87, Total Reward: 12\n",
      "Episode 88, Total Reward: 8\n",
      "Episode 89, Total Reward: 12\n",
      "Episode 90, Total Reward: -8\n",
      "Episode 91, Total Reward: -12\n",
      "Episode 92, Total Reward: -8\n",
      "Episode 93, Total Reward: 8\n",
      "Episode 94, Total Reward: -48\n",
      "Episode 95, Total Reward: 12\n",
      "Episode 96, Total Reward: -24\n",
      "Episode 97, Total Reward: -24\n",
      "Episode 98, Total Reward: 12\n",
      "Episode 99, Total Reward: -4\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100  # 訓練 10,000 場遊戲\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, log_prob, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.train()  # 更新 PPO\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # if episode % 100 == 0:\n",
    "    #     print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# everage : 22.5s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 決定下注，倍率為 3 倍\n"
     ]
    }
   ],
   "source": [
    "# 假設你的環境已經初始化\n",
    "env = NiuNiuEnv()\n",
    "\n",
    "# 假設你已經訓練好 PPO Agent\n",
    "agent = PPOAgent(input_dim=env.get_state().shape[0], action_dim=10)\n",
    "\n",
    "my_hand=[('diamond', 'K'), ('diamond', '9'), ('diamond', '6'), ('club', '4')]\n",
    "# 測試：給定目前的手牌，讓 AI 決定下注策略\n",
    "state = env.get_state()  # 取得手牌狀態\n",
    "action, _ = agent.select_action(state)  # 讓 PPO 決定動作\n",
    "\n",
    "# 解讀動作\n",
    "if action < 5:\n",
    "    print(f\"AI 決定搶莊，倍率為 {action + 1} 倍\")\n",
    "else:\n",
    "    print(f\"AI 決定下注，倍率為 {action - 4} 倍\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiuNiuDecisionHelper:\n",
    "    def __init__(self, agent, env):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.suit_map = {'diamond': 0, 'club': 1, 'heart': 2, 'spade': 3}\n",
    "        self.rank_map = {'J': 11, 'Q': 12, 'K': 13, 'A': 14}\n",
    "        self.rank_map.update({str(i): i for i in range(2, 11)})\n",
    "\n",
    "    def preprocess_state(self, hand_cards):\n",
    "        \"\"\"\n",
    "        將手牌轉換為數值型 NumPy 陣列\n",
    "        :param hand_cards: 玩家的手牌 (ex: [('diamond', '7'), ('diamond', '9')])\n",
    "        :return: 數值型 NumPy 陣列\n",
    "        \"\"\"\n",
    "        numerical_hand = []\n",
    "        for suit, rank in hand_cards:\n",
    "            suit_num = self.suit_map[suit]\n",
    "            rank_num = self.rank_map[rank]\n",
    "            numerical_hand.extend([suit_num, rank_num])  \n",
    "        return np.array(numerical_hand, dtype=np.float32)\n",
    "\n",
    "    def decide_qiangzhuang(self, hand_cards):\n",
    "        \"\"\"\n",
    "        根據手牌決定是否搶莊，並提供搶莊倍率 (1~4 倍) 或 不搶時下注倍率 (1~5 倍)\n",
    "        \"\"\"\n",
    "        processed_state = self.preprocess_state(hand_cards)\n",
    "        action, _ = self.agent.select_action(processed_state)\n",
    "\n",
    "        if action < 4:  # 0~3 搶莊 (倍率 1~4)\n",
    "            return True, action + 1\n",
    "        else:  # 4~9 不搶莊 (倍率 1~5)\n",
    "            return False, action - 3\n",
    "\n",
    "    def compute_banker_loss(self, hand_cards, final_banker, qiangzhuang_multiplier):\n",
    "        \"\"\"\n",
    "        計算搶莊失敗時應該下注的倍率\n",
    "        :param hand_cards: 玩家的手牌\n",
    "        :param final_banker: 是否成為莊家 (True/False)\n",
    "        :param qiangzhuang_multiplier: 當初搶莊時的倍率\n",
    "        :return: 搶莊失敗時，應該下注的倍率 (1~5 倍)，若成功成為莊家則回傳 None\n",
    "        \"\"\"\n",
    "        if final_banker:\n",
    "            return None  # 成功當莊，不用算賠率\n",
    "\n",
    "        # **使用 AI 來決定下注倍率**\n",
    "        processed_state = self.preprocess_state(hand_cards)\n",
    "        action, _ = self.agent.select_action(processed_state)\n",
    "\n",
    "        if action >= 4:  # 4~9 代表下注 (對應倍率 1~5)\n",
    "            return action - 3\n",
    "        else:\n",
    "            return 1  # 預設為最低倍率 1 倍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "應該搶莊，建議倍率: 3 倍\n",
      "搶莊失敗，AI 建議下注倍率: 1 倍\n"
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=8, action_dim=10)  # 輸入 8 維，輸出 10 個動作\n",
    "decision_helper = NiuNiuDecisionHelper(agent, env)\n",
    "\n",
    "hand_cards = [('diamond', 'J'), ('diamond', 'K'), ('diamond', '10'), ('club', '4')]\n",
    "should_qiang, multiplier = decision_helper.decide_qiangzhuang(hand_cards)\n",
    "\n",
    "if should_qiang:\n",
    "    print(f\"應該搶莊，建議倍率: {multiplier} 倍\")\n",
    "    final_banker = bool(int(input(\"最後是否成功成為莊家？(1: 是, 0: 否): \")))\n",
    "    banker_loss = decision_helper.compute_banker_loss(hand_cards, final_banker, multiplier)\n",
    "    if banker_loss:\n",
    "        print(f\"搶莊失敗，AI 建議下注倍率: {banker_loss} 倍\")\n",
    "else:\n",
    "    print(f\"不搶莊，建議下注倍率: {multiplier} 倍\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 代理人\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=0.002, gamma=0.99, epsilon=0.2, update_steps=5):\n",
    "        self.policy = PolicyNetwork(input_dim, action_dim)  # ✅ 確保這裡有 PolicyNetwork\n",
    "        self.value = ValueNetwork(input_dim)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.update_steps = update_steps\n",
    "        self.memory = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\" 使用策略網絡選擇行動 \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # 加維度 (batch=1)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, next_state):\n",
    "        \"\"\" 存儲交互數據 \"\"\"\n",
    "        self.memory.append((state, action, reward, log_prob, next_state))\n",
    "\n",
    "    def compute_discounted_rewards(self, rewards):\n",
    "        \"\"\" 計算折扣回報 G_t \"\"\"\n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "        return torch.FloatTensor(discounted_rewards)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" 使用 PPO 來訓練策略網絡和價值網絡 \"\"\"\n",
    "        if len(self.memory) == 0:\n",
    "            return\n",
    "        \n",
    "        # 1. 解析記憶\n",
    "        states, actions, rewards, log_probs, next_states = zip(*self.memory)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        rewards = self.compute_discounted_rewards(rewards)\n",
    "        \n",
    "        # 2. 計算優勢值 Advantage = G_t - V(s)\n",
    "        values = self.value(states).squeeze()\n",
    "        advantages = rewards - values.detach().clone()  # 使用 clone() 來避免原地修改\n",
    "\n",
    "        # 3. 更新策略網絡 (PPO Loss)\n",
    "        for _ in range(self.update_steps):  # 重複多次更新\n",
    "            probs = self.policy(states)\n",
    "            dist = Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - log_probs)  # 重要性權重\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "            policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "            \n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)  # 確保計算圖不會被釋放\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "        # 4. 更新價值網絡 (MSE 損失)\n",
    "        value_loss = (self.value(states).squeeze() - rewards).pow(2).mean()\n",
    "        self.optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optimizer_value.step()\n",
    "\n",
    "        # 5. 清空記憶\n",
    "        self.memory = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     18\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# 確保 reward 是數字\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 訓練一次\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [116]\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m     policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmin(ratio \u001b[38;5;241m*\u001b[39m advantages, clipped_ratio \u001b[38;5;241m*\u001b[39m advantages)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 確保計算圖不會被釋放\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 4. 更新價值網絡 (MSE 損失)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=len(env.get_state()), action_dim=10)\n",
    "\n",
    "num_episodes = 10  # 進行 10 次訓練\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)  # 注意這裡解構返回值\n",
    "        \n",
    "        agent.store_transition(state, action, reward, log_prob, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward  # 確保 reward 是數字\n",
    "\n",
    "    agent.train()  # 訓練一次\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# 最後的訓練結果\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 12\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 呼叫 step() 並獲得下一步狀態和獎勳\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)"
     ]
    }
   ],
   "source": [
    "# 訓練 PPO\n",
    "env = NiuNiuEnv()\n",
    "agent = PPOAgent(input_dim=4, action_dim=10)\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    memory = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        \n",
    "        # 呼叫 step() 並獲得下一步狀態和獎勳\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        memory.append((state, action, reward, log_prob, next_state))\n",
    "        state = next_state\n",
    "    \n",
    "    agent.train(memory)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Last Reward: {reward}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 測試 1: 使用代理選擇行為\u001b[39;00m\n\u001b[0;32m      8\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 9\u001b[0m action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChosen Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Log Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_prob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 測試 2: 執行一步遊戲並查看結果\u001b[39;00m\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 4x64)"
     ]
    }
   ],
   "source": [
    "# 測試主程式\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化遊戲環境和代理\n",
    "    env = NiuNiuEnv()\n",
    "    agent = PPOAgent(input_dim=4, action_dim=10)\n",
    "    \n",
    "    # 測試 1: 使用代理選擇行為\n",
    "    state = env.reset()\n",
    "    action, log_prob = agent.select_action(state)\n",
    "    print(f\"Chosen Action: {action}, Log Probability: {log_prob}\")\n",
    "    \n",
    "    # 測試 2: 執行一步遊戲並查看結果\n",
    "    is_banker = (action < 5)  # 假設選擇 0-4 為搶莊，5-9 為下注\n",
    "    next_state, reward, done = env.step(action, is_banker)\n",
    "    print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設模型已經訓練完成並保存在 agent 中\n",
    "def get_action_from_model(hand, agent, is_banker=False):\n",
    "    \"\"\"\n",
    "    根據手牌與模型決策是否搶莊，並在搶莊與否時決定下注策略。\n",
    "    hand: 玩家手牌\n",
    "    agent: 訓練好的PPOAgent\n",
    "    is_banker: 是否為莊家，True表示是莊家，False表示閒家\n",
    "    \"\"\"\n",
    "    # 將手牌轉換為模型的狀態向量\n",
    "    state = np.array([card_value(card) for card in hand], dtype=np.float32)  # 轉換為數字狀態\n",
    "    \n",
    "    # 如果是閒家，先決定是否搶莊（選擇動作）\n",
    "    if not is_banker:\n",
    "        action, _ = agent.select_action(state)  # 根據狀態選擇動作\n",
    "        if action < 5:  # 如果選擇的動作是搶莊（0-4表示搶莊）\n",
    "            is_banker = True\n",
    "            print(\"決定搶莊！\")\n",
    "        else:\n",
    "            print(\"決定不搶莊，選擇下注。\")\n",
    "    \n",
    "    # 根據是否搶莊來決定下注\n",
    "    if is_banker:\n",
    "        # 如果是莊家，決定下注策略\n",
    "        action, _ = agent.select_action(state)  # 莊家可以下注的動作範圍是 0-4\n",
    "        print(f\"作為莊家，下注倍率為 {action % 5 + 1}\")\n",
    "    else:\n",
    "        # 如果是閒家，根據手牌決定下注\n",
    "        action, _ = agent.select_action(state)  # 閒家的下注動作範圍是 5-9\n",
    "        print(f\"作為閒家，下注倍率為 {action % 5 + 1}\")\n",
    "\n",
    "    return action\n",
    "\n",
    "# 假設我們有一副手牌\n",
    "player_hand = ['3♠', '7♣', 'K♦', '9♥']  # 玩家手牌\n",
    "\n",
    "# 使用訓練完成的模型來決定策略\n",
    "get_action_from_model(player_hand, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "player_hand = [('heart', '9'), ('diamond', 'J'), ('club', '3'), ('spade', '6'), ('heart', '2')]\n",
    "banker_hand = [('spade', '10'), ('club', 'J'), ('heart', '4'), ('diamond', '6'), ('diamond', '2')]\n",
    "print(type(calculate_payout(player_hand, banker_hand, verbose=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
