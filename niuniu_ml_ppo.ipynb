{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用機器學習的方法改善策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "## niuniu function\n",
    "from niuniu_func import *\n",
    "\n",
    "## caculating\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "\n",
    "## os\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build niuniu env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env of niuniu\n",
    "# set myself as player 0\n",
    "class NiuNiuEnv:\n",
    "    # init \n",
    "    def __init__(self):\n",
    "        # generate deck\n",
    "        self.deck = self.generate_deck()\n",
    "        # generate player == 4\n",
    "        self.players = [[] for _ in range(4)]\n",
    "        self.banker_index = -1\n",
    "        # banker multiplier\n",
    "        self.banker_multiplier = 1\n",
    "        # bet number\n",
    "        self.bets = [0, 0, 0, 0]\n",
    "        # state, 0: bank step, 1: bet step, 2: result step\n",
    "        self.current_phase = 0\n",
    "        # reset\n",
    "        self.reset()\n",
    "\n",
    "    # generate deck\n",
    "    def generate_deck(self):\n",
    "        suits = ['heart', 'spade', 'diamond', 'club']\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "        return [(suit, rank) for suit in suits for rank in ranks]\n",
    "\n",
    "    # reset\n",
    "    def reset(self):\n",
    "        # regenerate deck & shuffle\n",
    "        self.deck = self.generate_deck()        \n",
    "        random.shuffle(self.deck)\n",
    "\n",
    "        # every player have 4 cards\n",
    "        self.players = [[self.deck.pop() for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "        # init bets\n",
    "        self.bets = [0] * 4\n",
    "        self.banker_index = -1\n",
    "        self.banker_multiplier = 1\n",
    "\n",
    "        # init step\n",
    "        self.current_phase = 0\n",
    "\n",
    "        # reload state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "    \n",
    "    # get myself's hand number\n",
    "    def get_state(self):\n",
    "        # myself's hand\n",
    "        state = []\n",
    "        for card in self.players[0]:\n",
    "            state.append(get_suit_rank(card))\n",
    "            state.append(get_card_rank(card))\n",
    "        # which step\n",
    "        state.append(self.current_phase)\n",
    "        # who is banker\n",
    "        state.append(self.banker_index)\n",
    "        # banker multiplayer\n",
    "        state.append(self.banker_multiplier)\n",
    "        # every player's bet\n",
    "        state.extend(self.bets)\n",
    "        return np.array(state, dtype=np.float32)\n",
    "        \n",
    "    # step    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: [banker_action, bet_action]\n",
    "        banker_action: 0-4 is baker multi\n",
    "        bet_action: 1-5 is bet multi\n",
    "        \"\"\"\n",
    "        # unpack action\n",
    "        banker_action, bet_action = action \n",
    "        # banker\n",
    "        self.banker_bid = banker_action\n",
    "        # bet\n",
    "        self.bet_amount = bet_action\n",
    "\n",
    "        \"\"\"\n",
    "        step 1 : decide whether to get banker\n",
    "            * myself : by ppo agent\n",
    "            * others : by simulate_ev to decide\n",
    "        \"\"\"\n",
    "        bank_multipliers = [simulate_ev(self.players[i], 100000)[0] for i in range(4)]\n",
    "        bank_multipliers[0] = self.banker_bid\n",
    "        # run time : 22s\n",
    "\n",
    "        \"\"\"\n",
    "        step 2 : decide final banker(the max multiplier)\n",
    "            * if all not want to be banker, random choose one & set multiplier = 1\n",
    "            * if more than one have same multiplier, random choose one\n",
    "        \"\"\"\n",
    "        max_bet = max(bank_multipliers)\n",
    "        if max_bet == 1:\n",
    "            random_banker = random.choice(range(4))\n",
    "            bank_multipliers[random_banker] = 1\n",
    "        banker_candidates = [i for i, b in enumerate(bank_multipliers) if b == max_bet]\n",
    "        self.banker_index = random.choice(banker_candidates)\n",
    "        self.banker_multiplier = max_bet\n",
    "        banker_hand = self.players[self.banker_index]\n",
    "\n",
    "        # whether myself is banker\n",
    "        is_banker = (self.banker_index == 0)\n",
    "\n",
    "        # go to next action -- bet\n",
    "        self.current_phase = 1\n",
    "\n",
    "        # bet action\n",
    "        if is_banker:\n",
    "            \"\"\"\n",
    "            step 3 : if myself is banker\n",
    "                * I don't need to bet\n",
    "                * others use `calculate_ev_against_banker` to bet, besides\n",
    "                if banker multiplier over 3, we assume banker have niu\n",
    "            \"\"\"\n",
    "            self.bets[0] = self.bet_amount\n",
    "            for i in range(1, 4):\n",
    "                have_niu = self.banker_multiplier >= 3\n",
    "                self.bets[i] = calculate_ev_against_banker(self.players[i], 100000, have_niu)[1]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            step 4 : if myself is not banker\n",
    "                * let ppo decide bet\n",
    "                * others we don't care\n",
    "            \"\"\"\n",
    "            self.bets[0] = max(1, min(5, action[1]))\n",
    "\n",
    "        \"\"\"\n",
    "        step 5 : add the 5th card to every player\n",
    "        \"\"\"\n",
    "        for i in range(4):\n",
    "            self.players[i].append(self.deck.pop())\n",
    "\n",
    "        # go to next action -- result\n",
    "        self.current_phase = 2\n",
    "\n",
    "        \"\"\"\n",
    "        step 6 : caculate ev of myself\n",
    "            * I am banker : caculate payout of the sum of me against others(use negative)\n",
    "            * I am not banker : calculate the payout against the banker\n",
    "        \"\"\"\n",
    "        if is_banker:\n",
    "            # I am banker\n",
    "            total_payout = -sum(\n",
    "                calculate_payout(self.players[i], banker_hand, False) * self.bets[i] * self.banker_multiplier\n",
    "                for i in range(4) if i != self.banker_index\n",
    "            )\n",
    "\n",
    "            \"\"\"\n",
    "            step 7 : caculate reward(scaling & punishing)\n",
    "            \"\"\"\n",
    "            # def max payout : 3 player * max bet(5) * bank multi\n",
    "            max_payout = 3 * 5 * self.banker_multiplier\n",
    "            # # min max function, let reward in [0, 1]\n",
    "            # min_payout = -max_payout\n",
    "            # reward = (total_payout - min_payout) / (max_payout - min_payout)\n",
    "            reward = (total_payout) / (max_payout)\n",
    "\n",
    "            # win big, give prize\n",
    "            if reward > 0.7:\n",
    "                reward += 0.2\n",
    "            # lose, punish\n",
    "            elif reward < 0:\n",
    "                reward -= 0.2\n",
    "            elif reward < -0.5:\n",
    "                reward -= 0.4\n",
    "            elif reward < -0.8:\n",
    "                reward -= 0.6\n",
    "            \n",
    "            # different bank multiplier have different reward\n",
    "            if self.banker_multiplier == 4:\n",
    "                reward -= 0.3\n",
    "            elif self.banker_multiplier == 3:\n",
    "                reward -= 0.15\n",
    "            elif self.banker_multiplier == 2:\n",
    "                reward += 0.1\n",
    "\n",
    "            if self.banker_multiplier >= 3 and total_payout < 0:\n",
    "                reward -= 0.3\n",
    "\n",
    "\n",
    "        else:\n",
    "            # I am not banker\n",
    "            total_payout = calculate_payout(self.players[0], banker_hand, False) * self.bets[0] * self.banker_multiplier\n",
    "            \n",
    "            \"\"\"\n",
    "            step 7 : caculate reward(scaling & punishing)\n",
    "            \"\"\"\n",
    "            # def max payout : max bet(5) * bank multi\n",
    "            max_possible_profit = 5 * self.banker_multiplier\n",
    "            # # min max function, let reward in [0, 1]\n",
    "            # max_possible_loss = -max_possible_profit\n",
    "            # reward = (total_payout - max_possible_loss) / (max_possible_profit - max_possible_loss)\n",
    "            reward = (total_payout) / (max_possible_profit)\n",
    "\n",
    "            # win big, give prize\n",
    "            if reward > 0.8:\n",
    "                reward += 0.3\n",
    "            elif reward > 0.5:\n",
    "                reward += 0.2\n",
    "            elif reward > 0.2:\n",
    "                reward += 0.1\n",
    "            # lose, punish\n",
    "            elif reward < -0.2:\n",
    "                reward -= 0.1\n",
    "            elif reward < -0.5:\n",
    "                reward -= 0.3\n",
    "            elif reward < -0.8:\n",
    "                reward -= 0.6\n",
    "\n",
    "        \"\"\"\n",
    "        step 8 : finish one round\n",
    "        \"\"\"\n",
    "        done = True\n",
    "\n",
    "        \"\"\"\n",
    "        step 9 : reset\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        return self.get_state(), reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple test\n",
    "test whether niuniu env is runnable <br>\n",
    "to avoid getting error of having NaN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [ 1. 10.  2.  9.  4.  2.  2.  1.  0. -1.  1.  0.  0.  0.  0.]\n",
      "Step 0 - State: [ 4.  7.  1.  4.  1.  3.  3. 11.  0. -1.  1.  0.  0.  0.  0.], Reward: -0.7\n",
      "Step 1 - State: [ 4. 13.  2.  9.  2.  3.  3.  3.  0. -1.  1.  0.  0.  0.  0.], Reward: -0.7\n",
      "Step 2 - State: [ 1.  3.  2.  4.  2. 10.  4. 12.  0. -1.  1.  0.  0.  0.  0.], Reward: 0.5\n",
      "Step 3 - State: [ 2. 11.  2.  5.  3. 12.  4.  4.  0. -1.  1.  0.  0.  0.  0.], Reward: 0.5\n",
      "Step 4 - State: [ 1.  4.  3. 11.  3.  1.  1.  9.  0. -1.  1.  0.  0.  0.  0.], Reward: -0.9\n",
      "Step 5 - State: [ 3.  6.  3.  5.  3.  7.  3.  1.  0. -1.  1.  0.  0.  0.  0.], Reward: 1.0\n",
      "Step 6 - State: [ 1.  7.  2.  9.  1.  6.  3.  4.  0. -1.  1.  0.  0.  0.  0.], Reward: 2.3\n",
      "Step 7 - State: [ 4.  5.  3.  3.  3. 13.  3.  2.  0. -1.  1.  0.  0.  0.  0.], Reward: 1.5\n",
      "Step 8 - State: [ 1.  3.  3.  9.  4.  2.  4. 11.  0. -1.  1.  0.  0.  0.  0.], Reward: 0.8\n",
      "Step 9 - State: [ 2.  1.  1.  2.  3.  5.  2. 13.  0. -1.  1.  0.  0.  0.  0.], Reward: -1.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nresult explain :\\n    * the first 8 numbers represent 4 card in hands, (suit, card)\\n    * the others represent the other states\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test env of niuniu\n",
    "def test_env():\n",
    "    env = NiuNiuEnv()\n",
    "    state = env.reset()\n",
    "    print(\"Initial State:\", state)\n",
    "    # test 10 times\n",
    "    for i in range(10):\n",
    "        # random action\n",
    "        action = [np.random.randint(0, 5), np.random.randint(1, 6)]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if np.isnan(state).any():\n",
    "            print(f\"NaN detected in state at step {i}!\")\n",
    "        if np.isnan(reward):\n",
    "            print(f\"NaN detected in reward at step {i}!\")\n",
    "        print(f\"Step {i} - State: {state}, Reward: {reward}\")\n",
    "\n",
    "test_env()\n",
    "# windows run time : 3m 55s\n",
    "# mac run time : 2m 37.1s\n",
    "\"\"\"\n",
    "result explain :\n",
    "    * the first 8 numbers represent 4 card in hands, (suit, card)\n",
    "    * the others represent the other states\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO value network (V(s))\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            # avoid pop NA in normalization\n",
    "            nn.LayerNorm(128, eps=1e-5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128, eps=1e-5),\n",
    "            nn.LeakyReLU(),\n",
    "            # output V(s)\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # let output become (batch, )\n",
    "        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # get banker\n",
    "        self.banker_fc = nn.Linear(128, output_dim1)\n",
    "        # bet\n",
    "        self.bet_fc = nn.Linear(128, output_dim2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        banker_logits = self.banker_fc(x)\n",
    "        bet_logits = self.bet_fc(x)\n",
    "\n",
    "        banker_probs = F.softmax(banker_logits, dim=-1)\n",
    "        bet_probs = F.softmax(bet_logits, dim=-1)\n",
    "\n",
    "        return banker_probs, bet_probs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # turns to batch\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        banker_probs, bet_probs = self.forward(state)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().item()\n",
    "        bet_action = bet_dist.sample().item()\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action))\n",
    "\n",
    "        return (banker_action, bet_action), banker_log_prob, bet_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim1, output_dim2, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=10, model_path=\"niu_ppo_model\", num_envs=8):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_envs = num_envs\n",
    "        \n",
    "        self.policy = PolicyNetwork(input_dim, output_dim1, output_dim2).to(self.device)\n",
    "        self.value = ValueNetwork(input_dim).to(self.device)\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.model_path = model_path\n",
    "        # load saved model\n",
    "        self.load_model()\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        # init\n",
    "        R = torch.zeros(1, dtype=torch.float32).to(self.device)\n",
    "        # ensure rewards & dones is shape (T, 1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            R = rewards[t] + self.gamma * R * (1 - dones[t])\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        return torch.cat(returns).detach()\n",
    "\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, dones):\n",
    "        returns = self.compute_returns(rewards, dones)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        # ensure dimention\n",
    "        actions = torch.tensor(actions, dtype=torch.long).view(-1, 2).to(self.device)\n",
    "        # turns 1D\n",
    "        old_log_probs = torch.tensor(log_probs, dtype=torch.float32).view(-1).to(self.device)\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            banker_probs, bet_probs = self.policy(states)\n",
    "\n",
    "            banker_probs = torch.nan_to_num(banker_probs, nan=0.0)\n",
    "            bet_probs = torch.nan_to_num(bet_probs, nan=0.0)\n",
    "\n",
    "            banker_dist = Categorical(banker_probs)\n",
    "            bet_dist = Categorical(bet_probs)\n",
    "\n",
    "            new_banker_log_prob = banker_dist.log_prob(actions[:, 0])\n",
    "            new_bet_log_prob = bet_dist.log_prob(actions[:, 1] - 1)\n",
    "            new_log_probs = new_banker_log_prob + new_bet_log_prob\n",
    "            new_log_probs = torch.nan_to_num(new_banker_log_prob, nan=0.0) + torch.nan_to_num(new_bet_log_prob, nan=0.0)\n",
    "    \n",
    "            value_estimates = self.value(states).view(-1)\n",
    "            value_estimates = torch.nan_to_num(value_estimates, nan=0.0)\n",
    "\n",
    "            advantages = returns - value_estimates.detach()\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "            self.optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        # state = torch.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        state = torch.nan_to_num(state, nan=0.0)\n",
    "\n",
    "        banker_probs, bet_probs = self.policy(state)\n",
    "        banker_probs = torch.nan_to_num(banker_probs, nan=0.2)\n",
    "        bet_probs = torch.nan_to_num(bet_probs, nan=0.2)\n",
    "\n",
    "        banker_dist = Categorical(banker_probs)\n",
    "        bet_dist = Categorical(bet_probs)\n",
    "\n",
    "        banker_action = banker_dist.sample().cpu().numpy()\n",
    "        bet_action = (bet_dist.sample() + 1).cpu().numpy()\n",
    "\n",
    "        banker_log_prob = banker_dist.log_prob(torch.tensor(banker_action, device=self.device))\n",
    "        bet_log_prob = bet_dist.log_prob(torch.tensor(bet_action - 1, device=self.device))\n",
    "\n",
    "        return np.array([banker_action, bet_action]), banker_log_prob.detach().cpu().numpy(), bet_log_prob.detach().cpu().numpy()\n",
    "    \n",
    "    def save_model(self):\n",
    "        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "        torch.save(self.policy.state_dict(), f\"{self.model_path}_policy.pth\")\n",
    "        torch.save(self.value.state_dict(), f\"{self.model_path}_value.pth\")\n",
    "        print(\"model saved\")\n",
    "\n",
    "    def load_model(self):\n",
    "        policy_path = f\"{self.model_path}_policy.pth\"\n",
    "        value_path = f\"{self.model_path}_value.pth\"\n",
    "\n",
    "        if os.path.exists(policy_path) and os.path.exists(value_path):\n",
    "            self.policy.load_state_dict(torch.load(policy_path))\n",
    "            self.value.load_state_dict(torch.load(value_path))\n",
    "            print(\"load saved model\")\n",
    "        else:\n",
    "            print(\"model not found, start training from begining\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameter\n",
    "num_episodes = 1200\n",
    "batch_size = 2048\n",
    "gamma = 0.99\n",
    "clip_epsilon = 0.2\n",
    "lr=1e-4\n",
    "update_epochs = 10\n",
    "# save interval\n",
    "save_interval = 5\n",
    "\n",
    "# save path\n",
    "save_model_path = \"D:/python/poker_gto/ppo_models/ppo_model_v2\"\n",
    "num_envs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found, start training from begining\n"
     ]
    }
   ],
   "source": [
    "# set niuniu env\n",
    "env = NiuNiuEnv()\n",
    "state_dim = len(env.get_state())\n",
    "# banker bet (0~4)\n",
    "banker_action_dim = 5\n",
    "# player bet (1~5)\n",
    "bet_action_dim = 5\n",
    "\n",
    "# ppo agent\n",
    "ppo_agent = PPOAgent(state_dim, banker_action_dim, bet_action_dim, model_path=save_model_path, num_envs=num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 2.3\n",
      "Episode 2: Total Reward = 2.3\n",
      "Episode 3: Total Reward = 1.3\n",
      "Episode 4: Total Reward = 2.3\n",
      "Episode 5: Total Reward = 3.3\n",
      "model saved\n",
      "Episode 6: Total Reward = 1.3\n",
      "Episode 7: Total Reward = 2.3\n",
      "Episode 8: Total Reward = -1.1\n",
      "Episode 9: Total Reward = -1.1\n",
      "Episode 10: Total Reward = -1.1\n",
      "model saved\n",
      "Episode 11: Total Reward = 1.6666666666666665\n",
      "Episode 12: Total Reward = -1.7000000000000002\n",
      "Episode 13: Total Reward = -1.9000000000000001\n",
      "Episode 14: Total Reward = 1.0\n",
      "Episode 15: Total Reward = 0.05000000000000002\n",
      "model saved\n",
      "Episode 16: Total Reward = 2.3\n",
      "Episode 17: Total Reward = 3.3\n",
      "Episode 18: Total Reward = -0.7166666666666666\n",
      "Episode 19: Total Reward = -1.1\n",
      "Episode 20: Total Reward = -1.1\n",
      "model saved\n",
      "Episode 21: Total Reward = 1.2666666666666666\n",
      "Episode 22: Total Reward = -1.1\n",
      "Episode 23: Total Reward = 0.5\n",
      "Episode 24: Total Reward = 1.3\n",
      "Episode 25: Total Reward = -0.7166666666666666\n",
      "model saved\n",
      "Episode 26: Total Reward = 1.3\n",
      "Episode 27: Total Reward = -4.1\n",
      "Episode 28: Total Reward = -0.5\n",
      "Episode 29: Total Reward = 1.0\n",
      "Episode 30: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 31: Total Reward = -3.1\n",
      "Episode 32: Total Reward = -0.9\n",
      "Episode 33: Total Reward = 2.6999999999999997\n",
      "Episode 34: Total Reward = -0.9\n",
      "Episode 35: Total Reward = 2.1\n",
      "model saved\n",
      "Episode 36: Total Reward = -1.9000000000000001\n",
      "Episode 37: Total Reward = -0.08333333333333333\n",
      "Episode 38: Total Reward = -0.5\n",
      "Episode 39: Total Reward = -3.3000000000000003\n",
      "Episode 40: Total Reward = -0.8666666666666667\n",
      "model saved\n",
      "Episode 41: Total Reward = 1.0\n",
      "Episode 42: Total Reward = -1.3\n",
      "Episode 43: Total Reward = -0.5\n",
      "Episode 44: Total Reward = 1.0\n",
      "Episode 45: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 46: Total Reward = -0.9\n",
      "Episode 47: Total Reward = -0.9\n",
      "Episode 48: Total Reward = -1.3\n",
      "Episode 49: Total Reward = -0.5\n",
      "Episode 50: Total Reward = -1.7000000000000002\n",
      "model saved\n",
      "Episode 51: Total Reward = -0.9\n",
      "Episode 52: Total Reward = -0.9\n",
      "Episode 53: Total Reward = -2.5\n",
      "Episode 54: Total Reward = -6.1\n",
      "Episode 55: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 56: Total Reward = -0.5\n",
      "Episode 57: Total Reward = 0.5\n",
      "Episode 58: Total Reward = 1.0\n",
      "Episode 59: Total Reward = 1.5\n",
      "Episode 60: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 61: Total Reward = 1.0\n",
      "Episode 62: Total Reward = -0.9\n",
      "Episode 63: Total Reward = -0.9\n",
      "Episode 64: Total Reward = -0.9\n",
      "Episode 65: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 66: Total Reward = 0.5\n",
      "Episode 67: Total Reward = 1.5\n",
      "Episode 68: Total Reward = 1.5\n",
      "Episode 69: Total Reward = -2.5\n",
      "Episode 70: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 71: Total Reward = 1.0\n",
      "Episode 72: Total Reward = 1.5\n",
      "Episode 73: Total Reward = 0.5\n",
      "Episode 74: Total Reward = -1.3\n",
      "Episode 75: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 76: Total Reward = 1.5\n",
      "Episode 77: Total Reward = -0.5\n",
      "Episode 78: Total Reward = 1.0\n",
      "Episode 79: Total Reward = -0.5\n",
      "Episode 80: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 81: Total Reward = -0.5\n",
      "Episode 82: Total Reward = -0.5\n",
      "Episode 83: Total Reward = -0.9\n",
      "Episode 84: Total Reward = -0.5\n",
      "Episode 85: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 86: Total Reward = 0.5\n",
      "Episode 87: Total Reward = -0.5\n",
      "Episode 88: Total Reward = -0.9\n",
      "Episode 89: Total Reward = -0.6000000000000001\n",
      "Episode 90: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 91: Total Reward = 1.0\n",
      "Episode 92: Total Reward = -0.9\n",
      "Episode 93: Total Reward = -1.7000000000000002\n",
      "Episode 94: Total Reward = 1.5\n",
      "Episode 95: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 96: Total Reward = 1.0\n",
      "Episode 97: Total Reward = -0.9\n",
      "Episode 98: Total Reward = -0.5\n",
      "Episode 99: Total Reward = 0.5\n",
      "Episode 100: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 101: Total Reward = 0.5\n",
      "Episode 102: Total Reward = 1.0\n",
      "Episode 103: Total Reward = 0.5\n",
      "Episode 104: Total Reward = -0.5\n",
      "Episode 105: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 106: Total Reward = -0.9\n",
      "Episode 107: Total Reward = -0.9\n",
      "Episode 108: Total Reward = -0.9333333333333333\n",
      "Episode 109: Total Reward = 0.5\n",
      "Episode 110: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 111: Total Reward = 1.5\n",
      "Episode 112: Total Reward = -0.2\n",
      "Episode 113: Total Reward = 0.5\n",
      "Episode 114: Total Reward = 1.5\n",
      "Episode 115: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 116: Total Reward = 0.5\n",
      "Episode 117: Total Reward = -1.3\n",
      "Episode 118: Total Reward = 0.5\n",
      "Episode 119: Total Reward = -1.3\n",
      "Episode 120: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 121: Total Reward = 1.5\n",
      "Episode 122: Total Reward = 0.5\n",
      "Episode 123: Total Reward = -0.9\n",
      "Episode 124: Total Reward = 1.5\n",
      "Episode 125: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 126: Total Reward = -0.9\n",
      "Episode 127: Total Reward = -2.5\n",
      "Episode 128: Total Reward = 0.5\n",
      "Episode 129: Total Reward = 0.5\n",
      "Episode 130: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 131: Total Reward = -0.5\n",
      "Episode 132: Total Reward = -0.5\n",
      "Episode 133: Total Reward = -0.9\n",
      "Episode 134: Total Reward = -1.3\n",
      "Episode 135: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 136: Total Reward = 1.0\n",
      "Episode 137: Total Reward = 0.4666666666666667\n",
      "Episode 138: Total Reward = 0.5\n",
      "Episode 139: Total Reward = -0.5\n",
      "Episode 140: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 141: Total Reward = 0.5\n",
      "Episode 142: Total Reward = -0.9\n",
      "Episode 143: Total Reward = -0.9\n",
      "Episode 144: Total Reward = -0.5\n",
      "Episode 145: Total Reward = -1.7000000000000002\n",
      "model saved\n",
      "Episode 146: Total Reward = -0.9\n",
      "Episode 147: Total Reward = -1.3\n",
      "Episode 148: Total Reward = 1.0\n",
      "Episode 149: Total Reward = 1.0\n",
      "Episode 150: Total Reward = 0.8\n",
      "model saved\n",
      "Episode 151: Total Reward = 0.5\n",
      "Episode 152: Total Reward = 0.5\n",
      "Episode 153: Total Reward = -0.5\n",
      "Episode 154: Total Reward = -0.5\n",
      "Episode 155: Total Reward = 0.2\n",
      "model saved\n",
      "Episode 156: Total Reward = 0.5\n",
      "Episode 157: Total Reward = -0.7\n",
      "Episode 158: Total Reward = -0.5\n",
      "Episode 159: Total Reward = 0.2\n",
      "Episode 160: Total Reward = 0.2\n",
      "model saved\n",
      "Episode 161: Total Reward = 1.5\n",
      "Episode 162: Total Reward = -0.5\n",
      "Episode 163: Total Reward = 0.5\n",
      "Episode 164: Total Reward = -0.5\n",
      "Episode 165: Total Reward = -0.7\n",
      "model saved\n",
      "Episode 166: Total Reward = -1.7000000000000002\n",
      "Episode 167: Total Reward = 1.0\n",
      "Episode 168: Total Reward = -0.9\n",
      "Episode 169: Total Reward = 1.5\n",
      "Episode 170: Total Reward = -0.7\n",
      "model saved\n",
      "Episode 171: Total Reward = -0.5\n",
      "Episode 172: Total Reward = -0.9\n",
      "Episode 173: Total Reward = -0.9\n",
      "Episode 174: Total Reward = -0.9\n",
      "Episode 175: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 176: Total Reward = -0.9\n",
      "Episode 177: Total Reward = -1.3\n",
      "Episode 178: Total Reward = -0.9\n",
      "Episode 179: Total Reward = -1.7000000000000002\n",
      "Episode 180: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 181: Total Reward = -1.3\n",
      "Episode 182: Total Reward = -1.3\n",
      "Episode 183: Total Reward = -1.3\n",
      "Episode 184: Total Reward = -0.9\n",
      "Episode 185: Total Reward = 2.6999999999999997\n",
      "model saved\n",
      "Episode 186: Total Reward = -0.9\n",
      "Episode 187: Total Reward = 0.5\n",
      "Episode 188: Total Reward = -0.9\n",
      "Episode 189: Total Reward = -0.9\n",
      "Episode 190: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 191: Total Reward = 1.5\n",
      "Episode 192: Total Reward = -0.5\n",
      "Episode 193: Total Reward = -0.9\n",
      "Episode 194: Total Reward = -1.3\n",
      "Episode 195: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 196: Total Reward = -0.5\n",
      "Episode 197: Total Reward = 0.5\n",
      "Episode 198: Total Reward = -1.3\n",
      "Episode 199: Total Reward = 1.5\n",
      "Episode 200: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 201: Total Reward = 1.0\n",
      "Episode 202: Total Reward = -0.9\n",
      "Episode 203: Total Reward = 1.9000000000000001\n",
      "Episode 204: Total Reward = -1.3\n",
      "Episode 205: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 206: Total Reward = -0.5\n",
      "Episode 207: Total Reward = 0.5\n",
      "Episode 208: Total Reward = -0.9\n",
      "Episode 209: Total Reward = -0.5\n",
      "Episode 210: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 211: Total Reward = -0.5\n",
      "Episode 212: Total Reward = 0.5\n",
      "Episode 213: Total Reward = 1.5\n",
      "Episode 214: Total Reward = -1.3\n",
      "Episode 215: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 216: Total Reward = -1.3\n",
      "Episode 217: Total Reward = -0.9\n",
      "Episode 218: Total Reward = -1.3\n",
      "Episode 219: Total Reward = -1.3\n",
      "Episode 220: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 221: Total Reward = -0.5\n",
      "Episode 222: Total Reward = -0.9\n",
      "Episode 223: Total Reward = -0.9\n",
      "Episode 224: Total Reward = -1.3\n",
      "Episode 225: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 226: Total Reward = 0.5\n",
      "Episode 227: Total Reward = 0.5\n",
      "Episode 228: Total Reward = 0.5\n",
      "Episode 229: Total Reward = -0.5\n",
      "Episode 230: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 231: Total Reward = -1.3\n",
      "Episode 232: Total Reward = -0.5\n",
      "Episode 233: Total Reward = -1.3\n",
      "Episode 234: Total Reward = -0.9\n",
      "Episode 235: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 236: Total Reward = -0.5\n",
      "Episode 237: Total Reward = 1.0\n",
      "Episode 238: Total Reward = -0.5\n",
      "Episode 239: Total Reward = -1.7000000000000002\n",
      "Episode 240: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 241: Total Reward = -0.9\n",
      "Episode 242: Total Reward = -1.3\n",
      "Episode 243: Total Reward = -1.3\n",
      "Episode 244: Total Reward = 0.5\n",
      "Episode 245: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 246: Total Reward = -1.3\n",
      "Episode 247: Total Reward = 1.5\n",
      "Episode 248: Total Reward = 0.5\n",
      "Episode 249: Total Reward = 1.5\n",
      "Episode 250: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 251: Total Reward = -0.9\n",
      "Episode 252: Total Reward = -0.5\n",
      "Episode 253: Total Reward = -0.9\n",
      "Episode 254: Total Reward = -0.5\n",
      "Episode 255: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 256: Total Reward = 1.0\n",
      "Episode 257: Total Reward = -0.9\n",
      "Episode 258: Total Reward = -1.3\n",
      "Episode 259: Total Reward = 1.5\n",
      "Episode 260: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 261: Total Reward = 0.5\n",
      "Episode 262: Total Reward = 1.0\n",
      "Episode 263: Total Reward = 1.0\n",
      "Episode 264: Total Reward = 1.0\n",
      "Episode 265: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 266: Total Reward = 0.5\n",
      "Episode 267: Total Reward = -0.9\n",
      "Episode 268: Total Reward = -1.3\n",
      "Episode 269: Total Reward = 1.0\n",
      "Episode 270: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 271: Total Reward = 1.0\n",
      "Episode 272: Total Reward = -0.9\n",
      "Episode 273: Total Reward = -0.9\n",
      "Episode 274: Total Reward = 0.5\n",
      "Episode 275: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 276: Total Reward = -0.9\n",
      "Episode 277: Total Reward = -0.9\n",
      "Episode 278: Total Reward = 1.0\n",
      "Episode 279: Total Reward = 0.5\n",
      "Episode 280: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 281: Total Reward = -0.9\n",
      "Episode 282: Total Reward = -1.3\n",
      "Episode 283: Total Reward = 1.0\n",
      "Episode 284: Total Reward = -0.9\n",
      "Episode 285: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 286: Total Reward = -0.5\n",
      "Episode 287: Total Reward = 0.5\n",
      "Episode 288: Total Reward = 0.5\n",
      "Episode 289: Total Reward = -0.9\n",
      "Episode 290: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 291: Total Reward = -0.9\n",
      "Episode 292: Total Reward = -0.5\n",
      "Episode 293: Total Reward = 0.5\n",
      "Episode 294: Total Reward = -1.3\n",
      "Episode 295: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 296: Total Reward = 1.5\n",
      "Episode 297: Total Reward = -0.5\n",
      "Episode 298: Total Reward = -1.3\n",
      "Episode 299: Total Reward = -0.5\n",
      "Episode 300: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 301: Total Reward = -0.9\n",
      "Episode 302: Total Reward = 1.5\n",
      "Episode 303: Total Reward = -0.5\n",
      "Episode 304: Total Reward = 1.0\n",
      "Episode 305: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 306: Total Reward = -0.9\n",
      "Episode 307: Total Reward = 1.0\n",
      "Episode 308: Total Reward = 0.5\n",
      "Episode 309: Total Reward = -0.9\n",
      "Episode 310: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 311: Total Reward = 1.0\n",
      "Episode 312: Total Reward = -1.7000000000000002\n",
      "Episode 313: Total Reward = -1.3\n",
      "Episode 314: Total Reward = -0.5\n",
      "Episode 315: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 316: Total Reward = 0.5\n",
      "Episode 317: Total Reward = -0.9\n",
      "Episode 318: Total Reward = -1.7000000000000002\n",
      "Episode 319: Total Reward = -0.9\n",
      "Episode 320: Total Reward = 0.2\n",
      "model saved\n",
      "Episode 321: Total Reward = 1.0\n",
      "Episode 322: Total Reward = 1.5\n",
      "Episode 323: Total Reward = -0.5\n",
      "Episode 324: Total Reward = 1.0\n",
      "Episode 325: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 326: Total Reward = 0.5\n",
      "Episode 327: Total Reward = 1.5\n",
      "Episode 328: Total Reward = -0.5\n",
      "Episode 329: Total Reward = -1.3\n",
      "Episode 330: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 331: Total Reward = -1.3\n",
      "Episode 332: Total Reward = 1.5\n",
      "Episode 333: Total Reward = 1.0\n",
      "Episode 334: Total Reward = -0.5\n",
      "Episode 335: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 336: Total Reward = 1.0\n",
      "Episode 337: Total Reward = 1.0\n",
      "Episode 338: Total Reward = 1.5\n",
      "Episode 339: Total Reward = 1.5\n",
      "Episode 340: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 341: Total Reward = -0.5\n",
      "Episode 342: Total Reward = 1.0\n",
      "Episode 343: Total Reward = -0.5\n",
      "Episode 344: Total Reward = -0.9\n",
      "Episode 345: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 346: Total Reward = -0.9\n",
      "Episode 347: Total Reward = 1.5\n",
      "Episode 348: Total Reward = -0.9\n",
      "Episode 349: Total Reward = 1.0\n",
      "Episode 350: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 351: Total Reward = -0.9\n",
      "Episode 352: Total Reward = -0.9\n",
      "Episode 353: Total Reward = -0.9\n",
      "Episode 354: Total Reward = -0.9\n",
      "Episode 355: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 356: Total Reward = -0.5\n",
      "Episode 357: Total Reward = 1.0\n",
      "Episode 358: Total Reward = 0.5\n",
      "Episode 359: Total Reward = 1.2\n",
      "Episode 360: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 361: Total Reward = 0.5\n",
      "Episode 362: Total Reward = 1.0\n",
      "Episode 363: Total Reward = -0.5\n",
      "Episode 364: Total Reward = -0.9\n",
      "Episode 365: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 366: Total Reward = 1.5\n",
      "Episode 367: Total Reward = 1.5\n",
      "Episode 368: Total Reward = 1.0\n",
      "Episode 369: Total Reward = -1.3\n",
      "Episode 370: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 371: Total Reward = -0.9\n",
      "Episode 372: Total Reward = -0.9\n",
      "Episode 373: Total Reward = -0.9\n",
      "Episode 374: Total Reward = -0.5\n",
      "Episode 375: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 376: Total Reward = -0.9\n",
      "Episode 377: Total Reward = -0.5\n",
      "Episode 378: Total Reward = 1.0\n",
      "Episode 379: Total Reward = -0.5\n",
      "Episode 380: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 381: Total Reward = 1.0\n",
      "Episode 382: Total Reward = -0.9\n",
      "Episode 383: Total Reward = -1.3\n",
      "Episode 384: Total Reward = -0.9\n",
      "Episode 385: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 386: Total Reward = -0.9\n",
      "Episode 387: Total Reward = 1.0\n",
      "Episode 388: Total Reward = 1.5\n",
      "Episode 389: Total Reward = -0.9\n",
      "Episode 390: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 391: Total Reward = -0.9\n",
      "Episode 392: Total Reward = -0.9\n",
      "Episode 393: Total Reward = -0.9\n",
      "Episode 394: Total Reward = -1.3\n",
      "Episode 395: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 396: Total Reward = -1.3\n",
      "Episode 397: Total Reward = 0.5\n",
      "Episode 398: Total Reward = -0.5\n",
      "Episode 399: Total Reward = 1.0\n",
      "Episode 400: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 401: Total Reward = -0.9\n",
      "Episode 402: Total Reward = -0.9\n",
      "Episode 403: Total Reward = -0.5\n",
      "Episode 404: Total Reward = 1.5\n",
      "Episode 405: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 406: Total Reward = 1.0\n",
      "Episode 407: Total Reward = -1.3\n",
      "Episode 408: Total Reward = -0.9\n",
      "Episode 409: Total Reward = 1.5\n",
      "Episode 410: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 411: Total Reward = 1.0\n",
      "Episode 412: Total Reward = 2.6999999999999997\n",
      "Episode 413: Total Reward = 1.0\n",
      "Episode 414: Total Reward = -1.3\n",
      "Episode 415: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 416: Total Reward = -0.9\n",
      "Episode 417: Total Reward = 1.0\n",
      "Episode 418: Total Reward = -0.9\n",
      "Episode 419: Total Reward = -0.5\n",
      "Episode 420: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 421: Total Reward = 0.5\n",
      "Episode 422: Total Reward = -0.5\n",
      "Episode 423: Total Reward = -0.9\n",
      "Episode 424: Total Reward = -0.5\n",
      "Episode 425: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 426: Total Reward = -1.3\n",
      "Episode 427: Total Reward = -1.3\n",
      "Episode 428: Total Reward = -0.9\n",
      "Episode 429: Total Reward = 1.0\n",
      "Episode 430: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 431: Total Reward = -0.5\n",
      "Episode 432: Total Reward = 2.6999999999999997\n",
      "Episode 433: Total Reward = 1.5\n",
      "Episode 434: Total Reward = -0.5\n",
      "Episode 435: Total Reward = -1.7000000000000002\n",
      "model saved\n",
      "Episode 436: Total Reward = 0.5\n",
      "Episode 437: Total Reward = 1.0\n",
      "Episode 438: Total Reward = -0.9\n",
      "Episode 439: Total Reward = 0.5\n",
      "Episode 440: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 441: Total Reward = 0.5\n",
      "Episode 442: Total Reward = 1.0\n",
      "Episode 443: Total Reward = -1.3\n",
      "Episode 444: Total Reward = 1.0\n",
      "Episode 445: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 446: Total Reward = -0.5\n",
      "Episode 447: Total Reward = -0.5\n",
      "Episode 448: Total Reward = -0.9\n",
      "Episode 449: Total Reward = -0.5\n",
      "Episode 450: Total Reward = -1.7000000000000002\n",
      "model saved\n",
      "Episode 451: Total Reward = -0.5\n",
      "Episode 452: Total Reward = -0.5\n",
      "Episode 453: Total Reward = 0.5\n",
      "Episode 454: Total Reward = 0.5\n",
      "Episode 455: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 456: Total Reward = -1.7000000000000002\n",
      "Episode 457: Total Reward = -0.5\n",
      "Episode 458: Total Reward = -0.9\n",
      "Episode 459: Total Reward = 1.0\n",
      "Episode 460: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 461: Total Reward = 0.5\n",
      "Episode 462: Total Reward = -0.9\n",
      "Episode 463: Total Reward = -0.9\n",
      "Episode 464: Total Reward = -0.5\n",
      "Episode 465: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 466: Total Reward = 1.0\n",
      "Episode 467: Total Reward = 1.5\n",
      "Episode 468: Total Reward = -0.9\n",
      "Episode 469: Total Reward = -1.3\n",
      "Episode 470: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 471: Total Reward = -1.3\n",
      "Episode 472: Total Reward = 0.5\n",
      "Episode 473: Total Reward = 1.0\n",
      "Episode 474: Total Reward = -1.7000000000000002\n",
      "Episode 475: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 476: Total Reward = 1.0\n",
      "Episode 477: Total Reward = -0.5\n",
      "Episode 478: Total Reward = 0.5\n",
      "Episode 479: Total Reward = -1.3\n",
      "Episode 480: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 481: Total Reward = -0.9\n",
      "Episode 482: Total Reward = -0.5\n",
      "Episode 483: Total Reward = -0.9\n",
      "Episode 484: Total Reward = -1.3\n",
      "Episode 485: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 486: Total Reward = 1.5\n",
      "Episode 487: Total Reward = -0.9\n",
      "Episode 488: Total Reward = 1.0\n",
      "Episode 489: Total Reward = 1.0\n",
      "Episode 490: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 491: Total Reward = -0.9\n",
      "Episode 492: Total Reward = 1.5\n",
      "Episode 493: Total Reward = 1.5\n",
      "Episode 494: Total Reward = -0.9\n",
      "Episode 495: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 496: Total Reward = -0.5\n",
      "Episode 497: Total Reward = -0.5\n",
      "Episode 498: Total Reward = 1.0\n",
      "Episode 499: Total Reward = -0.5\n",
      "Episode 500: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 501: Total Reward = -1.3\n",
      "Episode 502: Total Reward = -0.9\n",
      "Episode 503: Total Reward = -1.3\n",
      "Episode 504: Total Reward = 1.5\n",
      "Episode 505: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 506: Total Reward = -2.5\n",
      "Episode 507: Total Reward = 1.0\n",
      "Episode 508: Total Reward = -1.3\n",
      "Episode 509: Total Reward = 1.0\n",
      "Episode 510: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 511: Total Reward = -0.9\n",
      "Episode 512: Total Reward = -0.5\n",
      "Episode 513: Total Reward = 1.5\n",
      "Episode 514: Total Reward = -0.9\n",
      "Episode 515: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 516: Total Reward = 0.5\n",
      "Episode 517: Total Reward = -0.9\n",
      "Episode 518: Total Reward = -1.7000000000000002\n",
      "Episode 519: Total Reward = -0.9\n",
      "Episode 520: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 521: Total Reward = -0.9\n",
      "Episode 522: Total Reward = -2.5\n",
      "Episode 523: Total Reward = -0.5\n",
      "Episode 524: Total Reward = 1.5\n",
      "Episode 525: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 526: Total Reward = -1.7000000000000002\n",
      "Episode 527: Total Reward = 1.5\n",
      "Episode 528: Total Reward = -1.3\n",
      "Episode 529: Total Reward = -0.5\n",
      "Episode 530: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 531: Total Reward = -0.5\n",
      "Episode 532: Total Reward = -0.5\n",
      "Episode 533: Total Reward = 1.0\n",
      "Episode 534: Total Reward = -0.5\n",
      "Episode 535: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 536: Total Reward = 0.5\n",
      "Episode 537: Total Reward = 1.5\n",
      "Episode 538: Total Reward = 1.5\n",
      "Episode 539: Total Reward = 1.5\n",
      "Episode 540: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 541: Total Reward = 1.0\n",
      "Episode 542: Total Reward = -0.9\n",
      "Episode 543: Total Reward = -0.5\n",
      "Episode 544: Total Reward = 0.5\n",
      "Episode 545: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 546: Total Reward = -0.9\n",
      "Episode 547: Total Reward = 1.0\n",
      "Episode 548: Total Reward = -0.9\n",
      "Episode 549: Total Reward = -0.5\n",
      "Episode 550: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 551: Total Reward = -0.5\n",
      "Episode 552: Total Reward = -0.9\n",
      "Episode 553: Total Reward = -0.9\n",
      "Episode 554: Total Reward = -0.9\n",
      "Episode 555: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 556: Total Reward = -0.5\n",
      "Episode 557: Total Reward = -0.5\n",
      "Episode 558: Total Reward = -1.3\n",
      "Episode 559: Total Reward = 0.5\n",
      "Episode 560: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 561: Total Reward = -0.9\n",
      "Episode 562: Total Reward = -1.3\n",
      "Episode 563: Total Reward = -0.9\n",
      "Episode 564: Total Reward = -0.9\n",
      "Episode 565: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 566: Total Reward = -0.9\n",
      "Episode 567: Total Reward = -0.5\n",
      "Episode 568: Total Reward = 1.5\n",
      "Episode 569: Total Reward = -0.5\n",
      "Episode 570: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 571: Total Reward = -1.3\n",
      "Episode 572: Total Reward = 1.0\n",
      "Episode 573: Total Reward = 0.5\n",
      "Episode 574: Total Reward = -0.9\n",
      "Episode 575: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 576: Total Reward = 0.5\n",
      "Episode 577: Total Reward = -0.9\n",
      "Episode 578: Total Reward = -0.9\n",
      "Episode 579: Total Reward = -0.9\n",
      "Episode 580: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 581: Total Reward = 1.0\n",
      "Episode 582: Total Reward = -0.9\n",
      "Episode 583: Total Reward = -0.5\n",
      "Episode 584: Total Reward = 1.0\n",
      "Episode 585: Total Reward = -1.7000000000000002\n",
      "model saved\n",
      "Episode 586: Total Reward = -1.7000000000000002\n",
      "Episode 587: Total Reward = 1.5\n",
      "Episode 588: Total Reward = -1.3\n",
      "Episode 589: Total Reward = 1.0\n",
      "Episode 590: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 591: Total Reward = -0.9\n",
      "Episode 592: Total Reward = -0.5\n",
      "Episode 593: Total Reward = -0.5\n",
      "Episode 594: Total Reward = 1.0\n",
      "Episode 595: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 596: Total Reward = 1.0\n",
      "Episode 597: Total Reward = -1.3\n",
      "Episode 598: Total Reward = 0.5\n",
      "Episode 599: Total Reward = -1.7000000000000002\n",
      "Episode 600: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 601: Total Reward = -0.9\n",
      "Episode 602: Total Reward = -0.5\n",
      "Episode 603: Total Reward = -0.9\n",
      "Episode 604: Total Reward = 1.5\n",
      "Episode 605: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 606: Total Reward = -0.9\n",
      "Episode 607: Total Reward = -0.5\n",
      "Episode 608: Total Reward = -0.5\n",
      "Episode 609: Total Reward = 0.5\n",
      "Episode 610: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 611: Total Reward = -0.9\n",
      "Episode 612: Total Reward = 1.5\n",
      "Episode 613: Total Reward = 1.0\n",
      "Episode 614: Total Reward = -1.3\n",
      "Episode 615: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 616: Total Reward = -1.3\n",
      "Episode 617: Total Reward = -1.3\n",
      "Episode 618: Total Reward = -1.3\n",
      "Episode 619: Total Reward = -0.9\n",
      "Episode 620: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 621: Total Reward = -1.3\n",
      "Episode 622: Total Reward = 1.5\n",
      "Episode 623: Total Reward = -0.5\n",
      "Episode 624: Total Reward = -0.5\n",
      "Episode 625: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 626: Total Reward = -2.5\n",
      "Episode 627: Total Reward = -0.9\n",
      "Episode 628: Total Reward = -1.3\n",
      "Episode 629: Total Reward = -0.9\n",
      "Episode 630: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 631: Total Reward = -0.5\n",
      "Episode 632: Total Reward = 0.5\n",
      "Episode 633: Total Reward = -0.9\n",
      "Episode 634: Total Reward = -1.3\n",
      "Episode 635: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 636: Total Reward = 1.0\n",
      "Episode 637: Total Reward = -0.5\n",
      "Episode 638: Total Reward = 1.0\n",
      "Episode 639: Total Reward = -1.3\n",
      "Episode 640: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 641: Total Reward = -0.5\n",
      "Episode 642: Total Reward = 1.0\n",
      "Episode 643: Total Reward = -0.5\n",
      "Episode 644: Total Reward = -0.9\n",
      "Episode 645: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 646: Total Reward = -1.3\n",
      "Episode 647: Total Reward = -0.5\n",
      "Episode 648: Total Reward = -0.5\n",
      "Episode 649: Total Reward = -0.9\n",
      "Episode 650: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 651: Total Reward = -0.5\n",
      "Episode 652: Total Reward = 0.5\n",
      "Episode 653: Total Reward = -0.9\n",
      "Episode 654: Total Reward = 0.5\n",
      "Episode 655: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 656: Total Reward = 1.0\n",
      "Episode 657: Total Reward = 1.0\n",
      "Episode 658: Total Reward = -0.9\n",
      "Episode 659: Total Reward = 1.0\n",
      "Episode 660: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 661: Total Reward = 1.5\n",
      "Episode 662: Total Reward = -0.9\n",
      "Episode 663: Total Reward = -0.9\n",
      "Episode 664: Total Reward = -0.9\n",
      "Episode 665: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 666: Total Reward = -0.9\n",
      "Episode 667: Total Reward = -0.5\n",
      "Episode 668: Total Reward = 1.5\n",
      "Episode 669: Total Reward = -0.5\n",
      "Episode 670: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 671: Total Reward = -0.9\n",
      "Episode 672: Total Reward = -0.9\n",
      "Episode 673: Total Reward = -0.5\n",
      "Episode 674: Total Reward = -0.5\n",
      "Episode 675: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 676: Total Reward = -1.3\n",
      "Episode 677: Total Reward = -1.3\n",
      "Episode 678: Total Reward = -0.5\n",
      "Episode 679: Total Reward = -1.3\n",
      "Episode 680: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 681: Total Reward = 1.0\n",
      "Episode 682: Total Reward = -1.3\n",
      "Episode 683: Total Reward = -0.9\n",
      "Episode 684: Total Reward = 0.5\n",
      "Episode 685: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 686: Total Reward = -0.9\n",
      "Episode 687: Total Reward = 0.5\n",
      "Episode 688: Total Reward = -2.5\n",
      "Episode 689: Total Reward = -0.5\n",
      "Episode 690: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 691: Total Reward = -1.3\n",
      "Episode 692: Total Reward = -1.7000000000000002\n",
      "Episode 693: Total Reward = 0.5\n",
      "Episode 694: Total Reward = -0.5\n",
      "Episode 695: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 696: Total Reward = -0.9\n",
      "Episode 697: Total Reward = -1.3\n",
      "Episode 698: Total Reward = -0.9\n",
      "Episode 699: Total Reward = -0.5\n",
      "Episode 700: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 701: Total Reward = -0.9\n",
      "Episode 702: Total Reward = -1.3\n",
      "Episode 703: Total Reward = 1.0\n",
      "Episode 704: Total Reward = 1.0\n",
      "Episode 705: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 706: Total Reward = 0.5\n",
      "Episode 707: Total Reward = -1.7000000000000002\n",
      "Episode 708: Total Reward = -1.3\n",
      "Episode 709: Total Reward = -0.5\n",
      "Episode 710: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 711: Total Reward = -1.3\n",
      "Episode 712: Total Reward = -0.9\n",
      "Episode 713: Total Reward = -1.3\n",
      "Episode 714: Total Reward = 1.0\n",
      "Episode 715: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 716: Total Reward = 0.5\n",
      "Episode 717: Total Reward = 1.0\n",
      "Episode 718: Total Reward = -1.3\n",
      "Episode 719: Total Reward = 1.5\n",
      "Episode 720: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 721: Total Reward = -0.9\n",
      "Episode 722: Total Reward = -0.5\n",
      "Episode 723: Total Reward = 1.0\n",
      "Episode 724: Total Reward = 0.5\n",
      "Episode 725: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 726: Total Reward = 1.0\n",
      "Episode 727: Total Reward = 0.5\n",
      "Episode 728: Total Reward = -0.5\n",
      "Episode 729: Total Reward = -0.5\n",
      "Episode 730: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 731: Total Reward = 1.5\n",
      "Episode 732: Total Reward = -0.5\n",
      "Episode 733: Total Reward = -0.5\n",
      "Episode 734: Total Reward = 1.5\n",
      "Episode 735: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 736: Total Reward = 1.5\n",
      "Episode 737: Total Reward = -0.9\n",
      "Episode 738: Total Reward = -1.7000000000000002\n",
      "Episode 739: Total Reward = 3.2\n",
      "Episode 740: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 741: Total Reward = -1.3\n",
      "Episode 742: Total Reward = -0.9\n",
      "Episode 743: Total Reward = -0.5\n",
      "Episode 744: Total Reward = -0.5\n",
      "Episode 745: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 746: Total Reward = -0.9\n",
      "Episode 747: Total Reward = -0.9\n",
      "Episode 748: Total Reward = -0.5\n",
      "Episode 749: Total Reward = -0.9\n",
      "Episode 750: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 751: Total Reward = 1.5\n",
      "Episode 752: Total Reward = 0.5\n",
      "Episode 753: Total Reward = -0.9\n",
      "Episode 754: Total Reward = -0.5\n",
      "Episode 755: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 756: Total Reward = -0.5\n",
      "Episode 757: Total Reward = -0.9\n",
      "Episode 758: Total Reward = -0.9\n",
      "Episode 759: Total Reward = 0.5\n",
      "Episode 760: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 761: Total Reward = 1.5\n",
      "Episode 762: Total Reward = 1.9000000000000001\n",
      "Episode 763: Total Reward = 0.5\n",
      "Episode 764: Total Reward = 1.5\n",
      "Episode 765: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 766: Total Reward = -1.3\n",
      "Episode 767: Total Reward = -0.5\n",
      "Episode 768: Total Reward = -0.5\n",
      "Episode 769: Total Reward = -0.9\n",
      "Episode 770: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 771: Total Reward = -0.5\n",
      "Episode 772: Total Reward = -0.9\n",
      "Episode 773: Total Reward = -0.9\n",
      "Episode 774: Total Reward = -0.9\n",
      "Episode 775: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 776: Total Reward = -1.3\n",
      "Episode 777: Total Reward = -0.9\n",
      "Episode 778: Total Reward = -0.5\n",
      "Episode 779: Total Reward = -0.5\n",
      "Episode 780: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 781: Total Reward = 1.0\n",
      "Episode 782: Total Reward = 1.9000000000000001\n",
      "Episode 783: Total Reward = -1.7000000000000002\n",
      "Episode 784: Total Reward = -0.5\n",
      "Episode 785: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 786: Total Reward = -0.5\n",
      "Episode 787: Total Reward = -0.9\n",
      "Episode 788: Total Reward = -0.9\n",
      "Episode 789: Total Reward = 0.5\n",
      "Episode 790: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 791: Total Reward = -1.3\n",
      "Episode 792: Total Reward = 1.5\n",
      "Episode 793: Total Reward = 0.5\n",
      "Episode 794: Total Reward = -0.9\n",
      "Episode 795: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 796: Total Reward = 1.5\n",
      "Episode 797: Total Reward = 1.5\n",
      "Episode 798: Total Reward = 0.5\n",
      "Episode 799: Total Reward = -0.5\n",
      "Episode 800: Total Reward = -0.4\n",
      "model saved\n",
      "Episode 801: Total Reward = 1.0\n",
      "Episode 802: Total Reward = -1.3\n",
      "Episode 803: Total Reward = -0.5\n",
      "Episode 804: Total Reward = -0.5\n",
      "Episode 805: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 806: Total Reward = 1.0\n",
      "Episode 807: Total Reward = 1.5999999999999999\n",
      "Episode 808: Total Reward = -0.9\n",
      "Episode 809: Total Reward = 0.5\n",
      "Episode 810: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 811: Total Reward = 1.5\n",
      "Episode 812: Total Reward = -0.5\n",
      "Episode 813: Total Reward = 1.5\n",
      "Episode 814: Total Reward = -0.9\n",
      "Episode 815: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 816: Total Reward = -0.9\n",
      "Episode 817: Total Reward = -1.7000000000000002\n",
      "Episode 818: Total Reward = -0.5\n",
      "Episode 819: Total Reward = 1.5\n",
      "Episode 820: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 821: Total Reward = -0.9\n",
      "Episode 822: Total Reward = -0.5\n",
      "Episode 823: Total Reward = 0.5\n",
      "Episode 824: Total Reward = 0.5\n",
      "Episode 825: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 826: Total Reward = -0.5\n",
      "Episode 827: Total Reward = -1.3\n",
      "Episode 828: Total Reward = -1.3\n",
      "Episode 829: Total Reward = 1.0\n",
      "Episode 830: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 831: Total Reward = 0.5\n",
      "Episode 832: Total Reward = -0.5\n",
      "Episode 833: Total Reward = -1.3\n",
      "Episode 834: Total Reward = -0.5\n",
      "Episode 835: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 836: Total Reward = -1.3\n",
      "Episode 837: Total Reward = -0.5\n",
      "Episode 838: Total Reward = -1.3\n",
      "Episode 839: Total Reward = -1.3\n",
      "Episode 840: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 841: Total Reward = -0.5\n",
      "Episode 842: Total Reward = 0.5\n",
      "Episode 843: Total Reward = 0.5\n",
      "Episode 844: Total Reward = -1.3\n",
      "Episode 845: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 846: Total Reward = -1.3\n",
      "Episode 847: Total Reward = 1.0\n",
      "Episode 848: Total Reward = 0.5\n",
      "Episode 849: Total Reward = 2.6999999999999997\n",
      "Episode 850: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 851: Total Reward = -0.9\n",
      "Episode 852: Total Reward = -1.3\n",
      "Episode 853: Total Reward = 1.5\n",
      "Episode 854: Total Reward = -1.3\n",
      "Episode 855: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 856: Total Reward = -1.3\n",
      "Episode 857: Total Reward = -1.3\n",
      "Episode 858: Total Reward = 1.0\n",
      "Episode 859: Total Reward = -0.9\n",
      "Episode 860: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 861: Total Reward = -0.5\n",
      "Episode 862: Total Reward = -1.3\n",
      "Episode 863: Total Reward = -0.5333333333333333\n",
      "Episode 864: Total Reward = 1.0\n",
      "Episode 865: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 866: Total Reward = -0.5\n",
      "Episode 867: Total Reward = -0.5\n",
      "Episode 868: Total Reward = -0.9\n",
      "Episode 869: Total Reward = -0.5\n",
      "Episode 870: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 871: Total Reward = -0.9\n",
      "Episode 872: Total Reward = -1.7000000000000002\n",
      "Episode 873: Total Reward = 1.0\n",
      "Episode 874: Total Reward = -1.7000000000000002\n",
      "Episode 875: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 876: Total Reward = -1.7000000000000002\n",
      "Episode 877: Total Reward = -0.5\n",
      "Episode 878: Total Reward = -0.5\n",
      "Episode 879: Total Reward = 1.0\n",
      "Episode 880: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 881: Total Reward = 0.5\n",
      "Episode 882: Total Reward = -0.5\n",
      "Episode 883: Total Reward = -0.5\n",
      "Episode 884: Total Reward = 1.0\n",
      "Episode 885: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 886: Total Reward = 0.5\n",
      "Episode 887: Total Reward = -0.9\n",
      "Episode 888: Total Reward = -0.9\n",
      "Episode 889: Total Reward = -0.5\n",
      "Episode 890: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 891: Total Reward = -0.9\n",
      "Episode 892: Total Reward = -0.5\n",
      "Episode 893: Total Reward = -0.9\n",
      "Episode 894: Total Reward = 0.5\n",
      "Episode 895: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 896: Total Reward = 0.5\n",
      "Episode 897: Total Reward = -0.9\n",
      "Episode 898: Total Reward = -1.7000000000000002\n",
      "Episode 899: Total Reward = -0.5\n",
      "Episode 900: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 901: Total Reward = 1.0\n",
      "Episode 902: Total Reward = 1.5\n",
      "Episode 903: Total Reward = -1.7000000000000002\n",
      "Episode 904: Total Reward = -2.5\n",
      "Episode 905: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 906: Total Reward = -0.5\n",
      "Episode 907: Total Reward = 0.5\n",
      "Episode 908: Total Reward = -0.9\n",
      "Episode 909: Total Reward = 0.5\n",
      "Episode 910: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 911: Total Reward = 1.0\n",
      "Episode 912: Total Reward = 1.5\n",
      "Episode 913: Total Reward = -0.5\n",
      "Episode 914: Total Reward = -0.5\n",
      "Episode 915: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 916: Total Reward = -1.3\n",
      "Episode 917: Total Reward = -0.9\n",
      "Episode 918: Total Reward = -0.5\n",
      "Episode 919: Total Reward = 1.0\n",
      "Episode 920: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 921: Total Reward = 1.5\n",
      "Episode 922: Total Reward = 1.0\n",
      "Episode 923: Total Reward = -0.5\n",
      "Episode 924: Total Reward = 1.5\n",
      "Episode 925: Total Reward = -2.5\n",
      "model saved\n",
      "Episode 926: Total Reward = 1.0\n",
      "Episode 927: Total Reward = -1.3\n",
      "Episode 928: Total Reward = -1.3\n",
      "Episode 929: Total Reward = -0.9\n",
      "Episode 930: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 931: Total Reward = -0.9\n",
      "Episode 932: Total Reward = -0.5\n",
      "Episode 933: Total Reward = -0.9\n",
      "Episode 934: Total Reward = -1.3\n",
      "Episode 935: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 936: Total Reward = 1.0\n",
      "Episode 937: Total Reward = -1.3\n",
      "Episode 938: Total Reward = -1.3\n",
      "Episode 939: Total Reward = -1.3\n",
      "Episode 940: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 941: Total Reward = 0.5\n",
      "Episode 942: Total Reward = -0.5\n",
      "Episode 943: Total Reward = 1.0\n",
      "Episode 944: Total Reward = 0.5\n",
      "Episode 945: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 946: Total Reward = -1.3\n",
      "Episode 947: Total Reward = -0.9\n",
      "Episode 948: Total Reward = 0.5\n",
      "Episode 949: Total Reward = 1.5\n",
      "Episode 950: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 951: Total Reward = -0.5\n",
      "Episode 952: Total Reward = 1.0\n",
      "Episode 953: Total Reward = -0.9\n",
      "Episode 954: Total Reward = -0.9\n",
      "Episode 955: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 956: Total Reward = -0.5\n",
      "Episode 957: Total Reward = 1.0\n",
      "Episode 958: Total Reward = -0.9\n",
      "Episode 959: Total Reward = 0.5\n",
      "Episode 960: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 961: Total Reward = -1.7000000000000002\n",
      "Episode 962: Total Reward = 1.0\n",
      "Episode 963: Total Reward = -0.5\n",
      "Episode 964: Total Reward = -0.5\n",
      "Episode 965: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 966: Total Reward = 1.5\n",
      "Episode 967: Total Reward = 0.5\n",
      "Episode 968: Total Reward = 0.5\n",
      "Episode 969: Total Reward = -0.9\n",
      "Episode 970: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 971: Total Reward = -0.9\n",
      "Episode 972: Total Reward = -1.3\n",
      "Episode 973: Total Reward = -2.5\n",
      "Episode 974: Total Reward = -0.9\n",
      "Episode 975: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 976: Total Reward = 0.5\n",
      "Episode 977: Total Reward = -0.9\n",
      "Episode 978: Total Reward = -0.9\n",
      "Episode 979: Total Reward = -0.5\n",
      "Episode 980: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 981: Total Reward = -0.9\n",
      "Episode 982: Total Reward = 1.0\n",
      "Episode 983: Total Reward = -0.5\n",
      "Episode 984: Total Reward = 1.0\n",
      "Episode 985: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 986: Total Reward = -0.9\n",
      "Episode 987: Total Reward = -0.9\n",
      "Episode 988: Total Reward = -0.9\n",
      "Episode 989: Total Reward = 1.0\n",
      "Episode 990: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 991: Total Reward = 1.0\n",
      "Episode 992: Total Reward = -1.7000000000000002\n",
      "Episode 993: Total Reward = 1.9000000000000001\n",
      "Episode 994: Total Reward = -0.5\n",
      "Episode 995: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 996: Total Reward = -1.4666666666666666\n",
      "Episode 997: Total Reward = -0.9\n",
      "Episode 998: Total Reward = -0.5\n",
      "Episode 999: Total Reward = -0.5\n",
      "Episode 1000: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 1001: Total Reward = 0.5\n",
      "Episode 1002: Total Reward = -0.5\n",
      "Episode 1003: Total Reward = -0.9\n",
      "Episode 1004: Total Reward = 0.5\n",
      "Episode 1005: Total Reward = -1.7000000000000002\n",
      "model saved\n",
      "Episode 1006: Total Reward = 1.0\n",
      "Episode 1007: Total Reward = 1.0\n",
      "Episode 1008: Total Reward = -1.3\n",
      "Episode 1009: Total Reward = -0.9\n",
      "Episode 1010: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1011: Total Reward = -0.5\n",
      "Episode 1012: Total Reward = -0.9\n",
      "Episode 1013: Total Reward = 1.5\n",
      "Episode 1014: Total Reward = 1.0\n",
      "Episode 1015: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1016: Total Reward = -0.5\n",
      "Episode 1017: Total Reward = -0.5\n",
      "Episode 1018: Total Reward = 1.0\n",
      "Episode 1019: Total Reward = 1.5\n",
      "Episode 1020: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1021: Total Reward = 0.5\n",
      "Episode 1022: Total Reward = 0.5\n",
      "Episode 1023: Total Reward = -0.9\n",
      "Episode 1024: Total Reward = -1.7000000000000002\n",
      "Episode 1025: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1026: Total Reward = 1.6666666666666665\n",
      "Episode 1027: Total Reward = 1.0\n",
      "Episode 1028: Total Reward = -0.5\n",
      "Episode 1029: Total Reward = -1.3\n",
      "Episode 1030: Total Reward = -2.5\n",
      "model saved\n",
      "Episode 1031: Total Reward = 0.5\n",
      "Episode 1032: Total Reward = -0.9\n",
      "Episode 1033: Total Reward = -1.3\n",
      "Episode 1034: Total Reward = 0.5\n",
      "Episode 1035: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1036: Total Reward = 0.5\n",
      "Episode 1037: Total Reward = 0.5\n",
      "Episode 1038: Total Reward = -0.9\n",
      "Episode 1039: Total Reward = -1.7000000000000002\n",
      "Episode 1040: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 1041: Total Reward = -0.9\n",
      "Episode 1042: Total Reward = -0.5\n",
      "Episode 1043: Total Reward = -0.5\n",
      "Episode 1044: Total Reward = -0.5\n",
      "Episode 1045: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 1046: Total Reward = 1.0\n",
      "Episode 1047: Total Reward = -1.3\n",
      "Episode 1048: Total Reward = 0.5\n",
      "Episode 1049: Total Reward = 1.5\n",
      "Episode 1050: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 1051: Total Reward = -0.5\n",
      "Episode 1052: Total Reward = 1.0\n",
      "Episode 1053: Total Reward = -1.7000000000000002\n",
      "Episode 1054: Total Reward = -1.3\n",
      "Episode 1055: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1056: Total Reward = -0.5\n",
      "Episode 1057: Total Reward = 1.0\n",
      "Episode 1058: Total Reward = -1.3\n",
      "Episode 1059: Total Reward = 0.5\n",
      "Episode 1060: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 1061: Total Reward = -0.9\n",
      "Episode 1062: Total Reward = 1.0\n",
      "Episode 1063: Total Reward = -1.7000000000000002\n",
      "Episode 1064: Total Reward = -0.9\n",
      "Episode 1065: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1066: Total Reward = -0.5\n",
      "Episode 1067: Total Reward = 1.5\n",
      "Episode 1068: Total Reward = -0.9\n",
      "Episode 1069: Total Reward = 1.0\n",
      "Episode 1070: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 1071: Total Reward = 0.5\n",
      "Episode 1072: Total Reward = -0.5\n",
      "Episode 1073: Total Reward = -0.5\n",
      "Episode 1074: Total Reward = 0.5\n",
      "Episode 1075: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1076: Total Reward = -0.5\n",
      "Episode 1077: Total Reward = -0.9\n",
      "Episode 1078: Total Reward = 1.5\n",
      "Episode 1079: Total Reward = -2.5\n",
      "Episode 1080: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1081: Total Reward = 1.5\n",
      "Episode 1082: Total Reward = -1.7000000000000002\n",
      "Episode 1083: Total Reward = -0.5\n",
      "Episode 1084: Total Reward = -1.3\n",
      "Episode 1085: Total Reward = -1.3\n",
      "model saved\n",
      "Episode 1086: Total Reward = -1.3\n",
      "Episode 1087: Total Reward = 0.5\n",
      "Episode 1088: Total Reward = -0.5\n",
      "Episode 1089: Total Reward = -0.5\n",
      "Episode 1090: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 1091: Total Reward = -1.0\n",
      "Episode 1092: Total Reward = -0.9\n",
      "Episode 1093: Total Reward = 1.0\n",
      "Episode 1094: Total Reward = 1.5\n",
      "Episode 1095: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1096: Total Reward = 1.0\n",
      "Episode 1097: Total Reward = -0.9\n",
      "Episode 1098: Total Reward = -1.3\n",
      "Episode 1099: Total Reward = -0.9\n",
      "Episode 1100: Total Reward = 0.5\n",
      "model saved\n",
      "Episode 1101: Total Reward = 0.5\n",
      "Episode 1102: Total Reward = 0.5\n",
      "Episode 1103: Total Reward = -0.9\n",
      "Episode 1104: Total Reward = -0.9\n",
      "Episode 1105: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1106: Total Reward = -0.9\n",
      "Episode 1107: Total Reward = 1.5\n",
      "Episode 1108: Total Reward = -0.5\n",
      "Episode 1109: Total Reward = -1.7000000000000002\n",
      "Episode 1110: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 1111: Total Reward = -1.3\n",
      "Episode 1112: Total Reward = 1.0\n",
      "Episode 1113: Total Reward = -0.9\n",
      "Episode 1114: Total Reward = -0.9\n",
      "Episode 1115: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1116: Total Reward = 0.5\n",
      "Episode 1117: Total Reward = -1.3\n",
      "Episode 1118: Total Reward = 0.5\n",
      "Episode 1119: Total Reward = -0.9\n",
      "Episode 1120: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1121: Total Reward = -0.5\n",
      "Episode 1122: Total Reward = -0.9\n",
      "Episode 1123: Total Reward = -1.3\n",
      "Episode 1124: Total Reward = 0.5\n",
      "Episode 1125: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1126: Total Reward = -0.9\n",
      "Episode 1127: Total Reward = 0.5\n",
      "Episode 1128: Total Reward = -0.5\n",
      "Episode 1129: Total Reward = -0.9\n",
      "Episode 1130: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1131: Total Reward = -0.6666666666666667\n",
      "Episode 1132: Total Reward = 0.5\n",
      "Episode 1133: Total Reward = 1.0\n",
      "Episode 1134: Total Reward = -0.5\n",
      "Episode 1135: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1136: Total Reward = 0.5\n",
      "Episode 1137: Total Reward = 1.0\n",
      "Episode 1138: Total Reward = -1.3\n",
      "Episode 1139: Total Reward = -0.5\n",
      "Episode 1140: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1141: Total Reward = 0.5\n",
      "Episode 1142: Total Reward = 1.0\n",
      "Episode 1143: Total Reward = 0.5\n",
      "Episode 1144: Total Reward = -0.5\n",
      "Episode 1145: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 1146: Total Reward = 0.5\n",
      "Episode 1147: Total Reward = -0.9\n",
      "Episode 1148: Total Reward = 0.5\n",
      "Episode 1149: Total Reward = -0.5\n",
      "Episode 1150: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 1151: Total Reward = -0.9\n",
      "Episode 1152: Total Reward = -1.3\n",
      "Episode 1153: Total Reward = 0.5\n",
      "Episode 1154: Total Reward = -0.5\n",
      "Episode 1155: Total Reward = 1.9000000000000001\n",
      "model saved\n",
      "Episode 1156: Total Reward = -0.5\n",
      "Episode 1157: Total Reward = -0.5\n",
      "Episode 1158: Total Reward = 1.0\n",
      "Episode 1159: Total Reward = 1.0\n",
      "Episode 1160: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1161: Total Reward = -0.5\n",
      "Episode 1162: Total Reward = -1.3\n",
      "Episode 1163: Total Reward = 1.0\n",
      "Episode 1164: Total Reward = -1.3\n",
      "Episode 1165: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1166: Total Reward = 1.0\n",
      "Episode 1167: Total Reward = -0.5\n",
      "Episode 1168: Total Reward = -2.5\n",
      "Episode 1169: Total Reward = 0.5\n",
      "Episode 1170: Total Reward = 1.0\n",
      "model saved\n",
      "Episode 1171: Total Reward = 0.5\n",
      "Episode 1172: Total Reward = -0.9\n",
      "Episode 1173: Total Reward = -2.5\n",
      "Episode 1174: Total Reward = -0.9\n",
      "Episode 1175: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1176: Total Reward = 0.5\n",
      "Episode 1177: Total Reward = -0.9\n",
      "Episode 1178: Total Reward = 0.5\n",
      "Episode 1179: Total Reward = -0.5\n",
      "Episode 1180: Total Reward = 1.5\n",
      "model saved\n",
      "Episode 1181: Total Reward = -0.9\n",
      "Episode 1182: Total Reward = -0.5\n",
      "Episode 1183: Total Reward = 1.5\n",
      "Episode 1184: Total Reward = 1.0\n",
      "Episode 1185: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1186: Total Reward = -1.3\n",
      "Episode 1187: Total Reward = 1.0\n",
      "Episode 1188: Total Reward = 1.0\n",
      "Episode 1189: Total Reward = 1.0\n",
      "Episode 1190: Total Reward = -0.5\n",
      "model saved\n",
      "Episode 1191: Total Reward = 0.5\n",
      "Episode 1192: Total Reward = -2.5\n",
      "Episode 1193: Total Reward = 0.5\n",
      "Episode 1194: Total Reward = 1.0\n",
      "Episode 1195: Total Reward = -0.9\n",
      "model saved\n",
      "Episode 1196: Total Reward = 1.5\n",
      "Episode 1197: Total Reward = 0.5\n",
      "Episode 1198: Total Reward = 0.5\n",
      "Episode 1199: Total Reward = -0.9\n",
      "Episode 1200: Total Reward = 0.5\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, banker_log_prob, bet_log_prob = ppo_agent.select_action(state)\n",
    "\n",
    "        # ensure action\n",
    "        banker_action, bet_action = action\n",
    "        # ensure env\n",
    "        next_state, reward, done, _ = env.step((banker_action, bet_action))\n",
    "\n",
    "        # record data\n",
    "        states.append(state)\n",
    "        actions.append([banker_action, bet_action])\n",
    "        log_probs.append(banker_log_prob + bet_log_prob)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # update PPO\n",
    "    ppo_agent.update(states, actions, log_probs, rewards, dones)\n",
    "\n",
    "    # show result\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "    \n",
    "    # save model every interval\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        ppo_agent.save_model()\n",
    "\n",
    "# 1200 episode / 403m 45.3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load model\n",
    "def load_model(agent, policy_model_path, value_model_path):\n",
    "    print(f\"using model: policy -> {policy_model_path}, value -> {value_model_path}\")\n",
    "    agent.policy.load_state_dict(torch.load(policy_model_path))\n",
    "    agent.value.load_state_dict(torch.load(value_model_path))\n",
    "    print(\"finish model loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test function\n",
    "# use trained model to test\n",
    "# let user input 4 card in hand, and model will output banker bet and player bet\n",
    "def test_trained_model(env, agent, policy_model, value_model):\n",
    "    print(\"start testing, enter 'exit' can leave testing mode\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    load_model(agent, policy_model, value_model)\n",
    "    while True:\n",
    "        try:\n",
    "            # get init state\n",
    "            state = env.reset()\n",
    "\n",
    "            # let user input 4 cards in hand\n",
    "            print(\"\\n please enter 4 cards in hand(type : heart J daimond 10 club K spade A), or enter 'exit' to leave :\")\n",
    "            input_cards = input().strip()\n",
    "\n",
    "            # eixt & wrong input\n",
    "            if input_cards.lower() == 'exit':\n",
    "                print(\"finish testing !\")\n",
    "                break\n",
    "            input_cards = input_cards.split()\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"wrong input ! please enter the suit & value of 4 cards (total 8 str)\")\n",
    "                continue\n",
    "\n",
    "            # parse the input hand\n",
    "            player_hand = [(input_cards[i], input_cards[i + 1]) for i in range(0, 8, 2)]\n",
    "            print(f\"your hand : {player_hand}\")\n",
    "\n",
    "            # update state, make sure the hand information is correct\n",
    "            for i in range(4):\n",
    "                state[i * 2] = get_suit_rank(player_hand[i])\n",
    "                state[i * 2 + 1] = get_card_rank(player_hand[i])\n",
    "\n",
    "            # trun into pytorch and move to GPU\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # predict banker multiplier by model\n",
    "            with torch.no_grad():\n",
    "                banker_dist, bet_dist = agent.policy(state_tensor)\n",
    "                # print all banker multiplier's probability\n",
    "                print(f\"banker_dist: {banker_dist}\")\n",
    "                # print all bet multiplier's probability\n",
    "                print(f\"bet_dist: {bet_dist}\")\n",
    "                \n",
    "                # use max probability to be our decision\n",
    "                if hasattr(banker_dist, 'probs'):\n",
    "                    banker_action = torch.argmax(banker_dist.probs).item()\n",
    "                else:\n",
    "                    banker_action = torch.argmax(banker_dist).item()\n",
    "                    \n",
    "                if hasattr(bet_dist, 'probs'):\n",
    "                    bet_action = torch.argmax(bet_dist.probs).item()\n",
    "                else:\n",
    "                    bet_action = torch.argmax(bet_dist).item()\n",
    "\n",
    "            print(f\"the banker multiplier predict by model : {banker_action}\")\n",
    "\n",
    "            # enter whether get banker\n",
    "            is_banker = input(\"get banker (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"you are banker, no need to bet\")\n",
    "            else:\n",
    "                print(\"you are not banker\")\n",
    "                banker_multiplier = float(input(\"input the banker multiplier : \").strip())\n",
    "                print(f\"banker multiplier: {banker_multiplier}\")\n",
    "\n",
    "                # reload the state of banker multiplier\n",
    "                state[-4] = banker_multiplier\n",
    "\n",
    "                # transfer pytorch into GPU\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                # bet multiplier predicted by model\n",
    "                with torch.no_grad():\n",
    "                    _, bet_dist = agent.policy(state_tensor)\n",
    "                    # print all bet multiplier's probability\n",
    "                    print(f\"🔍 bet_dist: {bet_dist}\")\n",
    "                    if hasattr(bet_dist, 'probs'):\n",
    "                        bet_action = torch.argmax(bet_dist.probs).item()\n",
    "                    else:\n",
    "                        bet_action = torch.argmax(bet_dist).item()\n",
    "\n",
    "                print(f\"the bet multiplier predict by model: {bet_action}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 測試模式啟動！輸入 `exit` 可離開測試模式。\n",
      "🔍 正在使用的模型: policy -> D:\\python\\poker_gto\\ppo_models\\ppo_model_v2_policy.pth, value -> D:\\python\\poker_gto\\ppo_models\\ppo_model_v2_value.pth\n",
      "✅ 模型載入完成！\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "🔍 banker_dist: tensor([[1.0000e+00, 5.3097e-09, 7.0077e-09, 7.7462e-08, 5.9058e-08]])\n",
      "🔍 bet_dist: tensor([[3.2299e-07, 1.0000e+00, 2.4663e-10, 4.5769e-09, 2.0982e-08]])\n",
      "🤖 模型預測的搶莊倍率: 0\n",
      "沒有搶到莊家\n",
      "🤖 莊家的倍率: 4.0\n",
      "🔍 bet_dist: tensor([[1.2889e-07, 1.0000e+00, 9.3902e-11, 1.8543e-09, 9.2701e-09]])\n",
      "🤖 模型建議的下注倍率: 1\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', '8')]\n",
      "🔍 banker_dist: tensor([[1.0000e+00, 1.1953e-09, 9.0293e-10, 1.5182e-08, 1.4504e-08]])\n",
      "🔍 bet_dist: tensor([[1.8523e-09, 1.0000e+00, 1.3970e-12, 8.2057e-11, 3.9673e-10]])\n",
      "🤖 模型預測的搶莊倍率: 0\n",
      "沒有搶到莊家\n",
      "🤖 莊家的倍率: 2.0\n",
      "🔍 bet_dist: tensor([[1.1957e-09, 1.0000e+00, 9.2904e-13, 5.7513e-11, 2.8250e-10]])\n",
      "🤖 模型建議的下注倍率: 1\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "👋 測試結束！\n"
     ]
    }
   ],
   "source": [
    "# practical operate\n",
    "use_policy_model = \"D:\\python\\poker_gto\\ppo_models\\ppo_model_v2_policy.pth\"\n",
    "use_value_model = \"D:\\python\\poker_gto\\ppo_models\\ppo_model_v2_value.pth\"\n",
    "\n",
    "test_trained_model(env, ppo_agent, use_policy_model, use_value_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find banker distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random hand\n",
    "def generate_random_hand():\n",
    "    suits = [\"heart\", \"diamond\", \"club\", \"spade\"]\n",
    "    ranks = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\", \"A\"]\n",
    "    hand = np.random.choice([f\"{suit} {rank}\" for suit in suits for rank in ranks], 4, replace=False)\n",
    "    hand = [tuple(card.split()) for card in hand]\n",
    "    return hand\n",
    "\n",
    "# def banker distribution\n",
    "def test_banker_distribution(env, agent, num_tests=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    banker_choices = []\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        state = env.reset()\n",
    "        player_hand = generate_random_hand()\n",
    "        \n",
    "        for i in range(4):\n",
    "            state[i * 2] = get_suit_rank(player_hand[i])\n",
    "            state[i * 2 + 1] = get_card_rank(player_hand[i])\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            banker_dist, _ = agent.policy(state_tensor)\n",
    "            banker_action = torch.argmax(banker_dist).item()\n",
    "            banker_choices.append(banker_action)\n",
    "\n",
    "    # caculate distribution\n",
    "    counter = Counter(banker_choices)\n",
    "    total = sum(counter.values())\n",
    "    for action, count in sorted(counter.items()):\n",
    "        print(f\"倍率 {action}: {count} 次 ({count / total:.2%})\")\n",
    "    \n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "倍率 0: 500 次 (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 500})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test banker distribution for random hand\n",
    "test_banker_distribution(env, ppo_agent, num_tests=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 測試模式啟動！輸入 `exit` 可離開測試模式。\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "🎴 你的手牌: [('heart', 'J'), ('diamond', '10'), ('club', 'J'), ('spade', 'A')]\n",
      "📊 banker_dist 統計 (測試 100 次): {3: 100}\n",
      "\n",
      "請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\n",
      "👋 測試結束！\n"
     ]
    }
   ],
   "source": [
    "test_trained_model(env, ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do list\n",
    "* **(finish)** 訓練時應該包含花色 : 可能將花色也轉為數值，變成一個 2 維度的 state  <br>\n",
    "* **(finish)** 訓練後應該可以保存模型，並且疊家每次訓練的成果上去 <br>\n",
    "* 新增一個輸入 : 假設我不是莊家時，現在的倍率是幾倍，這會影響到我後續的下注策略 <br>\n",
    "* <b>(maybe finish)</b>輸入我的手牌之類的資訊後應該要可以當作回測，紀錄到模型訓練當中，並加以改進 <br>\n",
    "\n",
    "* 完成後可能可以架設簡單的 app 或 api  <br>\n",
    "* 完成後可以接著改做 德州撲克的訓練 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_with_real_hands(env, agent):\n",
    "    \"\"\"\n",
    "    讓使用者輸入實際手牌，讓 AI 提供決策建議，並將結果回報給模型，增強學習\n",
    "    \"\"\"\n",
    "    print(\"\\U0001F4CA 進入回測模式！輸入 `exit` 可離開回測模式。\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            state = env.reset()\n",
    "\n",
    "            print(\"\\n請輸入 4 張手牌（格式：heart J diamond 10 club J spade A），或輸入 `exit` 離開:\")\n",
    "            input_cards = input().strip()\n",
    "\n",
    "            if input_cards.lower() == 'exit':\n",
    "                print(\"\\U0001F44B 回測結束！\")\n",
    "                break\n",
    "\n",
    "            input_cards = input_cards.split()\n",
    "\n",
    "            if len(input_cards) != 8:\n",
    "                print(\"❌ 錯誤！請輸入 4 張手牌的花色與數值（共 8 個字串）。\")\n",
    "                continue\n",
    "\n",
    "            player_hand = [(input_cards[i], input_cards[i + 1]) for i in range(0, 8, 2)]\n",
    "            print(f\"\\U0001F3B4 你的手牌: {player_hand}\")\n",
    "\n",
    "            for i in range(4):\n",
    "                state[i] = card_value(player_hand[i])\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                banker_dist, bet_dist = agent.policy(state_tensor)\n",
    "                banker_action = torch.argmax(banker_dist.probs).item()\n",
    "                bet_action = torch.argmax(bet_dist.probs).item()\n",
    "\n",
    "            print(f\"\\U0001F916 AI 建議的搶莊倍率: {banker_action}\")\n",
    "\n",
    "            is_banker = input(\"✅ 是否搶到莊？ (y/n): \").strip().lower()\n",
    "            if is_banker == 'y':\n",
    "                print(\"\\U0001F389 你是莊家！不需要下注\")\n",
    "                bet_action = 0\n",
    "            else:\n",
    "                print(f\"\\U0001F916 AI 建議的下注倍率: {bet_action}\")\n",
    "\n",
    "            reward = float(input(\"\\U0001F4B0 請輸入這局的最終收益（負值代表虧損）: \").strip())\n",
    "\n",
    "            states = [state]\n",
    "            actions = [[banker_action, bet_action]]\n",
    "            rewards = [reward]\n",
    "            dones = [True]\n",
    "\n",
    "            agent.update(states, actions, rewards, dones)\n",
    "            print(\"\\U0001F4C8 AI 已學習這局的結果！\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 發生錯誤: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing reward settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 共產生 162 組 reward config\n",
      "model not found, start training from begining\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_bad\u001b[39m\u001b[38;5;124m\"\u001b[39m: is_bad, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounter\u001b[39m\u001b[38;5;124m\"\u001b[39m: counter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# === 5. 執行所有 config ===\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mevaluate_reward_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reward_configs)]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# === 6. 輸出結果 ===\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== 總結結果 =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 102\u001b[0m, in \u001b[0;36mevaluate_reward_function\u001b[0;34m(config_idx, config, episodes)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    101\u001b[0m     action, _, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m--> 102\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate([state], [action], [\u001b[38;5;241m0.0\u001b[39m], [reward], [done])\n\u001b[1;32m    104\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[11], line 61\u001b[0m, in \u001b[0;36mapply_reward_config_to_env_class.<locals>.CustomNiuNiuEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 61\u001b[0m     state, _, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     is_banker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbanker_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_banker:\n",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m, in \u001b[0;36mNiuNiuEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbet_amount \u001b[38;5;241m=\u001b[39m bet_action\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mstep 1 : decide whether to get banker\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    * myself : by ppo agent\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    * others : by simulate_ev to decide\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m bank_multipliers \u001b[38;5;241m=\u001b[39m [\u001b[43msimulate_ev\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)]\n\u001b[1;32m     84\u001b[0m bank_multipliers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbanker_bid\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# run time : 22s\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/poker_niuniu_optimize-master/niuniu_func.py:190\u001b[0m, in \u001b[0;36msimulate_ev\u001b[0;34m(player_hand, num_simulations)\u001b[0m\n\u001b[1;32m    187\u001b[0m     enemy_hands\u001b[38;5;241m.\u001b[39mappend(enemy_hand)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# calculate payout (bet 1 per game)\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m total_game_payout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcalculate_payout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_player_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menemy_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menemy_hand\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menemy_hands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# weighted payout\u001b[39;00m\n\u001b[1;32m    193\u001b[0m total_payout[multiplier] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_game_payout \u001b[38;5;241m*\u001b[39m multiplier\n",
      "File \u001b[0;32m~/Desktop/poker_niuniu_optimize-master/niuniu_func.py:190\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m     enemy_hands\u001b[38;5;241m.\u001b[39mappend(enemy_hand)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# calculate payout (bet 1 per game)\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m total_game_payout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mcalculate_payout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_player_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menemy_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m enemy_hand \u001b[38;5;129;01min\u001b[39;00m enemy_hands)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# weighted payout\u001b[39;00m\n\u001b[1;32m    193\u001b[0m total_payout[multiplier] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_game_payout \u001b[38;5;241m*\u001b[39m multiplier\n",
      "File \u001b[0;32m~/Desktop/poker_niuniu_optimize-master/niuniu_func.py:127\u001b[0m, in \u001b[0;36mcalculate_payout\u001b[0;34m(player_hand, banker_hand, verbose, debug)\u001b[0m\n\u001b[1;32m    120\u001b[0m rank_mult \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m五小牛\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m五公\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m四花牛\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛牛\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛9\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛8\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛7\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛6\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛5\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛4\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛3\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m牛1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m有公無牛\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m無公無牛\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    124\u001b[0m }\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# get player & banker hand\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m player_type \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_niu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_hand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m banker_type \u001b[38;5;241m=\u001b[39m calculate_niu(banker_hand)\n\u001b[1;32m    130\u001b[0m player_rank \u001b[38;5;241m=\u001b[39m rank_order[player_type]\n",
      "File \u001b[0;32m~/Desktop/poker_niuniu_optimize-master/niuniu_func.py:35\u001b[0m, in \u001b[0;36mcalculate_niu\u001b[0;34m(cards)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_niu\u001b[39m(cards):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# compare value rank then suit rank\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     cards_value \u001b[38;5;241m=\u001b[39m [card_value(card) \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m cards]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# sepcial card type\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(card[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m cards):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from niuniu_func import get_card_rank, get_suit_rank, calculate_payout\n",
    "\n",
    "# === 1. 自動產生 reward config 組合 ===\n",
    "scale_modes = [\"normalize\", \"raw\"]\n",
    "punish_negatives = [0.1, 0.2, 0.3]\n",
    "bonus_highs = [0.1, 0.2, 0.3]\n",
    "penalty_big_losses = [0.1, 0.2, 0.3]\n",
    "penalty_high_bid_losses = [0.1, 0.2, 0.3]\n",
    "\n",
    "reward_configs = []\n",
    "for scale, punish, bonus, big_loss, high_bid_loss in itertools.product(\n",
    "        scale_modes, punish_negatives, bonus_highs, penalty_big_losses, penalty_high_bid_losses):\n",
    "    config = {\n",
    "        \"scale_mode\": scale,\n",
    "        \"punish_negative\": punish,\n",
    "        \"bonus_high\": bonus,\n",
    "        \"penalty_big_loss\": big_loss,\n",
    "        \"penalty_high_bid_loss\": high_bid_loss\n",
    "    }\n",
    "    reward_configs.append(config)\n",
    "\n",
    "print(f\"✅ 共產生 {len(reward_configs)} 組 reward config\")\n",
    "\n",
    "# === 2. 測試 banker 分佈 ===\n",
    "def generate_random_hand():\n",
    "    suits = [\"heart\", \"diamond\", \"club\", \"spade\"]\n",
    "    ranks = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\", \"A\"]\n",
    "    hand = np.random.choice([f\"{suit} {rank}\" for suit in suits for rank in ranks], 4, replace=False)\n",
    "    return [tuple(card.split()) for card in hand]\n",
    "\n",
    "def test_banker_distribution(env, agent, num_tests=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.policy.to(device)\n",
    "    banker_choices = []\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        state = env.reset()\n",
    "        player_hand = generate_random_hand()\n",
    "\n",
    "        for i in range(4):\n",
    "            state[i * 2] = get_suit_rank(player_hand[i])\n",
    "            state[i * 2 + 1] = get_card_rank(player_hand[i])\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            banker_dist, _ = agent.policy(state_tensor)\n",
    "            banker_action = torch.argmax(banker_dist).item()\n",
    "            banker_choices.append(banker_action)\n",
    "\n",
    "    counter = Counter(banker_choices)\n",
    "    return counter\n",
    "\n",
    "# === 3. 注入 reward 設定進 Env ===\n",
    "def apply_reward_config_to_env_class(reward_config):\n",
    "    class CustomNiuNiuEnv(NiuNiuEnv):\n",
    "        def step(self, action):\n",
    "            state, _, done, _ = super().step(action)\n",
    "\n",
    "            is_banker = self.banker_index == 0\n",
    "            if is_banker:\n",
    "                total_payout = -sum(\n",
    "                    calculate_payout(self.players[i], self.players[0], False) * self.bets[i] * self.banker_multiplier\n",
    "                    for i in range(4) if i != self.banker_index\n",
    "                )\n",
    "                max_payout = 3 * 5 * self.banker_multiplier\n",
    "                reward = total_payout / max_payout if reward_config[\"scale_mode\"] == \"raw\" else (total_payout + max_payout) / (2 * max_payout)\n",
    "                if reward < 0:\n",
    "                    reward -= reward_config[\"punish_negative\"]\n",
    "                if reward > 0.7:\n",
    "                    reward += reward_config[\"bonus_high\"]\n",
    "                if self.banker_multiplier >= 3 and total_payout < 0:\n",
    "                    reward -= reward_config[\"penalty_high_bid_loss\"]\n",
    "            else:\n",
    "                total_payout = calculate_payout(self.players[0], self.players[self.banker_index], False) * self.bets[0] * self.banker_multiplier\n",
    "                max_possible = 5 * self.banker_multiplier\n",
    "                reward = total_payout / max_possible if reward_config[\"scale_mode\"] == \"raw\" else (total_payout + max_possible) / (2 * max_possible)\n",
    "                if reward > 0.7:\n",
    "                    reward += reward_config[\"bonus_high\"]\n",
    "                if reward < 0:\n",
    "                    reward -= reward_config[\"punish_negative\"]\n",
    "\n",
    "            return self.get_state(), reward, True, {}\n",
    "\n",
    "    return CustomNiuNiuEnv\n",
    "\n",
    "# === 4. 執行訓練與測試 ===\n",
    "def evaluate_reward_function(config_idx, config, episodes=60):\n",
    "    EnvClass = apply_reward_config_to_env_class(config)\n",
    "    env = EnvClass()\n",
    "    state_dim = len(env.get_state())\n",
    "    agent = PPOAgent(state_dim, 5, 5, model_path=f\"grid_model_{config_idx}\", num_envs=1)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _, _ = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.update([state], [action], [0.0], [reward], [done])\n",
    "            state = next_state\n",
    "\n",
    "    counter = test_banker_distribution(env, agent)\n",
    "    top_ratio = max(counter.values()) / sum(counter.values())\n",
    "    is_bad = top_ratio > 0.8\n",
    "\n",
    "    print(f\"\\n=== Config {config_idx} 完成 ===\")\n",
    "    print(f\"是否偏斷 (>80%): {'❌' if is_bad else '✅'}\")\n",
    "    print(f\"分佈: {counter}\")\n",
    "    print(f\"Config: {config}\")\n",
    "\n",
    "    return {\"index\": config_idx, \"is_bad\": is_bad, \"counter\": counter, \"config\": config}\n",
    "\n",
    "# === 5. 執行所有 config ===\n",
    "results = [evaluate_reward_function(i, config) for i, config in enumerate(reward_configs)]\n",
    "\n",
    "# === 6. 輸出結果 ===\n",
    "print(\"\\n===== 總結結果 =====\")\n",
    "for result in results:\n",
    "    print(f\"\\n--- Config {result['index']} ---\")\n",
    "    print(f\"是否偏斷 (>80%): {'❌' if result['is_bad'] else '✅'}\")\n",
    "    print(f\"分佈: {result['counter']}\")\n",
    "    print(f\"Config: {result['config']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上方程式碼預計: \n",
    "3 times/ 1 minutes\n",
    "60 times one config\n",
    "with 162 config\n",
    "3240 times --> 1080 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parellel computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
